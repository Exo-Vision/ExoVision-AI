import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.impute import KNNImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# [1] ë°ì´í„° ë¡œë“œ
print("=" * 80)
print("ğŸš€ ê³ ì„±ëŠ¥ ì•™ìƒë¸” ëª¨ë¸ - ì™¸ê³„í–‰ì„± ë¶„ë¥˜")
print("=" * 80)

data = pd.read_csv("datasets/preprocessed_all.csv")
print(f"\në°ì´í„° í¬ê¸°: {data.shape}")
print(f"íƒ€ê²Ÿ ë¶„í¬:\n{data['label'].value_counts()}")

# [2] Feature Engineering
def create_features(df):
    """ê³ ê¸‰ íŠ¹ì„± ìƒì„±"""
    df = df.copy()
    
    # ê¸°ì¡´ íŠ¹ì„±ë“¤
    if 'pl_rade' in df.columns and 'pl_orbper' in df.columns:
        df['rade_orbper_ratio'] = df['pl_rade'] / (df['pl_orbper'] + 1e-8)
    
    if 'pl_trandep' in df.columns and 'pl_trandurh' in df.columns:
        df['trandep_durh_product'] = df['pl_trandep'] * df['pl_trandurh']
    
    if 'st_teff' in df.columns and 'st_dist' in df.columns:
        df['teff_dist_ratio'] = df['st_teff'] / (df['st_dist'] + 1e-8)
    
    if 'pl_insol' in df.columns and 'snr' in df.columns:
        df['insol_snr_ratio'] = df['pl_insol'] / (df['snr'] + 1e-8)
    
    # ìƒˆë¡œìš´ íŠ¹ì„±ë“¤
    if 'pl_rade' in df.columns:
        df['rade_squared'] = df['pl_rade'] ** 2
        df['rade_log'] = np.log1p(df['pl_rade'])
    
    if 'pl_orbper' in df.columns:
        df['orbper_log'] = np.log1p(df['pl_orbper'])
    
    if 'snr' in df.columns:
        df['snr_log'] = np.log1p(df['snr'])
        df['snr_squared'] = df['snr'] ** 2
    
    if 'st_teff' in df.columns:
        df['teff_normalized'] = df['st_teff'] / 6000  # íƒœì–‘ ì˜¨ë„ ê¸°ì¤€
    
    # Confidence íŠ¹ì„± ì¡°í•©
    confidence_cols = [col for col in df.columns if 'confidence' in col]
    if len(confidence_cols) > 0:
        df['avg_confidence'] = df[confidence_cols].mean(axis=1)
        df['min_confidence'] = df[confidence_cols].min(axis=1)
        df['max_confidence'] = df[confidence_cols].max(axis=1)
    
    # Limit íŠ¹ì„± ì¡°í•©
    lim_cols = [col for col in df.columns if 'lim' in col]
    if len(lim_cols) > 0:
        df['sum_limits'] = df[lim_cols].sum(axis=1)
    
    return df

# [3] ë°ì´í„° ì¤€ë¹„
target = "label"
X = data.drop(target, axis=1)
y = data[target]

# Feature Engineering ì ìš©
X = create_features(X)

# ë²”ì£¼í˜• ì²˜ë¦¬
X = pd.get_dummies(X, columns=['mission'], drop_first=False)

# íƒ€ê²Ÿ ì¸ì½”ë”©
le = LabelEncoder()
y = le.fit_transform(y)

print(f"\níŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í›„ íŠ¹ì„± ìˆ˜: {X.shape[1]}")

# Train/Test ë¶„í•  (Stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"í›ˆë ¨ ì„¸íŠ¸ í¬ê¸°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í¬ê¸°: {X_test.shape}")

# [4] ê²°ì¸¡ê°’ ì²˜ë¦¬ - KNN Imputer (ë” ì •êµí•œ ë°©ë²•)
print("\nê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
imputer = KNNImputer(n_neighbors=5, weights='distance')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# [5] ìŠ¤ì¼€ì¼ë§ - Robust Scaler (ì´ìƒì¹˜ì— ê°•í•¨)
print("ìŠ¤ì¼€ì¼ë§ ì¤‘...")
scaler = RobustScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# [6] ê³ ì„±ëŠ¥ ëª¨ë¸ ì •ì˜
print("\nëª¨ë¸ í•™ìŠµ ì¤‘...")

# XGBoost - ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
xgb = XGBClassifier(
    n_estimators=1000,
    learning_rate=0.03,
    max_depth=8,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0.1,
    reg_alpha=0.1,
    reg_lambda=1.0,
    tree_method="hist",
    random_state=42,
    eval_metric='mlogloss',
    early_stopping_rounds=50
)

# LightGBM - ìµœì í™”
lgb = LGBMClassifier(
    n_estimators=1000,
    learning_rate=0.03,
    max_depth=8,
    num_leaves=64,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_samples=30,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42,
    verbose=-1
)

# CatBoost - ìµœì í™”
cb = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.03,
    depth=8,
    l2_leaf_reg=5,
    bootstrap_type='Bernoulli',
    subsample=0.8,
    random_strength=0.1,
    verbose=False,
    random_state=42,
    early_stopping_rounds=50
)

# RandomForest - ìµœì í™”
rf = RandomForestClassifier(
    n_estimators=800,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    bootstrap=True,
    oob_score=True,
    random_state=42,
    n_jobs=-1
)

# ExtraTrees - ì¶”ê°€ ë‹¤ì–‘ì„±
et = ExtraTreesClassifier(
    n_estimators=800,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    bootstrap=True,
    random_state=42,
    n_jobs=-1
)

# GradientBoosting
gb = GradientBoostingClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=7,
    min_samples_split=5,
    min_samples_leaf=2,
    subsample=0.8,
    max_features='sqrt',
    random_state=42
)

# [7] ê°œë³„ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
models = {
    'XGBoost': xgb,
    'LightGBM': lgb,
    'CatBoost': cb,
    'RandomForest': rf,
    'ExtraTrees': et,
    'GradientBoosting': gb
}

predictions = {}
accuracies = {}

print("\n" + "=" * 80)
print("ê°œë³„ ëª¨ë¸ ì„±ëŠ¥")
print("=" * 80)

for name, model in models.items():
    print(f"\ní•™ìŠµ ì¤‘: {name}...")
    
    # Early stoppingì„ ìœ„í•œ eval_set (XGBoost, CatBoost)
    if name in ['XGBoost', 'CatBoost']:
        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    else:
        model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    
    predictions[name] = y_pred
    accuracies[name] = acc
    
    print(f"{name} ì •í™•ë„: {acc:.4f}")

# [8] Soft Voting Ensemble (í™•ë¥  ê¸°ë°˜)
print("\n" + "=" * 80)
print("ì•™ìƒë¸” ì˜ˆì¸¡ (Soft Voting)")
print("=" * 80)

# ê° ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡
proba_predictions = {}
for name, model in models.items():
    proba_predictions[name] = model.predict_proba(X_test)

# ê°€ì¤‘ í‰ê·  (ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜)
total_acc = sum(accuracies.values())
weights = {name: acc / total_acc for name, acc in accuracies.items()}

print("\nëª¨ë¸ ê°€ì¤‘ì¹˜:")
for name, weight in weights.items():
    print(f"  {name}: {weight:.4f}")

# Weighted soft voting
weighted_proba = np.zeros_like(proba_predictions['XGBoost'])
for name, proba in proba_predictions.items():
    weighted_proba += weights[name] * proba

y_pred_ensemble = np.argmax(weighted_proba, axis=1)
acc_ensemble = accuracy_score(y_test, y_pred_ensemble)

print(f"\n{'=' * 80}")
print(f"ğŸ¯ ìµœì¢… ì•™ìƒë¸” ì •í™•ë„: {acc_ensemble:.4f}")
print(f"{'=' * 80}")

# [9] ìƒì„¸ ë¶„ì„
print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred_ensemble, 
                          target_names=le.classes_))

print("\ní˜¼ë™ í–‰ë ¬:")
cm = confusion_matrix(y_test, y_pred_ensemble)
cm_df = pd.DataFrame(cm, 
                     index=le.classes_, 
                     columns=le.classes_)
print(cm_df)

# [10] ì„±ëŠ¥ ìš”ì•½
print("\n" + "=" * 80)
print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½")
print("=" * 80)
print(f"ë² ì´ìŠ¤ë¼ì¸ (ìµœë¹ˆ í´ë˜ìŠ¤): {max(np.bincount(y_test)) / len(y_test):.4f}")
print(f"ìµœê³  ê°œë³„ ëª¨ë¸: {max(accuracies.values()):.4f} ({max(accuracies, key=accuracies.get)})")
print(f"ì•™ìƒë¸” ëª¨ë¸: {acc_ensemble:.4f}")
print(f"í–¥ìƒë„: {(acc_ensemble - max(accuracies.values())) * 100:.2f}%p")
print("=" * 80)
