"""
ì§ì ‘ 3-í´ë˜ìŠ¤ ë¶„ë¥˜ ì‹œìŠ¤í…œ (ë‹¨ì¼ ëª¨ë¸ ì ‘ê·¼)
- 2-ëª¨ë¸ ì‹œìŠ¤í…œì˜ ë³µì¡ì„± ì œê±°
- CatBoost ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜
- SMOTE + í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
- 44ê°œ ê³ ê¸‰ í”¼ì²˜
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from imblearn.over_sampling import SMOTE

from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier

import joblib
import os
from datetime import datetime

print("=" * 100)
print("ğŸ¯ ì§ì ‘ 3-í´ë˜ìŠ¤ ë¶„ë¥˜ ì‹œìŠ¤í…œ")
print("=" * 100)

# ============================================================================
# ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# ============================================================================
print("\n[1] ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
print("-" * 100)

df = pd.read_csv('datasets/exoplanets_integrated.csv', low_memory=False)
print(f"ì›ë³¸ ë°ì´í„°: {df.shape[0]:,} ìƒ˜í”Œ")

# REFUTED ì œê±° (22ê°œë¿) + NaN ì œê±°
df = df[df['koi_disposition'] != 'REFUTED']
df = df.dropna(subset=['koi_disposition'])
print(f"REFUTED ë° NaN ì œê±° í›„: {df.shape[0]:,} ìƒ˜í”Œ")

y_full = df['koi_disposition']
X_full = df.drop(['koi_disposition', 'kepoi_name'], axis=1, errors='ignore')

print(f"\níƒ€ê²Ÿ ë¶„í¬:")
for label, count in y_full.value_counts().items():
    print(f"  {label}: {count:,} ({count/len(y_full)*100:.1f}%)")

# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ
numeric_cols = X_full.select_dtypes(include=[np.number]).columns
X_full = X_full[numeric_cols]

# ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (44ê°œ)
print("\ní”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘ (44ê°œ)...")

# ê¸°ì¡´ í”¼ì²˜ (10ê°œ)
X_full['planet_star_ratio'] = X_full['koi_prad'] / (X_full['koi_srad'] + 1e-10)
X_full['orbital_energy'] = 1.0 / (X_full['koi_sma'] + 1e-10)
X_full['transit_signal'] = X_full['koi_depth'] * X_full['koi_duration']
X_full['stellar_density'] = X_full['koi_smass'] / (X_full['koi_srad']**3 + 1e-10)
X_full['planet_density_proxy'] = X_full['koi_prad']**3 / (X_full['koi_sma']**2 + 1e-10)
X_full['log_period'] = np.log1p(X_full['koi_period'])
X_full['log_depth'] = np.log1p(X_full['koi_depth'])
X_full['log_insol'] = np.log1p(X_full['koi_insol'])
X_full['orbit_stability'] = X_full['koi_eccen'] * X_full['koi_impact']
X_full['transit_snr'] = X_full['koi_depth'] / (X_full['koi_duration'] + 1e-10)

# ê³ ê¸‰ í”¼ì²˜ (15ê°œ)
X_full['habitable_zone_score'] = 1.0 / (1.0 + np.abs(X_full['koi_insol'] - 1.0))
X_full['temp_habitable_score'] = 1.0 / (1.0 + np.abs(X_full['koi_teq'] - 288) / 100)
X_full['roche_limit'] = 2.46 * X_full['koi_srad'] * (X_full['koi_smass'] / (X_full['koi_prad'] / 109.0))**(1/3)
X_full['hill_sphere'] = X_full['koi_sma'] * (X_full['koi_smass'] / 3.0)**(1/3)
X_full['transit_probability'] = X_full['koi_srad'] / (X_full['koi_sma'] * 215.032 + 1e-10)
X_full['improved_snr'] = (X_full['koi_depth'] * np.sqrt(X_full['koi_duration'])) / (X_full['koi_period'] + 1e-10)
X_full['stability_index'] = (1 - X_full['koi_eccen']) * (1 - X_full['koi_impact'])
X_full['mass_ratio'] = (X_full['koi_prad'] / 109.0)**3 / (X_full['koi_smass'] + 1e-10)
X_full['tidal_heating'] = X_full['koi_eccen'] / (X_full['koi_sma']**3 + 1e-10)
X_full['duration_ratio'] = X_full['koi_duration'] / (X_full['koi_period'] + 1e-10)
X_full['radiation_balance'] = X_full['koi_insol'] * (X_full['koi_prad']**2) / (X_full['koi_sma']**2 + 1e-10)
X_full['age_metallicity'] = X_full['koi_sage'] * (X_full['koi_smet'] + 2.5)
X_full['depth_size_ratio'] = X_full['koi_depth'] / (X_full['koi_prad']**2 + 1e-10)
X_full['kepler_ratio'] = X_full['koi_period']**2 / (X_full['koi_sma']**3 + 1e-10)
X_full['depth_variability'] = X_full['koi_depth'] / (X_full['koi_duration'] * X_full['koi_period'] + 1e-10)

# Inf, NaN ì²˜ë¦¬
X_full = X_full.replace([np.inf, -np.inf], np.nan)
X_full = X_full.fillna(X_full.median())

print(f"ìµœì¢… í”¼ì²˜ ìˆ˜: {X_full.shape[1]}ê°œ")

# ============================================================================
# ë ˆì´ë¸” ì¸ì½”ë”©
# ============================================================================
le = LabelEncoder()
y_encoded = le.fit_transform(y_full)

print(f"\në ˆì´ë¸” ë§¤í•‘:")
for i, label in enumerate(le.classes_):
    print(f"  {i}: {label} ({(y_encoded == i).sum():,}ê°œ)")

# ============================================================================
# Train/Test Split
# ============================================================================
X_train, X_test, y_train, y_test = train_test_split(
    X_full, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded
)

print(f"\nTrain: {len(y_train):,} / Test: {len(y_test):,}")

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ============================================================================
# SMOTE ë°ì´í„° ì¦ê°•
# ============================================================================
print("\n" + "=" * 100)
print("ğŸ“ˆ SMOTE ë°ì´í„° ì¦ê°•")
print("=" * 100)

print("\nì¦ê°• ì „ ë¶„í¬:")
for i, label in enumerate(le.classes_):
    count = (y_train == i).sum()
    print(f"  {label}: {count:,} ({count/len(y_train)*100:.1f}%)")

smote = SMOTE(random_state=42, k_neighbors=5)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"\nì¦ê°• í›„ ë¶„í¬:")
for i, label in enumerate(le.classes_):
    count = (y_train_smote == i).sum()
    print(f"  {label}: {count:,} ({count/len(y_train_smote)*100:.1f}%)")

print(f"\nì´ ìƒ˜í”Œ: {len(y_train):,} â†’ {len(y_train_smote):,} (+{len(y_train_smote)-len(y_train):,})")

# ============================================================================
# ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìë™ ì¡°ì •)
# ============================================================================
print("\n" + "=" * 100)
print("ğŸ¤– ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ")
print("=" * 100)

# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
class_counts = np.bincount(y_train)
total = len(y_train)
class_weights = {i: total / (len(class_counts) * count) for i, count in enumerate(class_counts)}

print("\ní´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:")
for i, label in enumerate(le.classes_):
    print(f"  {label}: {class_weights[i]:.3f}")

models = {}

# CatBoost (ìµœì í™”) - subsample ì œê±°í•˜ê³  MVS Sampling ì‚¬ìš©
print("\n[1] CatBoost í•™ìŠµ ì¤‘...")
models['CatBoost'] = CatBoostClassifier(
    iterations=2000,
    depth=8,
    learning_rate=0.01,
    l2_leaf_reg=20.0,
    bagging_temperature=0.5,
    border_count=128,
    bootstrap_type='Bernoulli',  # subsample ëŒ€ì‹ 
    subsample=0.85,
    class_weights=list(class_weights.values()),
    random_state=42,
    verbose=False
)
models['CatBoost'].fit(X_train_smote, y_train_smote)

train_pred_cb = models['CatBoost'].predict(X_train_smote)
test_pred_cb = models['CatBoost'].predict(X_test_scaled)
train_acc_cb = accuracy_score(y_train_smote, train_pred_cb)
test_acc_cb = accuracy_score(y_test, test_pred_cb)
cv_scores_cb = cross_val_score(models['CatBoost'], X_train_smote, y_train_smote, cv=5, scoring='accuracy', n_jobs=-1)

print(f"  Train: {train_acc_cb:.4f} ({train_acc_cb*100:.2f}%)")
print(f"  Test: {test_acc_cb:.4f} ({test_acc_cb*100:.2f}%)")
print(f"  CV: {cv_scores_cb.mean():.4f}Â±{cv_scores_cb.std():.4f}")
print(f"  ê³¼ì í•©: {(train_acc_cb - test_acc_cb)*100:.2f}%p")

# XGBoost
print("\n[2] XGBoost í•™ìŠµ ì¤‘...")
models['XGBoost'] = XGBClassifier(
    n_estimators=2000,
    max_depth=8,
    learning_rate=0.01,
    subsample=0.85,
    colsample_bytree=0.85,
    reg_alpha=8.0,
    reg_lambda=20.0,
    random_state=42,
    eval_metric='mlogloss',
    n_jobs=-1
)
models['XGBoost'].fit(X_train_smote, y_train_smote)

train_pred_xgb = models['XGBoost'].predict(X_train_smote)
test_pred_xgb = models['XGBoost'].predict(X_test_scaled)
train_acc_xgb = accuracy_score(y_train_smote, train_pred_xgb)
test_acc_xgb = accuracy_score(y_test, test_pred_xgb)
cv_scores_xgb = cross_val_score(models['XGBoost'], X_train_smote, y_train_smote, cv=5, scoring='accuracy', n_jobs=-1)

print(f"  Train: {train_acc_xgb:.4f} ({train_acc_xgb*100:.2f}%)")
print(f"  Test: {test_acc_xgb:.4f} ({test_acc_xgb*100:.2f}%)")
print(f"  CV: {cv_scores_xgb.mean():.4f}Â±{cv_scores_xgb.std():.4f}")
print(f"  ê³¼ì í•©: {(train_acc_xgb - test_acc_xgb)*100:.2f}%p")

# LightGBM
print("\n[3] LightGBM í•™ìŠµ ì¤‘...")
models['LightGBM'] = LGBMClassifier(
    n_estimators=2000,
    max_depth=8,
    learning_rate=0.01,
    subsample=0.85,
    colsample_bytree=0.85,
    reg_alpha=8.0,
    reg_lambda=20.0,
    class_weight='balanced',
    random_state=42,
    verbose=-1,
    n_jobs=-1
)
models['LightGBM'].fit(X_train_smote, y_train_smote)

train_pred_lgb = models['LightGBM'].predict(X_train_smote)
test_pred_lgb = models['LightGBM'].predict(X_test_scaled)
train_acc_lgb = accuracy_score(y_train_smote, train_pred_lgb)
test_acc_lgb = accuracy_score(y_test, test_pred_lgb)
cv_scores_lgb = cross_val_score(models['LightGBM'], X_train_smote, y_train_smote, cv=5, scoring='accuracy', n_jobs=-1)

print(f"  Train: {train_acc_lgb:.4f} ({train_acc_lgb*100:.2f}%)")
print(f"  Test: {test_acc_lgb:.4f} ({test_acc_lgb*100:.2f}%)")
print(f"  CV: {cv_scores_lgb.mean():.4f}Â±{cv_scores_lgb.std():.4f}")
print(f"  ê³¼ì í•©: {(train_acc_lgb - test_acc_lgb)*100:.2f}%p")

# Voting Ensemble
print("\n[4] Voting Ensemble í•™ìŠµ ì¤‘...")
voting = VotingClassifier(
    estimators=[
        ('catboost', models['CatBoost']),
        ('xgboost', models['XGBoost']),
        ('lightgbm', models['LightGBM'])
    ],
    voting='soft',
    weights=[2, 1, 1]  # CatBoostì— ë†’ì€ ê°€ì¤‘ì¹˜
)
voting.fit(X_train_smote, y_train_smote)

train_pred_voting = voting.predict(X_train_smote)
test_pred_voting = voting.predict(X_test_scaled)
train_acc_voting = accuracy_score(y_train_smote, train_pred_voting)
test_acc_voting = accuracy_score(y_test, test_pred_voting)
cv_scores_voting = cross_val_score(voting, X_train_smote, y_train_smote, cv=5, scoring='accuracy', n_jobs=-1)

print(f"  Train: {train_acc_voting:.4f} ({train_acc_voting*100:.2f}%)")
print(f"  Test: {test_acc_voting:.4f} ({test_acc_voting*100:.2f}%)")
print(f"  CV: {cv_scores_voting.mean():.4f}Â±{cv_scores_voting.std():.4f}")
print(f"  ê³¼ì í•©: {(train_acc_voting - test_acc_voting)*100:.2f}%p")

# ============================================================================
# ìµœê³  ëª¨ë¸ ì„ íƒ
# ============================================================================
results = {
    'CatBoost': test_acc_cb,
    'XGBoost': test_acc_xgb,
    'LightGBM': test_acc_lgb,
    'Voting': test_acc_voting
}

best_model_name = max(results, key=results.get)
best_model = models.get(best_model_name, voting)
best_accuracy = results[best_model_name]

print("\n" + "=" * 100)
print(f"âœ… ìµœê³  ëª¨ë¸: {best_model_name} - {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
print("=" * 100)

# ìµœì¢… ì˜ˆì¸¡
y_pred = best_model.predict(X_test_scaled)
y_pred_labels = le.inverse_transform(y_pred)
y_test_labels = le.inverse_transform(y_test)

# ============================================================================
# ìµœì¢… í‰ê°€
# ============================================================================
print("\n" + "=" * 100)
print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€")
print("=" * 100)

print(f"\nìµœì¢… ì •í™•ë„: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_labels, y_pred_labels,
                      labels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'])

print("\nConfusion Matrix:")
print("-" * 100)
print(f"{'':15} {'CANDIDATE':>12} {'CONFIRMED':>12} {'FALSE POS':>12}")
print("-" * 100)
for i, label in enumerate(['CANDIDATE', 'CONFIRMED', 'FALSE POS']):
    print(f"{label:15} {cm[i][0]:>12} {cm[i][1]:>12} {cm[i][2]:>12}")

print("\nClassification Report:")
print("-" * 100)
print(classification_report(y_test_labels, y_pred_labels,
                           labels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'],
                           target_names=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']))

# í´ë˜ìŠ¤ë³„ ì •í™•ë„
print("\ní´ë˜ìŠ¤ë³„ ì •í™•ë„:")
print("-" * 100)
for label in ['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']:
    mask = y_test_labels == label
    if mask.sum() > 0:
        class_acc = accuracy_score(y_test_labels[mask], y_pred_labels[mask])
        print(f"  {label:20} {class_acc:.4f} ({class_acc*100:.2f}%)  [{mask.sum():,}ê°œ ìƒ˜í”Œ]")

# ============================================================================
# ëª¨ë¸ ì €ì¥
# ============================================================================
print("\n" + "=" * 100)
print("ğŸ’¾ ëª¨ë¸ ì €ì¥")
print("=" * 100)

save_dir = 'saved_models'
os.makedirs(save_dir, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

model_path = os.path.join(save_dir, f'model_3class_{timestamp}.pkl')
scaler_path = os.path.join(save_dir, f'scaler_3class_{timestamp}.pkl')
encoder_path = os.path.join(save_dir, f'encoder_3class_{timestamp}.pkl')

joblib.dump(best_model, model_path)
joblib.dump(scaler, scaler_path)
joblib.dump(le, encoder_path)

config = {
    'model_name': best_model_name,
    'accuracy': best_accuracy,
    'timestamp': timestamp,
    'feature_count': X_full.shape[1],
    'classes': le.classes_.tolist()
}

config_path = os.path.join(save_dir, f'config_3class_{timestamp}.pkl')
joblib.dump(config, config_path)

print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
print(f"  â€¢ {model_path}")
print(f"  â€¢ {scaler_path}")
print(f"  â€¢ {encoder_path}")
print(f"  â€¢ {config_path}")

print("\n" + "=" * 100)
print("ğŸ¯ ìµœì¢… ê²°ê³¼")
print("=" * 100)
print(f"ëª¨ë¸: {best_model_name}")
print(f"ì •í™•ë„: {best_accuracy*100:.2f}%")

if best_accuracy >= 0.95:
    print("\nğŸ‰ğŸ‰ğŸ‰ 95% ëª©í‘œ ë‹¬ì„±! ğŸ‰ğŸ‰ğŸ‰")
elif best_accuracy >= 0.90:
    print(f"\nğŸ’ª 90% ì´ìƒ ë‹¬ì„±! ëª©í‘œê¹Œì§€ {(0.95-best_accuracy)*100:.2f}%p")
else:
    print(f"\nğŸ“Š í˜„ì¬ ìˆ˜ì¤€: {best_accuracy*100:.2f}% (ëª©í‘œ: 95.00%)")
    print(f"   ê°œì„  í•„ìš”: {(0.95-best_accuracy)*100:.2f}%p")

print("\n" + "=" * 100)
