"""
ë¹ ë¥¸ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (Fast Hybrid Model)
- 1ë‹¨ê³„: 92% ë‹¬ì„±í•œ 2-í´ë˜ìŠ¤ ëª¨ë¸ (CONFIRMED vs FALSE POSITIVE)
- 2ë‹¨ê³„: CANDIDATE íŒë³„ ëª¨ë¸ (í™•ì‹ ë„ ë‚®ì€ ì¼€ì´ìŠ¤ë§Œ)
- 26ê°œ í”¼ì²˜ ì‚¬ìš©, ë¹ ë¥¸ êµ¬í˜„, ë†’ì€ ì •í™•ë„ ëª©í‘œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier

import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("âš¡ ë¹ ë¥¸ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (92% ê¸°ë°˜ + CANDIDATE íŒë³„)")
print("=" * 100)

# ============================================================================
# ë°ì´í„° ë¡œë“œ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (92% ëª¨ë¸ê³¼ ë™ì¼)
# ============================================================================
print("\n[ë°ì´í„° ë¡œë“œ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§]")
print("-" * 100)

df = pd.read_csv('datasets/exoplanets.csv')
print(f"ì›ë³¸ ë°ì´í„°: {df.shape[0]:,} ìƒ˜í”Œ")

y_full = df['koi_disposition']
X_full = df.drop(['koi_disposition', 'kepoi_name'], axis=1, errors='ignore')

# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
numeric_cols = X_full.select_dtypes(include=[np.number]).columns
X_full = X_full[numeric_cols]
print(f"ê¸°ë³¸ í”¼ì²˜: {len(numeric_cols)}ê°œ")

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (92% ëª¨ë¸ ë™ì¼)
print("í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
X_full['planet_star_ratio'] = X_full['koi_prad'] / (X_full['koi_srad'] + 1e-10)
X_full['orbital_energy'] = 1.0 / (X_full['koi_sma'] + 1e-10)
X_full['transit_signal'] = X_full['koi_depth'] * X_full['koi_duration']
X_full['stellar_density'] = X_full['koi_smass'] / (X_full['koi_srad']**3 + 1e-10)
X_full['planet_density_proxy'] = X_full['koi_prad']**3 / (X_full['koi_sma']**2 + 1e-10)
X_full['log_period'] = np.log1p(X_full['koi_period'])
X_full['log_depth'] = np.log1p(X_full['koi_depth'])
X_full['log_insol'] = np.log1p(X_full['koi_insol'])
X_full['orbit_stability'] = X_full['koi_eccen'] * X_full['koi_impact']
X_full['transit_snr'] = X_full['koi_depth'] / (X_full['koi_duration'] + 1e-10)

X_full = X_full.replace([np.inf, -np.inf], np.nan)
X_full = X_full.fillna(X_full.median())

print(f"ìµœì¢… í”¼ì²˜ ìˆ˜: {X_full.shape[1]}ê°œ")

# ============================================================================
# 1ë‹¨ê³„: 92% 2-í´ë˜ìŠ¤ ëª¨ë¸ (CONFIRMED vs FALSE POSITIVE)
# ============================================================================
print("\n" + "=" * 100)
print("[1ë‹¨ê³„] 2-í´ë˜ìŠ¤ ëª¨ë¸: CONFIRMED vs FALSE POSITIVE (92% ëª©í‘œ)")
print("=" * 100)

# CANDIDATE ì œì™¸
y_binary = y_full[y_full.isin(['CONFIRMED', 'FALSE POSITIVE'])]
X_binary = X_full.loc[y_binary.index]

print(f"\n1ë‹¨ê³„ í•™ìŠµ ë°ì´í„°:")
print(f"  ì´ ìƒ˜í”Œ: {len(y_binary):,}")
for label, count in y_binary.value_counts().sort_index().items():
    print(f"  {label}: {count:,} ({count/len(y_binary)*100:.1f}%)")

# ë ˆì´ë¸” ì¸ì½”ë”©
y_binary_encoded = (y_binary == 'CONFIRMED').astype(int)

# Train/Test ë¶„í• 
X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(
    X_binary, y_binary_encoded, test_size=0.1, random_state=42, stratify=y_binary_encoded
)

print(f"\nTrain: {len(y_train_1):,} / Test: {len(y_test_1):,}")

# ìŠ¤ì¼€ì¼ë§
scaler_1 = StandardScaler()
X_train_1_scaled = scaler_1.fit_transform(X_train_1)
X_test_1_scaled = scaler_1.transform(X_test_1)

# ê°•í•œ ì •ê·œí™” ëª¨ë¸ (92% ë‹¬ì„±í•œ ì„¤ì •)
print("\n1ë‹¨ê³„ ëª¨ë¸ í•™ìŠµ ì¤‘...")

models_1 = {
    'XGBoost': XGBClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        eval_metric='logloss'
    ),
    'LightGBM': LGBMClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        verbose=-1
    ),
    'CatBoost': CatBoostClassifier(
        iterations=500,
        depth=4,
        learning_rate=0.02,
        l2_leaf_reg=10.0,
        bagging_temperature=1.0,
        random_state=42,
        verbose=False
    )
}

results_1 = {}

for name, model in models_1.items():
    print(f"\n{name} í•™ìŠµ ì¤‘...")
    model.fit(X_train_1_scaled, y_train_1)
    
    y_train_pred = model.predict(X_train_1_scaled)
    y_test_pred = model.predict(X_test_1_scaled)
    y_test_proba = model.predict_proba(X_test_1_scaled)
    
    train_acc = accuracy_score(y_train_1, y_train_pred)
    test_acc = accuracy_score(y_test_1, y_test_pred)
    cv_scores = cross_val_score(model, X_train_1_scaled, y_train_1, cv=5, scoring='accuracy')
    auc = roc_auc_score(y_test_1, y_test_proba[:, 1])
    
    results_1[name] = {
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'overfitting': train_acc - test_acc,
        'auc': auc,
        'y_test_proba': y_test_proba
    }
    
    print(f"  Train: {train_acc:.4f} | Test: {test_acc:.4f} | CV: {cv_scores.mean():.4f}Â±{cv_scores.std():.4f}")
    print(f"  ê³¼ì í•©: {train_acc - test_acc:.4f} ({(train_acc - test_acc)*100:.2f}%p) | AUC: {auc:.4f}")

# ì•™ìƒë¸”
print("\n1ë‹¨ê³„ ì•™ìƒë¸” (Soft Voting)...")
voting_1 = VotingClassifier(
    estimators=[(name, model) for name, model in models_1.items()],
    voting='soft'
)
voting_1.fit(X_train_1_scaled, y_train_1)

y_train_pred_v1 = voting_1.predict(X_train_1_scaled)
y_test_pred_v1 = voting_1.predict(X_test_1_scaled)
y_test_proba_v1 = voting_1.predict_proba(X_test_1_scaled)

train_acc_v1 = accuracy_score(y_train_1, y_train_pred_v1)
test_acc_v1 = accuracy_score(y_test_1, y_test_pred_v1)
cv_scores_v1 = cross_val_score(voting_1, X_train_1_scaled, y_train_1, cv=5, scoring='accuracy')
auc_v1 = roc_auc_score(y_test_1, y_test_proba_v1[:, 1])

print(f"  Train: {train_acc_v1:.4f} | Test: {test_acc_v1:.4f} | CV: {cv_scores_v1.mean():.4f}Â±{cv_scores_v1.std():.4f}")
print(f"  ê³¼ì í•©: {train_acc_v1 - test_acc_v1:.4f} ({(train_acc_v1 - test_acc_v1)*100:.2f}%p) | AUC: {auc_v1:.4f}")

results_1['Voting'] = {
    'model': voting_1,
    'train_acc': train_acc_v1,
    'test_acc': test_acc_v1,
    'cv_mean': cv_scores_v1.mean(),
    'cv_std': cv_scores_v1.std(),
    'overfitting': train_acc_v1 - test_acc_v1,
    'auc': auc_v1,
    'y_test_proba': y_test_proba_v1
}

best_1 = max(results_1.items(), key=lambda x: x[1]['test_acc'])
print(f"\nâœ… 1ë‹¨ê³„ ìµœê³ : {best_1[0]} - {best_1[1]['test_acc']:.4f} ({best_1[1]['test_acc']*100:.2f}%)")

# ============================================================================
# 2ë‹¨ê³„: CANDIDATE íŒë³„ ëª¨ë¸
# ============================================================================
print("\n" + "=" * 100)
print("[2ë‹¨ê³„] CANDIDATE íŒë³„ ëª¨ë¸")
print("=" * 100)

# ì „ì²´ ë°ì´í„°ì—ì„œ CANDIDATE ë ˆì´ë¸” ìƒì„±
y_candidate = (y_full == 'CANDIDATE').astype(int)
X_candidate = X_full.copy()

print(f"\n2ë‹¨ê³„ í•™ìŠµ ë°ì´í„°:")
print(f"  ì´ ìƒ˜í”Œ: {len(y_candidate):,}")
print(f"  CANDIDATE: {y_candidate.sum():,} ({y_candidate.sum()/len(y_candidate)*100:.1f}%)")
print(f"  NOT CANDIDATE: {(~y_candidate.astype(bool)).sum():,} ({(~y_candidate.astype(bool)).sum()/len(y_candidate)*100:.1f}%)")

# Train/Test ë¶„í• 
X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(
    X_candidate, y_candidate, test_size=0.1, random_state=42, stratify=y_candidate
)

print(f"\nTrain: {len(y_train_2):,} / Test: {len(y_test_2):,}")

# ìŠ¤ì¼€ì¼ë§
scaler_2 = StandardScaler()
X_train_2_scaled = scaler_2.fit_transform(X_train_2)
X_test_2_scaled = scaler_2.transform(X_test_2)

# ê°•í•œ ì •ê·œí™” ëª¨ë¸
print("\n2ë‹¨ê³„ ëª¨ë¸ í•™ìŠµ ì¤‘...")

models_2 = {
    'XGBoost': XGBClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        eval_metric='logloss'
    ),
    'LightGBM': LGBMClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        verbose=-1
    ),
    'CatBoost': CatBoostClassifier(
        iterations=500,
        depth=4,
        learning_rate=0.02,
        l2_leaf_reg=10.0,
        bagging_temperature=1.0,
        random_state=42,
        verbose=False
    )
}

results_2 = {}

for name, model in models_2.items():
    print(f"\n{name} í•™ìŠµ ì¤‘...")
    model.fit(X_train_2_scaled, y_train_2)
    
    y_train_pred = model.predict(X_train_2_scaled)
    y_test_pred = model.predict(X_test_2_scaled)
    y_test_proba = model.predict_proba(X_test_2_scaled)
    
    train_acc = accuracy_score(y_train_2, y_train_pred)
    test_acc = accuracy_score(y_test_2, y_test_pred)
    cv_scores = cross_val_score(model, X_train_2_scaled, y_train_2, cv=5, scoring='accuracy')
    auc = roc_auc_score(y_test_2, y_test_proba[:, 1])
    
    results_2[name] = {
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'overfitting': train_acc - test_acc,
        'auc': auc,
        'y_test_proba': y_test_proba
    }
    
    print(f"  Train: {train_acc:.4f} | Test: {test_acc:.4f} | CV: {cv_scores.mean():.4f}Â±{cv_scores.std():.4f}")
    print(f"  ê³¼ì í•©: {train_acc - test_acc:.4f} ({(train_acc - test_acc)*100:.2f}%p) | AUC: {auc:.4f}")

# ì•™ìƒë¸”
print("\n2ë‹¨ê³„ ì•™ìƒë¸” (Soft Voting)...")
voting_2 = VotingClassifier(
    estimators=[(name, model) for name, model in models_2.items()],
    voting='soft'
)
voting_2.fit(X_train_2_scaled, y_train_2)

y_train_pred_v2 = voting_2.predict(X_train_2_scaled)
y_test_pred_v2 = voting_2.predict(X_test_2_scaled)
y_test_proba_v2 = voting_2.predict_proba(X_test_2_scaled)

train_acc_v2 = accuracy_score(y_train_2, y_train_pred_v2)
test_acc_v2 = accuracy_score(y_test_2, y_test_pred_v2)
cv_scores_v2 = cross_val_score(voting_2, X_train_2_scaled, y_train_2, cv=5, scoring='accuracy')
auc_v2 = roc_auc_score(y_test_2, y_test_proba_v2[:, 1])

print(f"  Train: {train_acc_v2:.4f} | Test: {test_acc_v2:.4f} | CV: {cv_scores_v2.mean():.4f}Â±{cv_scores_v2.std():.4f}")
print(f"  ê³¼ì í•©: {train_acc_v2 - test_acc_v2:.4f} ({(train_acc_v2 - test_acc_v2)*100:.2f}%p) | AUC: {auc_v2:.4f}")

results_2['Voting'] = {
    'model': voting_2,
    'train_acc': train_acc_v2,
    'test_acc': test_acc_v2,
    'cv_mean': cv_scores_v2.mean(),
    'cv_std': cv_scores_v2.std(),
    'overfitting': train_acc_v2 - test_acc_v2,
    'auc': auc_v2,
    'y_test_proba': y_test_proba_v2
}

best_2 = max(results_2.items(), key=lambda x: x[1]['test_acc'])
print(f"\nâœ… 2ë‹¨ê³„ ìµœê³ : {best_2[0]} - {best_2[1]['test_acc']:.4f} ({best_2[1]['test_acc']*100:.2f}%)")

# ============================================================================
# íŒŒì´í”„ë¼ì¸ í†µí•© ë° í™•ì‹ ë„ ì„ê³„ê°’ ìµœì í™”
# ============================================================================
print("\n" + "=" * 100)
print("[íŒŒì´í”„ë¼ì¸ í†µí•©] í™•ì‹ ë„ ì„ê³„ê°’ ìµœì í™”")
print("=" * 100)

# ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
y_test_full = y_full.loc[X_test_2.index]
X_test_full_scaled_1 = scaler_1.transform(X_test_2)
X_test_full_scaled_2 = scaler_2.transform(X_test_2)

# 1ë‹¨ê³„ ì˜ˆì¸¡
stage1_proba = results_1['Voting']['model'].predict_proba(X_test_full_scaled_1)
stage1_pred = results_1['Voting']['model'].predict(X_test_full_scaled_1)

# 2ë‹¨ê³„ ì˜ˆì¸¡
stage2_proba = results_2['Voting']['model'].predict_proba(X_test_full_scaled_2)
stage2_pred = results_2['Voting']['model'].predict(X_test_full_scaled_2)

# í™•ì‹ ë„ ì„ê³„ê°’ ìµœì í™”
thresholds = np.arange(0.80, 0.96, 0.02)
threshold_results = []

print(f"\n{'ì„ê³„ê°’':<10} {'1ë‹¨ê³„ì‚¬ìš©':<12} {'CANDIDATE':<12} {'ìµœì¢…ì •í™•ë„':<12}")
print("-" * 100)

for threshold in thresholds:
    final_predictions = np.empty(len(y_test_full), dtype=object)
    
    # 1ë‹¨ê³„ í™•ì‹ ë„ê°€ ë†’ì€ ì¼€ì´ìŠ¤
    high_conf_mask = (stage1_proba.max(axis=1) >= threshold)
    
    # 1ë‹¨ê³„ ê³ í™•ì‹ ë„ â†’ CONFIRMED or FALSE POSITIVE
    final_predictions[high_conf_mask] = np.where(
        stage1_pred[high_conf_mask] == 1,
        'CONFIRMED',
        'FALSE POSITIVE'
    )
    
    # 1ë‹¨ê³„ ì €í™•ì‹ ë„ â†’ CANDIDATE íŒë³„ ì‚¬ìš©
    low_conf_mask = ~high_conf_mask
    final_predictions[low_conf_mask] = np.where(
        stage2_pred[low_conf_mask] == 1,
        'CANDIDATE',
        np.where(stage1_pred[low_conf_mask] == 1, 'CONFIRMED', 'FALSE POSITIVE')
    )
    
    accuracy = accuracy_score(y_test_full, final_predictions)
    stage1_ratio = high_conf_mask.sum() / len(y_test_full) * 100
    candidate_count = (final_predictions == 'CANDIDATE').sum()
    
    threshold_results.append({
        'threshold': threshold,
        'accuracy': accuracy,
        'stage1_ratio': stage1_ratio,
        'candidate_count': candidate_count
    })
    
    print(f"{threshold:.2f}      {stage1_ratio:>6.1f}%      {candidate_count:>4}ê°œ       {accuracy:.4f}")

# ìµœì  ì„ê³„ê°’
best_result = max(threshold_results, key=lambda x: x['accuracy'])
best_threshold = best_result['threshold']
best_accuracy = best_result['accuracy']

print(f"\nâœ… ìµœì  ì„ê³„ê°’: {best_threshold:.2f}")
print(f"âœ… ìµœì¢… ì •í™•ë„: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
print(f"   1ë‹¨ê³„ ì‚¬ìš©: {best_result['stage1_ratio']:.1f}%")
print(f"   CANDIDATE íŒë³„: {best_result['candidate_count']}ê°œ")

# ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
high_conf_mask = (stage1_proba.max(axis=1) >= best_threshold)
final_predictions = np.empty(len(y_test_full), dtype=object)

final_predictions[high_conf_mask] = np.where(
    stage1_pred[high_conf_mask] == 1,
    'CONFIRMED',
    'FALSE POSITIVE'
)

low_conf_mask = ~high_conf_mask
final_predictions[low_conf_mask] = np.where(
    stage2_pred[low_conf_mask] == 1,
    'CANDIDATE',
    np.where(stage1_pred[low_conf_mask] == 1, 'CONFIRMED', 'FALSE POSITIVE')
)

# ============================================================================
# ê³¼ì í•© ë¶„ì„ ë° ìµœì¢… í‰ê°€
# ============================================================================
print("\n" + "=" * 100)
print("[ê³¼ì í•© ë¶„ì„]")
print("=" * 100)

print("\n1ë‹¨ê³„ ëª¨ë¸ ê³¼ì í•©:")
for name, r in results_1.items():
    status = "âœ…" if r['overfitting'] < 0.03 else "âš ï¸"
    print(f"{status} {name:15} {r['overfitting']:>7.4f} ({r['overfitting']*100:>5.2f}%p)")

print("\n2ë‹¨ê³„ ëª¨ë¸ ê³¼ì í•©:")
for name, r in results_2.items():
    status = "âœ…" if r['overfitting'] < 0.03 else "âš ï¸"
    print(f"{status} {name:15} {r['overfitting']:>7.4f} ({r['overfitting']*100:>5.2f}%p)")

# Confusion Matrix
print("\n" + "=" * 100)
print("[ìµœì¢… ì„±ëŠ¥ í‰ê°€]")
print("=" * 100)

cm = confusion_matrix(y_test_full, final_predictions,
                      labels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'])

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'],
            yticklabels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'])
plt.title(f'Confusion Matrix (ë¹ ë¥¸ í•˜ì´ë¸Œë¦¬ë“œ) - ì •í™•ë„: {best_accuracy:.4f}')
plt.ylabel('ì‹¤ì œ')
plt.xlabel('ì˜ˆì¸¡')
plt.tight_layout()
plt.savefig('confusion_matrix_fast_hybrid.png', dpi=150, bbox_inches='tight')
print("\nâœ… Confusion Matrix ì €ì¥: confusion_matrix_fast_hybrid.png")

print("\nClassification Report:")
print(classification_report(y_test_full, final_predictions,
                           labels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'],
                           target_names=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']))

print("\ní´ë˜ìŠ¤ë³„ ì •í™•ë„:")
for label in ['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']:
    mask = y_test_full == label
    if mask.sum() > 0:
        class_acc = accuracy_score(y_test_full[mask], final_predictions[mask])
        print(f"  {label:20} {class_acc:.4f} ({class_acc*100:.2f}%)  [{mask.sum()}ê°œ]")

# ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
print("\n" + "=" * 100)
print("ğŸ¯ ëª©í‘œ ë‹¬ì„± í‰ê°€")
print("=" * 100)
print(f"ëª©í‘œ: 95.00%")
print(f"ë‹¬ì„±: {best_accuracy*100:.2f}%")
print(f"ê²©ì°¨: {(best_accuracy - 0.95)*100:+.2f}%p")

if best_accuracy >= 0.95:
    print("\nğŸ‰ğŸ‰ğŸ‰ 95% ëª©í‘œ ë‹¬ì„±! ğŸ‰ğŸ‰ğŸ‰")
elif best_accuracy >= 0.90:
    print(f"\nğŸ’ª 90% ì´ìƒ ë‹¬ì„±! ì¶”ê°€ {(0.95 - best_accuracy)*100:.2f}%p í•„ìš”")
else:
    print(f"\nğŸ“Š ì¶”ê°€ ê°œì„  í•„ìš”: {(0.95 - best_accuracy)*100:.2f}%p")

print("\n" + "=" * 100)
print("âœ… ë¶„ì„ ì™„ë£Œ")
print("=" * 100)
