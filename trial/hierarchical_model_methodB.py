"""
ë°©ë²• B: í™•ì‹ ë„ ê¸°ë°˜ ê³„ì¸µì  ë¶„ë¥˜ ëª¨ë¸
- 1ë‹¨ê³„: CONFIRMED vs FALSE POSITIVE (CANDIDATE ì œì™¸, 92% ì •í™•ë„ ëª©í‘œ)
- 2ë‹¨ê³„: 3-í´ë˜ìŠ¤ (CANDIDATE í¬í•¨, ë‚®ì€ í™•ì‹ ë„ ì¼€ì´ìŠ¤ ì²˜ë¦¬)
- ë‹¤ì¤‘ê³µì„ ì„± ì œê±° ì ìš©
- ê³¼ì í•© ë°©ì§€ ë° 95% ì •í™•ë„ ëª©í‘œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, 
                             roc_auc_score, roc_curve)
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC

from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("ë°©ë²• B: í™•ì‹ ë„ ê¸°ë°˜ ê³„ì¸µì  ë¶„ë¥˜ ì‹œìŠ¤í…œ")
print("=" * 100)

# ============================================================================
# STEP 1: ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ë° ì œê±°
# ============================================================================
print("\n[STEP 1] ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ë° ì œê±°")
print("-" * 100)

df = pd.read_csv('datasets/exoplanets.csv')
print(f"ì›ë³¸ ë°ì´í„°: {df.shape[0]:,} ìƒ˜í”Œ, {df.shape[1]-1} í”¼ì²˜")

# íƒ€ê²Ÿ ë¶„ë¦¬
y_full = df['koi_disposition']
X_full = df.drop(['koi_disposition', 'kepoi_name'], axis=1, errors='ignore')

# NaN ì œê±°
valid_idx = y_full.notna()
X_full = X_full[valid_idx]
y_full = y_full[valid_idx]

print(f"NaN ì œê±° í›„: {len(y_full):,} ìƒ˜í”Œ")

# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ (ë¬¸ìì—´ ì»¬ëŸ¼ ì œì™¸)
numeric_cols = X_full.select_dtypes(include=[np.number]).columns
X_full = X_full[numeric_cols]
print(f"ìˆ˜ì¹˜í˜• í”¼ì²˜ë§Œ ì„ íƒ: {len(numeric_cols)}ê°œ")

# ============================================================================
# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (92% ë‹¬ì„± ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ í”¼ì²˜ë“¤)
# ============================================================================
print("\ní”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")

# 1. í–‰ì„±-í•­ì„± ë¹„ìœ¨ (Planet-Star Radius Ratio)
X_full['planet_star_ratio'] = X_full['koi_prad'] / X_full['koi_srad']

# 2. ê¶¤ë„ ì—ë„ˆì§€ (Orbital Energy) - ë°˜ì¥ì¶•ì˜ ì—­ìˆ˜
X_full['orbital_energy'] = 1.0 / (X_full['koi_sma'] + 1e-10)

# 3. í†µê³¼ ì‹ í˜¸ ê°•ë„ (Transit Signal Strength)
X_full['transit_signal'] = X_full['koi_depth'] * X_full['koi_duration']

# 4. í•­ì„± ë°€ë„ (Stellar Density)
X_full['stellar_density'] = X_full['koi_smass'] / (X_full['koi_srad']**3 + 1e-10)

# 5. í–‰ì„± ë°€ë„ í”„ë¡ì‹œ (Planet Density Proxy)
X_full['planet_density_proxy'] = X_full['koi_prad']**3 / (X_full['koi_sma']**2 + 1e-10)

# 6. Log ë³€í™˜ (ì™œë„ ê°ì†Œ)
X_full['log_period'] = np.log1p(X_full['koi_sma'])
X_full['log_depth'] = np.log1p(X_full['koi_depth'])
X_full['log_insol'] = np.log1p(X_full['koi_insol'])

# 7. ê¶¤ë„ ì•ˆì •ì„± ì§€í‘œ (ì´ì‹¬ë¥ ê³¼ ì¶©ëŒ íŒŒë¼ë¯¸í„°ì˜ ì¡°í•©)
X_full['orbit_stability'] = X_full['koi_eccen'] * X_full['koi_impact']

# NaN ì²˜ë¦¬ (ë¬´í•œëŒ€ë‚˜ NaNì´ ìƒì„±ëœ ê²½ìš°)
X_full = X_full.replace([np.inf, -np.inf], np.nan)
X_full = X_full.fillna(X_full.median())

print(f"ì—”ì§€ë‹ˆì–´ë§ í›„ ì´ í”¼ì²˜: {X_full.shape[1]}ê°œ")
print(f"ì¶”ê°€ëœ í”¼ì²˜: planet_star_ratio, orbital_energy, transit_signal, stellar_density,")
print(f"            planet_density_proxy, log_period, log_depth, log_insol, orbit_stability")

# ìƒê´€ê³„ìˆ˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
correlation_matrix = X_full.corr().abs()

# ìƒê´€ê³„ìˆ˜ 0.8 ì´ìƒì¸ í”¼ì²˜ ìŒ ì°¾ê¸°
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if correlation_matrix.iloc[i, j] >= 0.8:
            high_corr_pairs.append({
                'feature1': correlation_matrix.columns[i],
                'feature2': correlation_matrix.columns[j],
                'correlation': correlation_matrix.iloc[i, j]
            })

print(f"\në‹¤ì¤‘ê³µì„ ì„± ë°œê²¬ (ìƒê´€ê³„ìˆ˜ â‰¥ 0.8): {len(high_corr_pairs)}ê°œ ìŒ")
for pair in high_corr_pairs:
    print(f"  {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}")

# ë‹¤ì¤‘ê³µì„ ì„± ì œê±° ì „ëµ: ê° ìŒì—ì„œ íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ í”¼ì²˜ ì œê±°
features_to_drop = set()

# 2-í´ë˜ìŠ¤ íƒ€ê²Ÿ ìƒì„± (ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ìš©)
y_binary = y_full.copy()
y_binary = y_binary[y_binary.isin(['CONFIRMED', 'FALSE POSITIVE'])]
X_binary_temp = X_full.loc[y_binary.index]
y_binary_encoded = (y_binary == 'CONFIRMED').astype(int)

for pair in high_corr_pairs:
    feat1, feat2 = pair['feature1'], pair['feature2']
    
    # ê° í”¼ì²˜ì™€ íƒ€ê²Ÿì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    corr1 = abs(X_binary_temp[feat1].corr(y_binary_encoded))
    corr2 = abs(X_binary_temp[feat2].corr(y_binary_encoded))
    
    # íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ í”¼ì²˜ ì œê±°
    if corr1 < corr2:
        features_to_drop.add(feat1)
    else:
        features_to_drop.add(feat2)

print(f"\nì œê±°í•  í”¼ì²˜ ({len(features_to_drop)}ê°œ):")
for feat in sorted(features_to_drop):
    print(f"  - {feat}")

# í”¼ì²˜ ì œê±°
X_reduced = X_full.drop(columns=list(features_to_drop))
print(f"\nìµœì¢… í”¼ì²˜ ìˆ˜: {X_reduced.shape[1]}ê°œ")
print(f"ì‚¬ìš©í•  í”¼ì²˜: {', '.join(X_reduced.columns.tolist())}")

# ============================================================================
# STEP 2: 1ë‹¨ê³„ ëª¨ë¸ - 2-í´ë˜ìŠ¤ ë¶„ë¥˜ (CONFIRMED vs FALSE POSITIVE)
# ============================================================================
print("\n" + "=" * 100)
print("[STEP 2] 1ë‹¨ê³„ ëª¨ë¸: 2-í´ë˜ìŠ¤ ë¶„ë¥˜ (CONFIRMED vs FALSE POSITIVE)")
print("=" * 100)

# CANDIDATE ì œì™¸
y_stage1 = y_full[y_full.isin(['CONFIRMED', 'FALSE POSITIVE'])]
X_stage1 = X_reduced.loc[y_stage1.index]

print(f"\n1ë‹¨ê³„ í•™ìŠµ ë°ì´í„°:")
print(f"  ì´ ìƒ˜í”Œ: {len(y_stage1):,}")
for label, count in y_stage1.value_counts().sort_index().items():
    print(f"  {label}: {count:,} ({count/len(y_stage1)*100:.1f}%)")

# ë ˆì´ë¸” ì¸ì½”ë”© (CONFIRMED=1, FALSE POSITIVE=0)
y_stage1_encoded = (y_stage1 == 'CONFIRMED').astype(int)

# Train/Test ë¶„í• 
X_train_s1, X_test_s1, y_train_s1, y_test_s1 = train_test_split(
    X_stage1, y_stage1_encoded, test_size=0.1, random_state=42, stratify=y_stage1_encoded
)

print(f"\nTrain: {len(y_train_s1):,} / Test: {len(y_test_s1):,}")

# ìŠ¤ì¼€ì¼ë§
scaler_s1 = StandardScaler()
X_train_s1_scaled = scaler_s1.fit_transform(X_train_s1)
X_test_s1_scaled = scaler_s1.transform(X_test_s1)

# ê°•í•œ ì •ê·œí™” ì ìš© - ê³¼ì í•© ë°©ì§€
print("\n1ë‹¨ê³„ ëª¨ë¸ í•™ìŠµ ì¤‘ (ê°•í•œ ì •ê·œí™” ì ìš©)...")

models_stage1 = {
    'XGBoost': XGBClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        eval_metric='logloss'
    ),
    'LightGBM': LGBMClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        verbose=-1
    ),
    'CatBoost': CatBoostClassifier(
        iterations=500,
        depth=4,
        learning_rate=0.02,
        l2_leaf_reg=10.0,
        bagging_temperature=1.0,
        random_state=42,
        verbose=False
    )
}

results_stage1 = {}

for name, model in models_stage1.items():
    print(f"\n{name} í•™ìŠµ ì¤‘...")
    
    # í•™ìŠµ
    model.fit(X_train_s1_scaled, y_train_s1)
    
    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train_s1_scaled)
    y_test_pred = model.predict(X_test_s1_scaled)
    
    # í™•ì‹ ë„ (probability)
    y_test_proba = model.predict_proba(X_test_s1_scaled)
    
    # ì •í™•ë„
    train_acc = accuracy_score(y_train_s1, y_train_pred)
    test_acc = accuracy_score(y_test_s1, y_test_pred)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train_s1_scaled, y_train_s1, cv=5, scoring='accuracy')
    
    # AUC
    auc = roc_auc_score(y_test_s1, y_test_proba[:, 1])
    
    results_stage1[name] = {
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'overfitting': train_acc - test_acc,
        'auc': auc,
        'y_test_proba': y_test_proba
    }
    
    print(f"  Train ì •í™•ë„: {train_acc:.4f}")
    print(f"  Test ì •í™•ë„:  {test_acc:.4f}")
    print(f"  CV ì •í™•ë„:    {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    print(f"  ê³¼ì í•©:       {train_acc - test_acc:.4f} ({(train_acc - test_acc)*100:.2f}%p)")
    print(f"  AUC:          {auc:.4f}")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
best_model_s1 = max(results_stage1.items(), key=lambda x: x[1]['test_acc'])
print(f"\n1ë‹¨ê³„ ìµœê³  ëª¨ë¸: {best_model_s1[0]} - Test ì •í™•ë„: {best_model_s1[1]['test_acc']:.4f}")

# ì•™ìƒë¸” (Voting)
print("\n1ë‹¨ê³„ ì•™ìƒë¸” (Soft Voting) í•™ìŠµ ì¤‘...")
voting_s1 = VotingClassifier(
    estimators=[(name, model) for name, model in models_stage1.items()],
    voting='soft'
)
voting_s1.fit(X_train_s1_scaled, y_train_s1)

y_train_pred_voting = voting_s1.predict(X_train_s1_scaled)
y_test_pred_voting = voting_s1.predict(X_test_s1_scaled)
y_test_proba_voting = voting_s1.predict_proba(X_test_s1_scaled)

train_acc_voting = accuracy_score(y_train_s1, y_train_pred_voting)
test_acc_voting = accuracy_score(y_test_s1, y_test_pred_voting)
cv_scores_voting = cross_val_score(voting_s1, X_train_s1_scaled, y_train_s1, cv=5, scoring='accuracy')
auc_voting = roc_auc_score(y_test_s1, y_test_proba_voting[:, 1])

print(f"  Train ì •í™•ë„: {train_acc_voting:.4f}")
print(f"  Test ì •í™•ë„:  {test_acc_voting:.4f}")
print(f"  CV ì •í™•ë„:    {cv_scores_voting.mean():.4f} Â± {cv_scores_voting.std():.4f}")
print(f"  ê³¼ì í•©:       {train_acc_voting - test_acc_voting:.4f} ({(train_acc_voting - test_acc_voting)*100:.2f}%p)")
print(f"  AUC:          {auc_voting:.4f}")

results_stage1['Voting'] = {
    'model': voting_s1,
    'train_acc': train_acc_voting,
    'test_acc': test_acc_voting,
    'cv_mean': cv_scores_voting.mean(),
    'cv_std': cv_scores_voting.std(),
    'overfitting': train_acc_voting - test_acc_voting,
    'auc': auc_voting,
    'y_test_proba': y_test_proba_voting
}

# ============================================================================
# STEP 3: 2ë‹¨ê³„ ëª¨ë¸ - 3-í´ë˜ìŠ¤ ë¶„ë¥˜ (CANDIDATE í¬í•¨)
# ============================================================================
print("\n" + "=" * 100)
print("[STEP 3] 2ë‹¨ê³„ ëª¨ë¸: 3-í´ë˜ìŠ¤ ë¶„ë¥˜ (CANDIDATE í¬í•¨)")
print("=" * 100)

# ì „ì²´ ë°ì´í„° ì‚¬ìš©
y_stage2 = y_full.copy()
X_stage2 = X_reduced.copy()

print(f"\n2ë‹¨ê³„ í•™ìŠµ ë°ì´í„°:")
print(f"  ì´ ìƒ˜í”Œ: {len(y_stage2):,}")
for label, count in y_stage2.value_counts().sort_index().items():
    print(f"  {label}: {count:,} ({count/len(y_stage2)*100:.1f}%)")

# ë ˆì´ë¸” ì¸ì½”ë”©
label_encoder = LabelEncoder()
y_stage2_encoded = label_encoder.fit_transform(y_stage2)

print(f"\nì¸ì½”ë”©ëœ ë ˆì´ë¸”:")
for i, label in enumerate(label_encoder.classes_):
    print(f"  {label} â†’ {i}")

# Train/Test ë¶„í• 
X_train_s2, X_test_s2, y_train_s2, y_test_s2 = train_test_split(
    X_stage2, y_stage2_encoded, test_size=0.1, random_state=42, stratify=y_stage2_encoded
)

print(f"\nTrain: {len(y_train_s2):,} / Test: {len(y_test_s2):,}")

# ìŠ¤ì¼€ì¼ë§
scaler_s2 = StandardScaler()
X_train_s2_scaled = scaler_s2.fit_transform(X_train_s2)
X_test_s2_scaled = scaler_s2.transform(X_test_s2)

# ê°•í•œ ì •ê·œí™” ì ìš©
print("\n2ë‹¨ê³„ ëª¨ë¸ í•™ìŠµ ì¤‘ (ê°•í•œ ì •ê·œí™” ì ìš©)...")

models_stage2 = {
    'XGBoost': XGBClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        eval_metric='mlogloss'
    ),
    'LightGBM': LGBMClassifier(
        n_estimators=500,
        max_depth=4,
        learning_rate=0.02,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=2.0,
        reg_lambda=10.0,
        random_state=42,
        verbose=-1
    ),
    'CatBoost': CatBoostClassifier(
        iterations=500,
        depth=4,
        learning_rate=0.02,
        l2_leaf_reg=10.0,
        bagging_temperature=1.0,
        random_state=42,
        verbose=False
    )
}

results_stage2 = {}

for name, model in models_stage2.items():
    print(f"\n{name} í•™ìŠµ ì¤‘...")
    
    # í•™ìŠµ
    model.fit(X_train_s2_scaled, y_train_s2)
    
    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train_s2_scaled)
    y_test_pred = model.predict(X_test_s2_scaled)
    
    # í™•ì‹ ë„
    y_test_proba = model.predict_proba(X_test_s2_scaled)
    
    # ì •í™•ë„
    train_acc = accuracy_score(y_train_s2, y_train_pred)
    test_acc = accuracy_score(y_test_s2, y_test_pred)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train_s2_scaled, y_train_s2, cv=5, scoring='accuracy')
    
    results_stage2[name] = {
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'overfitting': train_acc - test_acc,
        'y_test_proba': y_test_proba
    }
    
    print(f"  Train ì •í™•ë„: {train_acc:.4f}")
    print(f"  Test ì •í™•ë„:  {test_acc:.4f}")
    print(f"  CV ì •í™•ë„:    {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    print(f"  ê³¼ì í•©:       {train_acc - test_acc:.4f} ({(train_acc - test_acc)*100:.2f}%p)")

# ì•™ìƒë¸”
print("\n2ë‹¨ê³„ ì•™ìƒë¸” (Soft Voting) í•™ìŠµ ì¤‘...")
voting_s2 = VotingClassifier(
    estimators=[(name, model) for name, model in models_stage2.items()],
    voting='soft'
)
voting_s2.fit(X_train_s2_scaled, y_train_s2)

y_train_pred_voting = voting_s2.predict(X_train_s2_scaled)
y_test_pred_voting = voting_s2.predict(X_test_s2_scaled)
y_test_proba_voting = voting_s2.predict_proba(X_test_s2_scaled)

train_acc_voting = accuracy_score(y_train_s2, y_train_pred_voting)
test_acc_voting = accuracy_score(y_test_s2, y_test_pred_voting)
cv_scores_voting = cross_val_score(voting_s2, X_train_s2_scaled, y_train_s2, cv=5, scoring='accuracy')

print(f"  Train ì •í™•ë„: {train_acc_voting:.4f}")
print(f"  Test ì •í™•ë„:  {test_acc_voting:.4f}")
print(f"  CV ì •í™•ë„:    {cv_scores_voting.mean():.4f} Â± {cv_scores_voting.std():.4f}")
print(f"  ê³¼ì í•©:       {train_acc_voting - test_acc_voting:.4f} ({(train_acc_voting - test_acc_voting)*100:.2f}%p)")

results_stage2['Voting'] = {
    'model': voting_s2,
    'train_acc': train_acc_voting,
    'test_acc': test_acc_voting,
    'cv_mean': cv_scores_voting.mean(),
    'cv_std': cv_scores_voting.std(),
    'overfitting': train_acc_voting - test_acc_voting,
    'y_test_proba': y_test_proba_voting
}

# ============================================================================
# STEP 4: íŒŒì´í”„ë¼ì¸ í†µí•© ë° í™•ì‹ ë„ ì„ê³„ê°’ ìµœì í™”
# ============================================================================
print("\n" + "=" * 100)
print("[STEP 4] íŒŒì´í”„ë¼ì¸ í†µí•© ë° í™•ì‹ ë„ ì„ê³„ê°’ ìµœì í™”")
print("=" * 100)

# ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
y_test_full = y_full.loc[X_test_s2.index]
X_test_full_scaled = scaler_s2.transform(X_test_s2)

# 1ë‹¨ê³„ ëª¨ë¸ë¡œ ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (2-í´ë˜ìŠ¤)
best_stage1_model = results_stage1['Voting']['model']
stage1_proba = best_stage1_model.predict_proba(X_test_full_scaled)

# ê° í™•ì‹ ë„ ì„ê³„ê°’ë³„ ì„±ëŠ¥ í‰ê°€
thresholds = np.arange(0.85, 0.96, 0.01)
threshold_results = []

print("\ní™•ì‹ ë„ ì„ê³„ê°’ ìµœì í™” ì¤‘...")
print(f"{'ì„ê³„ê°’':<10} {'1ë‹¨ê³„ ì‚¬ìš©':<12} {'2ë‹¨ê³„ ì‚¬ìš©':<12} {'ìµœì¢… ì •í™•ë„':<12} {'ìƒì„¸'}")
print("-" * 100)

for threshold in thresholds:
    # í™•ì‹ ë„ê°€ ë†’ì€ ìƒ˜í”Œ (1ë‹¨ê³„ ê²°ê³¼ ì‚¬ìš©)
    high_confidence_mask = (stage1_proba.max(axis=1) >= threshold)
    
    # í™•ì‹ ë„ê°€ ë‚®ì€ ìƒ˜í”Œ (2ë‹¨ê³„ ê²°ê³¼ ì‚¬ìš©)
    low_confidence_mask = ~high_confidence_mask
    
    # ìµœì¢… ì˜ˆì¸¡
    final_predictions = np.empty(len(y_test_full), dtype=object)
    
    # 1ë‹¨ê³„ ê³ í™•ì‹ ë„ ì˜ˆì¸¡
    stage1_pred = best_stage1_model.predict(X_test_full_scaled[high_confidence_mask])
    final_predictions[high_confidence_mask] = ['CONFIRMED' if p == 1 else 'FALSE POSITIVE' 
                                                 for p in stage1_pred]
    
    # 2ë‹¨ê³„ ì €í™•ì‹ ë„ ì˜ˆì¸¡
    stage2_pred = results_stage2['Voting']['model'].predict(X_test_full_scaled[low_confidence_mask])
    final_predictions[low_confidence_mask] = label_encoder.inverse_transform(stage2_pred)
    
    # ì •í™•ë„ ê³„ì‚°
    accuracy = accuracy_score(y_test_full, final_predictions)
    
    # 1ë‹¨ê³„/2ë‹¨ê³„ ì‚¬ìš© ë¹„ìœ¨
    stage1_ratio = high_confidence_mask.sum() / len(y_test_full) * 100
    stage2_ratio = low_confidence_mask.sum() / len(y_test_full) * 100
    
    threshold_results.append({
        'threshold': threshold,
        'accuracy': accuracy,
        'stage1_ratio': stage1_ratio,
        'stage2_ratio': stage2_ratio,
        'stage1_count': high_confidence_mask.sum(),
        'stage2_count': low_confidence_mask.sum()
    })
    
    print(f"{threshold:.2f}      {stage1_ratio:>5.1f}%      {stage2_ratio:>5.1f}%      "
          f"{accuracy:.4f}      (1ë‹¨ê³„:{high_confidence_mask.sum()}, 2ë‹¨ê³„:{low_confidence_mask.sum()})")

# ìµœì  ì„ê³„ê°’ ì„ íƒ
best_threshold_result = max(threshold_results, key=lambda x: x['accuracy'])
best_threshold = best_threshold_result['threshold']
best_accuracy = best_threshold_result['accuracy']

print("\n" + "=" * 100)
print(f"ìµœì  í™•ì‹ ë„ ì„ê³„ê°’: {best_threshold:.2f}")
print(f"ìµœì¢… ì •í™•ë„: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
print(f"1ë‹¨ê³„ ì‚¬ìš©: {best_threshold_result['stage1_ratio']:.1f}% ({best_threshold_result['stage1_count']}ê°œ)")
print(f"2ë‹¨ê³„ ì‚¬ìš©: {best_threshold_result['stage2_ratio']:.1f}% ({best_threshold_result['stage2_count']}ê°œ)")
print("=" * 100)

# ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
high_confidence_mask = (stage1_proba.max(axis=1) >= best_threshold)
low_confidence_mask = ~high_confidence_mask

final_predictions = np.empty(len(y_test_full), dtype=object)
stage1_pred = best_stage1_model.predict(X_test_full_scaled[high_confidence_mask])
final_predictions[high_confidence_mask] = ['CONFIRMED' if p == 1 else 'FALSE POSITIVE' 
                                             for p in stage1_pred]
stage2_pred = results_stage2['Voting']['model'].predict(X_test_full_scaled[low_confidence_mask])
final_predictions[low_confidence_mask] = label_encoder.inverse_transform(stage2_pred)

# ============================================================================
# STEP 5: ê³¼ì í•© ë¶„ì„
# ============================================================================
print("\n" + "=" * 100)
print("[STEP 5] ê³¼ì í•© ë¶„ì„")
print("=" * 100)

print("\n1ë‹¨ê³„ ëª¨ë¸ (2-í´ë˜ìŠ¤) ê³¼ì í•© ë¶„ì„:")
print("-" * 100)
for name, result in results_stage1.items():
    print(f"{name:20} Train: {result['train_acc']:.4f}  Test: {result['test_acc']:.4f}  "
          f"ê³¼ì í•©: {result['overfitting']:.4f} ({result['overfitting']*100:.2f}%p)")

print("\n2ë‹¨ê³„ ëª¨ë¸ (3-í´ë˜ìŠ¤) ê³¼ì í•© ë¶„ì„:")
print("-" * 100)
for name, result in results_stage2.items():
    print(f"{name:20} Train: {result['train_acc']:.4f}  Test: {result['test_acc']:.4f}  "
          f"ê³¼ì í•©: {result['overfitting']:.4f} ({result['overfitting']*100:.2f}%p)")

# ê³¼ì í•© íŒì •
overfitting_threshold = 0.03  # 3%p ì´ìƒì´ë©´ ê³¼ì í•©
print(f"\nê³¼ì í•© íŒì • ê¸°ì¤€: {overfitting_threshold*100:.1f}%p ì´ìƒ")

stage1_overfitting = [(name, r['overfitting']) for name, r in results_stage1.items() 
                      if r['overfitting'] > overfitting_threshold]
stage2_overfitting = [(name, r['overfitting']) for name, r in results_stage2.items() 
                      if r['overfitting'] > overfitting_threshold]

if stage1_overfitting:
    print(f"\nâš ï¸  1ë‹¨ê³„ ê³¼ì í•© ëª¨ë¸ ({len(stage1_overfitting)}ê°œ):")
    for name, ovf in stage1_overfitting:
        print(f"  - {name}: {ovf*100:.2f}%p")
else:
    print("\nâœ… 1ë‹¨ê³„ ëª¨ë“  ëª¨ë¸ ê³¼ì í•© ì—†ìŒ")

if stage2_overfitting:
    print(f"\nâš ï¸  2ë‹¨ê³„ ê³¼ì í•© ëª¨ë¸ ({len(stage2_overfitting)}ê°œ):")
    for name, ovf in stage2_overfitting:
        print(f"  - {name}: {ovf*100:.2f}%p")
else:
    print("\nâœ… 2ë‹¨ê³„ ëª¨ë“  ëª¨ë¸ ê³¼ì í•© ì—†ìŒ")

# ============================================================================
# STEP 6: ìµœì¢… ì„±ëŠ¥ í‰ê°€
# ============================================================================
print("\n" + "=" * 100)
print("[STEP 6] ìµœì¢… ì„±ëŠ¥ í‰ê°€")
print("=" * 100)

# Confusion Matrix
cm = confusion_matrix(y_test_full, final_predictions, 
                      labels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'])

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'],
            yticklabels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'])
plt.title(f'Confusion Matrix (ë°©ë²• B) - ì •í™•ë„: {best_accuracy:.4f}')
plt.ylabel('ì‹¤ì œ')
plt.xlabel('ì˜ˆì¸¡')
plt.tight_layout()
plt.savefig('confusion_matrix_methodB.png', dpi=150, bbox_inches='tight')
print("\nConfusion Matrix ì €ì¥: confusion_matrix_methodB.png")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test_full, final_predictions, 
                           labels=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE'],
                           target_names=['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']))

# í´ë˜ìŠ¤ë³„ ì •í™•ë„
print("\ní´ë˜ìŠ¤ë³„ ì •í™•ë„:")
for i, label in enumerate(['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']):
    mask = y_test_full == label
    if mask.sum() > 0:
        class_acc = accuracy_score(y_test_full[mask], final_predictions[mask])
        print(f"  {label:20} {class_acc:.4f} ({class_acc*100:.2f}%)")

# ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
target_accuracy = 0.95
gap = best_accuracy - target_accuracy

print("\n" + "=" * 100)
print("ëª©í‘œ ë‹¬ì„± í‰ê°€")
print("=" * 100)
print(f"ëª©í‘œ ì •í™•ë„: {target_accuracy:.4f} (95%)")
print(f"ë‹¬ì„± ì •í™•ë„: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
print(f"ê²©ì°¨:        {gap:.4f} ({gap*100:.2f}%p)")

if best_accuracy >= target_accuracy:
    print("\nğŸ‰ ëª©í‘œ ë‹¬ì„±! 95% ì´ìƒ ì •í™•ë„ ë‹¬ì„±!")
else:
    print(f"\nğŸ“Š ëª©í‘œ ë¯¸ë‹¬ì„±. ì¶”ê°€ ê°œì„  í•„ìš”: {abs(gap)*100:.2f}%p í–¥ìƒ í•„ìš”")
    print("\nê°œì„  ë°©ì•ˆ:")
    print("  1. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€ (ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜)")
    print("  2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna, GridSearch)")
    print("  3. ë°ì´í„° ì¦ê°• (SMOTE, ADASYN)")
    print("  4. ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ê°€ (Transformer, Attention)")
    print("  5. ì•™ìƒë¸” ë‹¤ì–‘ì„± ì¦ê°€ (Stacking ë©”íƒ€ ëª¨ë¸ ìµœì í™”)")

print("\n" + "=" * 100)
print("ë¶„ì„ ì™„ë£Œ")
print("=" * 100)
