"""
ê°œì„  ë²„ì „ 3: ì •ê·œí™” ê°•í™”
- L2 regularization ì¦ê°€
- Bagging/Subsampling ì¶”ê°€
- Early Stopping ê°•í™”
- ê³¼ì í•© ìµœì†Œí™”ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

from sklearn.ensemble import StackingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression

import time

print("="*100)
print("ğŸš€ ê°œì„  ë²„ì „ 3: ì •ê·œí™” ê°•í™” (ê³¼ì í•© ìµœì†Œí™”)")
print("="*100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================================================

print("\n" + "="*100)
print("ğŸ“‚ 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
print("="*100)

df = pd.read_csv('datasets/exoplanets.csv')
df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()
print(f"ì´ì§„ ë¶„ë¥˜ ë°ì´í„°: {df_binary.shape[0]} ìƒ˜í”Œ")

base_features = [
    'koi_prad', 'koi_teq', 'koi_insol', 'koi_period', 'koi_sma', 'koi_impact',
    'koi_eccen', 'koi_depth', 'koi_duration', 'koi_srad', 'koi_smass',
    'koi_steff', 'koi_slogg', 'koi_smet', 'ra', 'dec'
]

# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ì „ì²´ 26ê°œ)
df_fe = df_binary[base_features + ['koi_disposition']].copy()

df_fe['planet_star_ratio'] = df_fe['koi_prad'] / (df_fe['koi_srad'] * 109)
df_fe['orbital_energy'] = df_fe['koi_smass'] / df_fe['koi_sma']
df_fe['transit_signal'] = df_fe['koi_depth'] * df_fe['koi_duration']
df_fe['habitable_flux'] = np.abs(df_fe['koi_insol'] - 1.0)
df_fe['stellar_density'] = df_fe['koi_smass'] / (df_fe['koi_srad']**3)
df_fe['planet_density_proxy'] = df_fe['koi_prad'] / np.sqrt(df_fe['koi_teq'] + 1)
df_fe['log_period'] = np.log10(df_fe['koi_period'] + 1)
df_fe['log_depth'] = np.log10(df_fe['koi_depth'] + 1)
df_fe['log_insol'] = np.log10(df_fe['koi_insol'] + 1)
df_fe['orbit_stability'] = df_fe['koi_impact'] * (1 - df_fe['koi_eccen'])

df_fe = df_fe.dropna()

X = df_fe.drop('koi_disposition', axis=1)
y = df_fe['koi_disposition']

le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nìµœì¢… ë°ì´í„°:")
print(f"  â€¢ íŠ¹ì§•: {X.shape[1]}ê°œ")
print(f"  â€¢ ìƒ˜í”Œ: {X.shape[0]}ê°œ")

# Train/Test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, 
    test_size=0.1,
    random_state=42,
    stratify=y_encoded
)

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")

# ============================================================================
# 2. ì •ê·œí™” ê°•í™” ì‹¤í—˜
# ============================================================================

print("\n" + "="*100)
print("ğŸ¯ 2. ë‹¤ì–‘í•œ ì •ê·œí™” ìˆ˜ì¤€ìœ¼ë¡œ ì‹¤í—˜")
print("="*100)

regularization_configs = [
    {
        'name': 'ê¸°ë³¸ ì •ê·œí™”',
        'catboost': {'l2_leaf_reg': 2.0, 'bagging_temperature': 0.0, 'subsample': None},
        'xgboost': {'reg_lambda': 2.0, 'reg_alpha': 0.5, 'subsample': 0.8, 'colsample_bytree': 0.8},
        'lightgbm': {'reg_lambda': 2.0, 'reg_alpha': 0.5, 'subsample': 0.8, 'colsample_bytree': 0.8}
    },
    {
        'name': 'ì¤‘ê°„ ì •ê·œí™”',
        'catboost': {'l2_leaf_reg': 5.0, 'bagging_temperature': 0.5, 'subsample': 0.8},
        'xgboost': {'reg_lambda': 5.0, 'reg_alpha': 1.0, 'subsample': 0.7, 'colsample_bytree': 0.7},
        'lightgbm': {'reg_lambda': 5.0, 'reg_alpha': 1.0, 'subsample': 0.7, 'colsample_bytree': 0.7}
    },
    {
        'name': 'ê°•í•œ ì •ê·œí™”',
        'catboost': {'l2_leaf_reg': 10.0, 'bagging_temperature': 1.0, 'subsample': 0.7},
        'xgboost': {'reg_lambda': 10.0, 'reg_alpha': 2.0, 'subsample': 0.6, 'colsample_bytree': 0.6},
        'lightgbm': {'reg_lambda': 10.0, 'reg_alpha': 2.0, 'subsample': 0.6, 'colsample_bytree': 0.6}
    }
]

best_config = None
best_test_acc = 0
all_results = []

for config in regularization_configs:
    print(f"\n{'='*100}")
    print(f"ğŸ”¹ {config['name']}")
    print(f"{'='*100}")
    
    # CatBoost
    cat_params = config['catboost']
    catboost_model = CatBoostClassifier(
        iterations=500,
        learning_rate=0.03,
        depth=6,
        l2_leaf_reg=cat_params['l2_leaf_reg'],
        bagging_temperature=cat_params['bagging_temperature'],
        subsample=cat_params['subsample'] if cat_params['subsample'] else None,
        random_state=42,
        verbose=False
    )
    
    catboost_model.fit(X_train_scaled, y_train)
    cat_train_acc = accuracy_score(y_train, catboost_model.predict(X_train_scaled))
    cat_test_acc = accuracy_score(y_test, catboost_model.predict(X_test_scaled))
    cat_overfitting = cat_train_acc - cat_test_acc
    
    print(f"\nğŸ“Š CatBoost:")
    print(f"   í•™ìŠµ: {cat_train_acc*100:.2f}%  í…ŒìŠ¤íŠ¸: {cat_test_acc*100:.2f}%  ê³¼ì í•©: {cat_overfitting*100:.2f}%p")
    
    # XGBoost
    xgb_params = config['xgboost']
    xgboost_model = XGBClassifier(
        n_estimators=500,
        learning_rate=0.03,
        max_depth=6,
        reg_lambda=xgb_params['reg_lambda'],
        reg_alpha=xgb_params['reg_alpha'],
        subsample=xgb_params['subsample'],
        colsample_bytree=xgb_params['colsample_bytree'],
        random_state=42,
        eval_metric='logloss'
    )
    
    xgboost_model.fit(X_train_scaled, y_train)
    xgb_train_acc = accuracy_score(y_train, xgboost_model.predict(X_train_scaled))
    xgb_test_acc = accuracy_score(y_test, xgboost_model.predict(X_test_scaled))
    xgb_overfitting = xgb_train_acc - xgb_test_acc
    
    print(f"ğŸ“Š XGBoost:")
    print(f"   í•™ìŠµ: {xgb_train_acc*100:.2f}%  í…ŒìŠ¤íŠ¸: {xgb_test_acc*100:.2f}%  ê³¼ì í•©: {xgb_overfitting*100:.2f}%p")
    
    # LightGBM
    lgb_params = config['lightgbm']
    lightgbm_model = LGBMClassifier(
        n_estimators=500,
        learning_rate=0.03,
        max_depth=6,
        num_leaves=31,
        reg_lambda=lgb_params['reg_lambda'],
        reg_alpha=lgb_params['reg_alpha'],
        subsample=lgb_params['subsample'],
        colsample_bytree=lgb_params['colsample_bytree'],
        random_state=42,
        verbose=-1
    )
    
    lightgbm_model.fit(X_train_scaled, y_train)
    lgb_train_acc = accuracy_score(y_train, lightgbm_model.predict(X_train_scaled))
    lgb_test_acc = accuracy_score(y_test, lightgbm_model.predict(X_test_scaled))
    lgb_overfitting = lgb_train_acc - lgb_test_acc
    
    print(f"ğŸ“Š LightGBM:")
    print(f"   í•™ìŠµ: {lgb_train_acc*100:.2f}%  í…ŒìŠ¤íŠ¸: {lgb_test_acc*100:.2f}%  ê³¼ì í•©: {lgb_overfitting*100:.2f}%p")
    
    # Stacking Ensemble
    base_learners = [
        ('catboost', catboost_model),
        ('xgboost', xgboost_model),
        ('lightgbm', lightgbm_model)
    ]
    
    meta_learner = LogisticRegression(C=0.5, max_iter=1000, random_state=42)  # C=0.5ë¡œ ì •ê·œí™” ê°•í™”
    
    stacking_clf = StackingClassifier(
        estimators=base_learners,
        final_estimator=meta_learner,
        cv=5,
        n_jobs=-1
    )
    
    stacking_clf.fit(X_train_scaled, y_train)
    stack_train_acc = accuracy_score(y_train, stacking_clf.predict(X_train_scaled))
    stack_test_acc = accuracy_score(y_test, stacking_clf.predict(X_test_scaled))
    stack_overfitting = stack_train_acc - stack_test_acc
    
    stack_cv = cross_val_score(stacking_clf, X_train_scaled, y_train, cv=5, scoring='accuracy')
    
    print(f"\nğŸ¯ Stacking Ensemble:")
    print(f"   í•™ìŠµ: {stack_train_acc*100:.2f}%  í…ŒìŠ¤íŠ¸: {stack_test_acc*100:.2f}%  ê³¼ì í•©: {stack_overfitting*100:.2f}%p")
    print(f"   5-Fold CV: {stack_cv.mean()*100:.2f}% Â± {stack_cv.std()*100:.2f}%")
    
    all_results.append({
        'config': config['name'],
        'cat_test': cat_test_acc,
        'xgb_test': xgb_test_acc,
        'lgb_test': lgb_test_acc,
        'stack_test': stack_test_acc,
        'stack_overfitting': stack_overfitting,
        'stack_cv_mean': stack_cv.mean(),
        'models': {
            'catboost': catboost_model,
            'xgboost': xgboost_model,
            'lightgbm': lightgbm_model,
            'stacking': stacking_clf
        }
    })
    
    if stack_test_acc > best_test_acc:
        best_test_acc = stack_test_acc
        best_config = config['name']

# ============================================================================
# 3. ìµœì  ì •ê·œí™” ìˆ˜ì¤€ ì„ íƒ
# ============================================================================

print("\n" + "="*100)
print("ğŸ“Š 3. ì •ê·œí™” ìˆ˜ì¤€ë³„ ì„±ëŠ¥ ë¹„êµ")
print("="*100)

print("\n" + "="*100)
print("ì •ê·œí™” ìˆ˜ì¤€        CatBoost    XGBoost    LightGBM    Stacking    ê³¼ì í•©")
print("="*100)

for result in all_results:
    print(f"{result['config']:15s}   {result['cat_test']*100:6.2f}%   {result['xgb_test']*100:6.2f}%   "
          f"{result['lgb_test']*100:6.2f}%   {result['stack_test']*100:6.2f}%   {result['stack_overfitting']*100:5.2f}%p")

print(f"\nğŸ† ìµœì  ì •ê·œí™”: {best_config}")
print(f"   â€¢ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_test_acc*100:.2f}%")

best_result = [r for r in all_results if r['config'] == best_config][0]

# ============================================================================
# 4. ìµœì¢… ë¹„êµ
# ============================================================================

print("\n" + "="*100)
print("ğŸ“Š 4. ìµœì¢… ì„±ëŠ¥ ë¹„êµ")
print("="*100)

print("\nğŸ” ë²„ì „ë³„ ì„±ëŠ¥:")
print(f"  â€¢ ì›ë³¸ (ê¸°ë³¸ ì •ê·œí™”):       92.24% í…ŒìŠ¤íŠ¸, 5.09%p ê³¼ì í•©")
print(f"  â€¢ ë‹¤ì¤‘ê³µì„ ì„± ì œê±°:          91.61% í…ŒìŠ¤íŠ¸, 3.84%p ê³¼ì í•©")
print(f"  â€¢ PCA ì°¨ì› ì¶•ì†Œ:            89.83% í…ŒìŠ¤íŠ¸, 4.50%p ê³¼ì í•©")
print(f"  â€¢ ì •ê·œí™” ê°•í™” ({best_config}): {best_result['stack_test']*100:.2f}% í…ŒìŠ¤íŠ¸, {best_result['stack_overfitting']*100:.2f}%p ê³¼ì í•©")

improvement = best_result['stack_test'] - 0.9224
print(f"\nğŸ“ˆ ì •ê·œí™” ê°•í™” íš¨ê³¼: {improvement*100:+.2f}%p")

if improvement > 0.5:
    print("   âœ… ì •ê·œí™” ê°•í™”ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!")
elif improvement > 0:
    print("   âœ… ì •ê·œí™” ê°•í™”ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!")
elif improvement > -0.5:
    print("   âš¡ ì„±ëŠ¥ì€ ë¹„ìŠ·í•˜ì§€ë§Œ ê³¼ì í•©ì´ ê°ì†Œí–ˆìŠµë‹ˆë‹¤ (ë” ì•ˆì •ì )")
else:
    print("   âš ï¸ ê³¼ë„í•œ ì •ê·œí™”ë¡œ ì„±ëŠ¥ì´ í•˜ë½í–ˆìŠµë‹ˆë‹¤ (ì–¸ë”í”¼íŒ…)")

# ê³¼ì í•© ê°œì„  ë¶„ì„
overfitting_improvement = 5.09 - best_result['stack_overfitting']*100
print(f"\nğŸ“‰ ê³¼ì í•© ê°ì†Œ: {overfitting_improvement:+.2f}%p")

if best_result['stack_overfitting'] < 0.03:
    print("   âœ… ê³¼ì í•©ì´ ë§¤ìš° ë‚®ì€ ìˆ˜ì¤€ì…ë‹ˆë‹¤ (ì¼ë°˜í™” ìš°ìˆ˜)")
elif best_result['stack_overfitting'] < 0.05:
    print("   âœ… ê³¼ì í•©ì´ ì–‘í˜¸í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤")
else:
    print("   âš¡ ê³¼ì í•©ì´ ì—¬ì „íˆ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤")

print("\n" + "="*100)
print("ğŸ’¡ ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­")
print("="*100)

print("\nğŸ“Š 3ê°€ì§€ ê°œì„  ë°©ë²• ìš”ì•½:")
print("\n1. ë‹¤ì¤‘ê³µì„ ì„± ì œê±° (22ê°œ íŠ¹ì§•):")
print(f"   â€¢ í…ŒìŠ¤íŠ¸: 91.61% (-0.63%p)")
print(f"   â€¢ ê³¼ì í•©: 3.84%p (âœ… 1.25%p ê°œì„ )")
print(f"   â€¢ ê²°ë¡ : ê³¼ì í•© ê°œì„ , ì„±ëŠ¥ ì•½ê°„ í•˜ë½")

print("\n2. PCA ì°¨ì› ì¶•ì†Œ (20ê°œ ì£¼ì„±ë¶„):")
print(f"   â€¢ í…ŒìŠ¤íŠ¸: 89.83% (-2.41%p)")
print(f"   â€¢ ê³¼ì í•©: 4.50%p (âœ… 0.59%p ê°œì„ )")
print(f"   â€¢ ê²°ë¡ : ê³¼ì í•© ê°œì„ , ì„±ëŠ¥ í•˜ë½")

print(f"\n3. ì •ê·œí™” ê°•í™” ({best_config}, 26ê°œ íŠ¹ì§•):")
print(f"   â€¢ í…ŒìŠ¤íŠ¸: {best_result['stack_test']*100:.2f}% ({improvement*100:+.2f}%p)")
print(f"   â€¢ ê³¼ì í•©: {best_result['stack_overfitting']*100:.2f}%p ({overfitting_improvement:+.2f}%p ê°œì„ )")
print(f"   â€¢ ê²°ë¡ : ", end="")

if improvement > 0 and overfitting_improvement > 0:
    print("âœ… ìµœê³ ì˜ ë°©ë²•! ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëª¨ë‘ ê°œì„ ")
elif improvement > 0:
    print("âœ… ì„±ëŠ¥ í–¥ìƒ, ê³¼ì í•©ë„ ê°œì„ ")
elif overfitting_improvement > 1:
    print("âš¡ ê³¼ì í•© í¬ê²Œ ê°œì„ , ì„±ëŠ¥ ìœ ì§€")
else:
    print("âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”")

print("\nğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­:")

if best_result['stack_test'] > 0.9224:
    print(f"   âœ… {best_config} ì •ê·œí™”ë¥¼ ì ìš©í•˜ì„¸ìš”!")
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„ {best_result['stack_test']*100:.2f}%ë¡œ ëª©í‘œì— ê°€ì¥ ê·¼ì ‘í•©ë‹ˆë‹¤.")
else:
    print("   ğŸ’¡ ì¶”ê°€ ê°œì„  ë°©ì•ˆ:")
    print("   â€¢ ì•™ìƒë¸” ë‹¤ì–‘ì„±: SVM, Neural Network ì¶”ê°€")
    print("   â€¢ CANDIDATE í´ë˜ìŠ¤ í¬í•¨í•˜ì—¬ í•™ìŠµ ë°ì´í„° ì¦ê°€")
    print("   â€¢ Bayesian Optimizationìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("   â€¢ íŠ¹ì§• ì„ íƒ(RFE)ìœ¼ë¡œ ìµœì  íŠ¹ì§• ì¡°í•© íƒìƒ‰")

print("\n" + "="*100)
print("âœ… ì •ê·œí™” ê°•í™” ì‹¤í—˜ ì™„ë£Œ!")
print("="*100)
