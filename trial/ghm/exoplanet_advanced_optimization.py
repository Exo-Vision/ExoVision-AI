"""
ì™¸ê³„í–‰ì„± íŒë³„ ìµœì í™” ëª¨ë¸ (95% ëª©í‘œ)
- íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ìŠ¤íƒœí‚¹ ì•™ìƒë¸”
- ê³¼ì í•© ë°©ì§€ ê°•í™”
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.ensemble import (
    RandomForestClassifier,
    ExtraTreesClassifier,
    GradientBoostingClassifier,
    VotingClassifier,
    StackingClassifier
)

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression

import time

print("="*100)
print("ğŸš€ ì™¸ê³„í–‰ì„± íŒë³„ ê³ ê¸‰ ìµœì í™” ëª¨ë¸ (ëª©í‘œ: 95%)")
print("="*100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ì „ì²˜ë¦¬
# ============================================================================

print("\n" + "="*100)
print("ğŸ“‚ 1. ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ì „ì²˜ë¦¬")
print("="*100)

df = pd.read_csv('datasets/exoplanets.csv')
print(f"ì›ë³¸ ë°ì´í„°: {df.shape}")

# ì´ì§„ ë¶„ë¥˜
df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()
print(f"ì´ì§„ ë¶„ë¥˜ ë°ì´í„°: {df_binary.shape[0]} ìƒ˜í”Œ")

# í•µì‹¬ íŠ¹ì§•
base_features = [
    'koi_prad', 'koi_teq', 'koi_insol', 'koi_period', 'koi_sma', 'koi_impact',
    'koi_eccen', 'koi_depth', 'koi_duration', 'koi_srad', 'koi_smass',
    'koi_steff', 'koi_slogg', 'koi_smet', 'ra', 'dec'
]

# ============================================================================
# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ìƒˆë¡œìš´ íŠ¹ì§• ìƒì„±)
# ============================================================================

print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§:")

df_fe = df_binary[base_features + ['koi_disposition']].copy()

# 1. í–‰ì„±-í•­ì„± ë¹„ìœ¨ íŠ¹ì§•
df_fe['planet_star_ratio'] = df_fe['koi_prad'] / (df_fe['koi_srad'] * 109)  # R_earth / R_sun
print("  âœ… planet_star_ratio: í–‰ì„±/í•­ì„± ë°˜ì§€ë¦„ ë¹„ìœ¨")

# 2. ê¶¤ë„ ì—ë„ˆì§€ íŠ¹ì§•
df_fe['orbital_energy'] = df_fe['koi_smass'] / df_fe['koi_sma']  # M/a (ì—ë„ˆì§€ ëŒ€ë¦¬ ë³€ìˆ˜)
print("  âœ… orbital_energy: ê¶¤ë„ ì—ë„ˆì§€ (M/a)")

# 3. í†µê³¼ ì‹ í˜¸ ê°•ë„
df_fe['transit_signal'] = df_fe['koi_depth'] * df_fe['koi_duration']  # ì‹ í˜¸ ì ë¶„
print("  âœ… transit_signal: í†µê³¼ ì‹ í˜¸ ì ë¶„ (depth Ã— duration)")

# 4. Habitable Zone ì§€í‘œ
# ì§€êµ¬ì™€ ë¹„ìŠ·í•œ ì…ì‚¬ í”ŒëŸ­ìŠ¤ (0.5~1.5)
df_fe['habitable_flux'] = np.abs(df_fe['koi_insol'] - 1.0)
print("  âœ… habitable_flux: Habitable Zone ì§€í‘œ (|insol - 1.0|)")

# 5. í•­ì„± ë°€ë„ ëŒ€ë¦¬ ë³€ìˆ˜
df_fe['stellar_density'] = df_fe['koi_smass'] / (df_fe['koi_srad']**3)
print("  âœ… stellar_density: í•­ì„± ë°€ë„ (M/RÂ³)")

# 6. í–‰ì„± ë°€ë„ ëŒ€ë¦¬ ë³€ìˆ˜ (ê°„ì ‘ì  ì¶”ì •)
df_fe['planet_density_proxy'] = df_fe['koi_prad'] / np.sqrt(df_fe['koi_teq'] + 1)
print("  âœ… planet_density_proxy: í–‰ì„± ë°€ë„ ëŒ€ë¦¬ ë³€ìˆ˜")

# 7. ë¡œê·¸ ìŠ¤ì¼€ì¼ ë³€í™˜ (ì™œë„ ê°ì†Œ)
df_fe['log_period'] = np.log10(df_fe['koi_period'] + 1)
df_fe['log_depth'] = np.log10(df_fe['koi_depth'] + 1)
df_fe['log_insol'] = np.log10(df_fe['koi_insol'] + 1)
print("  âœ… log ë³€í™˜: period, depth, insol")

# 8. ê¶¤ë„ ì•ˆì •ì„± ì§€í‘œ
df_fe['orbit_stability'] = df_fe['koi_impact'] * (1 - df_fe['koi_eccen'])
print("  âœ… orbit_stability: ê¶¤ë„ ì•ˆì •ì„± ì§€í‘œ")

# ê²°ì¸¡ì¹˜ ì œê±°
df_clean = df_fe.dropna()
print(f"\nê²°ì¸¡ì¹˜ ì œê±° í›„: {df_clean.shape[0]} ìƒ˜í”Œ ({df_clean.shape[0]/df_binary.shape[0]*100:.1f}%)")

# íŠ¹ì§•ê³¼ ë ˆì´ë¸” ë¶„ë¦¬
feature_cols = [col for col in df_clean.columns if col != 'koi_disposition']
X = df_clean[feature_cols]
y = df_clean['koi_disposition']

# ë ˆì´ë¸” ì¸ì½”ë”©
le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nìµœì¢… ë°ì´í„°:")
print(f"  â€¢ íŠ¹ì§•: {X.shape[1]}ê°œ (ê¸°ë³¸ {len(base_features)}ê°œ + ì—”ì§€ë‹ˆì–´ë§ {X.shape[1]-len(base_features)}ê°œ)")
print(f"  â€¢ ìƒ˜í”Œ: {X.shape[0]}ê°œ")
print(f"  â€¢ ë ˆì´ë¸”: CONFIRMED={np.sum(y_encoded==1)}, FALSE POSITIVE={np.sum(y_encoded==0)}")

# Train/Test ë¶„í•  (90/10)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, 
    test_size=0.1,
    random_state=42,
    stratify=y_encoded
)

print(f"\në°ì´í„° ë¶„í•  (90/10):")
print(f"  â€¢ í•™ìŠµ: {X_train.shape[0]} ìƒ˜í”Œ")
print(f"  â€¢ í…ŒìŠ¤íŠ¸: {X_test.shape[0]} ìƒ˜í”Œ")

# íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nâœ… ê³ ê¸‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")

# ============================================================================
# 2. ìµœì í™”ëœ ê°œë³„ ëª¨ë¸ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì ìš©)
# ============================================================================

print("\n" + "="*100)
print("ğŸ¯ 2. ìµœì í™”ëœ ê°œë³„ ëª¨ë¸ í•™ìŠµ")
print("="*100)

model_results = []

def evaluate_optimized_model(name, model, X_train, y_train, X_test, y_test):
    """ìµœì í™”ëœ ëª¨ë¸ í‰ê°€"""
    
    print(f"\n{'='*100}")
    print(f"ğŸ”¹ {name} (ìµœì í™”)")
    print(f"{'='*100}")
    
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    overfitting = train_acc - test_acc
    
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
    print(f"ğŸ“Š í•™ìŠµ ì •í™•ë„: {train_acc*100:.2f}%")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc*100:.2f}%")
    print(f"âš ï¸ ê³¼ì í•© ì •ë„: {overfitting*100:.2f}%p", end="")
    
    if overfitting > 0.05:
        print(" (âš ï¸ ê³¼ì í•© ê²½ê³ !)")
    elif overfitting > 0.02:
        print(" (âš¡ ì•½ê°„ ê³¼ì í•©)")
    else:
        print(" (âœ… ì–‘í˜¸)")
    
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    print(f"ğŸ”„ êµì°¨ ê²€ì¦ (5-fold): {cv_mean*100:.2f}% Â± {cv_std*100:.2f}%")
    
    model_results.append({
        'name': name,
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'overfitting': overfitting
    })
    
    return model, test_acc

# ============================================================================
# ìµœì í™”ëœ LightGBM (ê°€ì¥ ì„±ëŠ¥ ì¢‹ì•˜ë˜ ëª¨ë¸)
# ============================================================================

lgbm_optimized = LGBMClassifier(
    n_estimators=500,        # ì¦ê°€
    max_depth=10,            # ì¦ê°€
    learning_rate=0.03,      # ê°ì†Œ (ê³¼ì í•© ë°©ì§€)
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_samples=20,    # ì¦ê°€ (ê³¼ì í•© ë°©ì§€)
    reg_alpha=0.5,           # L1 ì¦ê°€
    reg_lambda=2.0,          # L2 ì¦ê°€
    class_weight='balanced',
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgbm_optimized, lgbm_opt_acc = evaluate_optimized_model(
    "LightGBM", lgbm_optimized, X_train_scaled, y_train, X_test_scaled, y_test
)

# ============================================================================
# ìµœì í™”ëœ CatBoost
# ============================================================================

catboost_optimized = CatBoostClassifier(
    iterations=500,
    depth=10,
    learning_rate=0.03,
    l2_leaf_reg=5.0,
    random_strength=1.5,
    bagging_temperature=1.0,
    class_weights=[1, 1],
    random_state=42,
    verbose=0
)

catboost_optimized, catboost_opt_acc = evaluate_optimized_model(
    "CatBoost", catboost_optimized, X_train_scaled, y_train, X_test_scaled, y_test
)

# ============================================================================
# ìµœì í™”ëœ XGBoost
# ============================================================================

xgb_optimized = XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=10,
    gamma=0.2,
    reg_alpha=0.5,
    reg_lambda=2.0,
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)

xgb_optimized, xgb_opt_acc = evaluate_optimized_model(
    "XGBoost", xgb_optimized, X_train_scaled, y_train, X_test_scaled, y_test
)

# ============================================================================
# ìµœì í™”ëœ Random Forest
# ============================================================================

rf_optimized = RandomForestClassifier(
    n_estimators=500,
    max_depth=20,
    min_samples_split=20,
    min_samples_leaf=10,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf_optimized, rf_opt_acc = evaluate_optimized_model(
    "Random Forest", rf_optimized, X_train_scaled, y_train, X_test_scaled, y_test
)

# ============================================================================
# 3. Stacking Ensemble (ë©”íƒ€ ëŸ¬ë‹)
# ============================================================================

print("\n" + "="*100)
print("ğŸ—ï¸ 3. Stacking Ensemble êµ¬ì„±")
print("="*100)

# Base ëª¨ë¸ë“¤
base_models = [
    ('lgbm', lgbm_optimized),
    ('catboost', catboost_optimized),
    ('xgb', xgb_optimized),
    ('rf', rf_optimized)
]

# Meta ëª¨ë¸ (Logistic Regression)
meta_model = LogisticRegression(
    C=1.0,
    max_iter=1000,
    random_state=42
)

# Stacking Classifier
stacking_clf = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,
    n_jobs=-1
)

print("\nğŸ—ï¸ Stacking Ensemble í•™ìŠµ ì¤‘...")
print("  â€¢ Base ëª¨ë¸: LightGBM, CatBoost, XGBoost, Random Forest")
print("  â€¢ Meta ëª¨ë¸: Logistic Regression")
print("  â€¢ CV: 5-fold")

start_time = time.time()
stacking_clf.fit(X_train_scaled, y_train)
stacking_train_time = time.time() - start_time

y_train_pred_stacking = stacking_clf.predict(X_train_scaled)
y_test_pred_stacking = stacking_clf.predict(X_test_scaled)

stacking_train_acc = accuracy_score(y_train, y_train_pred_stacking)
stacking_test_acc = accuracy_score(y_test, y_test_pred_stacking)
stacking_overfitting = stacking_train_acc - stacking_test_acc

print(f"\nâ±ï¸ Stacking í•™ìŠµ ì‹œê°„: {stacking_train_time:.2f}ì´ˆ")
print(f"ğŸ“Š Stacking í•™ìŠµ ì •í™•ë„: {stacking_train_acc*100:.2f}%")
print(f"ğŸ“Š Stacking í…ŒìŠ¤íŠ¸ ì •í™•ë„: {stacking_test_acc*100:.2f}%")
print(f"âš ï¸ Stacking ê³¼ì í•© ì •ë„: {stacking_overfitting*100:.2f}%p", end="")

if stacking_overfitting > 0.05:
    print(" (âš ï¸ ê³¼ì í•© ê²½ê³ !)")
elif stacking_overfitting > 0.02:
    print(" (âš¡ ì•½ê°„ ê³¼ì í•©)")
else:
    print(" (âœ… ì–‘í˜¸)")

# Stacking êµì°¨ ê²€ì¦
print("\nğŸ”„ Stacking êµì°¨ ê²€ì¦ (5-fold)...")
stacking_cv_scores = cross_val_score(stacking_clf, X_train_scaled, y_train, cv=5, scoring='accuracy')
stacking_cv_mean = stacking_cv_scores.mean()
stacking_cv_std = stacking_cv_scores.std()

print(f"ğŸ”„ Stacking êµì°¨ ê²€ì¦: {stacking_cv_mean*100:.2f}% Â± {stacking_cv_std*100:.2f}%")

print(f"\nğŸ“Š Stacking ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_test_pred_stacking, 
                            target_names=['FALSE POSITIVE', 'CONFIRMED'],
                            digits=4))

# ============================================================================
# 4. Soft Voting Ensemble
# ============================================================================

print("\n" + "="*100)
print("ğŸ—³ï¸ 4. Soft Voting Ensemble")
print("="*100)

voting_clf_optimized = VotingClassifier(
    estimators=base_models,
    voting='soft',
    n_jobs=-1
)

print("\nğŸ—³ï¸ Voting Ensemble í•™ìŠµ ì¤‘...")
start_time = time.time()
voting_clf_optimized.fit(X_train_scaled, y_train)
voting_train_time = time.time() - start_time

y_train_pred_voting = voting_clf_optimized.predict(X_train_scaled)
y_test_pred_voting = voting_clf_optimized.predict(X_test_scaled)

voting_train_acc = accuracy_score(y_train, y_train_pred_voting)
voting_test_acc = accuracy_score(y_test, y_test_pred_voting)
voting_overfitting = voting_train_acc - voting_test_acc

print(f"\nâ±ï¸ Voting í•™ìŠµ ì‹œê°„: {voting_train_time:.2f}ì´ˆ")
print(f"ğŸ“Š Voting í•™ìŠµ ì •í™•ë„: {voting_train_acc*100:.2f}%")
print(f"ğŸ“Š Voting í…ŒìŠ¤íŠ¸ ì •í™•ë„: {voting_test_acc*100:.2f}%")
print(f"âš ï¸ Voting ê³¼ì í•© ì •ë„: {voting_overfitting*100:.2f}%p", end="")

if voting_overfitting > 0.05:
    print(" (âš ï¸ ê³¼ì í•© ê²½ê³ !)")
elif voting_overfitting > 0.02:
    print(" (âš¡ ì•½ê°„ ê³¼ì í•©)")
else:
    print(" (âœ… ì–‘í˜¸)")

# ============================================================================
# 5. ìµœì¢… ê²°ê³¼
# ============================================================================

print("\n" + "="*100)
print("ğŸ“ˆ 5. ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("="*100)

# ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
sorted_results = sorted(model_results, key=lambda x: x['test_acc'], reverse=True)

print("\nğŸ† ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„:")
print("-"*100)
for i, result in enumerate(sorted_results, 1):
    print(f"  {i}. {result['name']:<25} : {result['test_acc']*100:>6.2f}% "
          f"(CV: {result['cv_mean']*100:.2f}% Â± {result['cv_std']*100:.2f}%)")

print(f"\nğŸ¯ ì•™ìƒë¸” ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„:")
print("-"*100)
print(f"  1. Stacking Ensemble          : {stacking_test_acc*100:>6.2f}% "
      f"(CV: {stacking_cv_mean*100:.2f}% Â± {stacking_cv_std*100:.2f}%)")
print(f"  2. Soft Voting Ensemble       : {voting_test_acc*100:>6.2f}%")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸
best_single = sorted_results[0]
best_ensemble_acc = max(stacking_test_acc, voting_test_acc)
best_ensemble_name = "Stacking" if stacking_test_acc > voting_test_acc else "Voting"

print(f"\nğŸ¥‡ ìµœê³  ë‹¨ì¼ ëª¨ë¸:")
print(f"  â€¢ ëª¨ë¸: {best_single['name']}")
print(f"  â€¢ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_single['test_acc']*100:.2f}%")

print(f"\nğŸ† ìµœê³  ì•™ìƒë¸” ëª¨ë¸:")
print(f"  â€¢ ëª¨ë¸: {best_ensemble_name} Ensemble")
print(f"  â€¢ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_ensemble_acc*100:.2f}%")

# ê°œì„  ì •ë„
improvement = (best_ensemble_acc - best_single['test_acc']) * 100
print(f"\nğŸ“Š ì•™ìƒë¸” ê°œì„ :")
if improvement > 0:
    print(f"  â€¢ ì•™ìƒë¸”ì´ ìµœê³  ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ {improvement:.2f}%p ë†’ìŒ â¬†ï¸")
elif improvement < 0:
    print(f"  â€¢ ì•™ìƒë¸”ì´ ìµœê³  ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ {abs(improvement):.2f}%p ë‚®ìŒ â¬‡ï¸")
else:
    print(f"  â€¢ ì•™ìƒë¸”ê³¼ ìµœê³  ë‹¨ì¼ ëª¨ë¸ì´ ë™ì¼ â¡ï¸")

# ëª©í‘œ ë‹¬ì„± í™•ì¸
target_acc = 0.95
print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
print(f"  â€¢ ëª©í‘œ ì •í™•ë„: {target_acc*100:.0f}%")
print(f"  â€¢ ë‹¬ì„± ì •í™•ë„: {best_ensemble_acc*100:.2f}%")

if best_ensemble_acc >= target_acc:
    print(f"  â€¢ ê²°ê³¼: âœ… ëª©í‘œ ë‹¬ì„±!")
elif best_ensemble_acc >= target_acc - 0.02:
    print(f"  â€¢ ê²°ê³¼: ğŸ”¥ ê±°ì˜ ë‹¬ì„± (ëª©í‘œê¹Œì§€ {(target_acc - best_ensemble_acc)*100:.2f}%p)")
else:
    print(f"  â€¢ ê²°ê³¼: âš¡ ëª©í‘œì— ê·¼ì ‘ (ëª©í‘œê¹Œì§€ {(target_acc - best_ensemble_acc)*100:.2f}%p)")

# í˜¼ë™ í–‰ë ¬
print(f"\nğŸ“Š {best_ensemble_name} Ensemble í˜¼ë™ í–‰ë ¬:")
best_predictions = y_test_pred_stacking if best_ensemble_name == "Stacking" else y_test_pred_voting
cm = confusion_matrix(y_test, best_predictions)
print("\nì‹¤ì œ\\ì˜ˆì¸¡   FALSE POSITIVE   CONFIRMED")
print("-"*45)
print(f"FALSE POS   {cm[0,0]:>8}          {cm[0,1]:>8}")
print(f"CONFIRMED   {cm[1,0]:>8}          {cm[1,1]:>8}")

precision = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
recall = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f"\nğŸ“Š ì£¼ìš” ë©”íŠ¸ë¦­:")
print(f"  â€¢ Accuracy (ì •í™•ë„): {best_ensemble_acc*100:.2f}%")
print(f"  â€¢ Precision (ì •ë°€ë„): {precision*100:.2f}%")
print(f"  â€¢ Recall (ì¬í˜„ìœ¨): {recall*100:.2f}%")
print(f"  â€¢ F1-Score: {f1*100:.2f}%")

print("\n" + "="*100)
print("ğŸ‰ ìµœì í™”ëœ ì™¸ê³„í–‰ì„± íŒë³„ ëª¨ë¸ ì™„ì„±!")
print("="*100)
