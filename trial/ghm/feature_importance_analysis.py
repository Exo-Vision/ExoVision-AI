"""
ì™¸ê³„í–‰ì„± íŒë³„ ëª¨ë¸ Feature Importance ë¶„ì„
- ê° ëª¨ë¸ì˜ íŠ¹ì§• ì¤‘ìš”ë„ í™•ì¸
- íŠ¹ì • ì»¬ëŸ¼ í¸í–¥ ê²€ì‚¬
- ê· í˜•ì¡íŒ ëª¨ë¸ í‰ê°€
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

import matplotlib.pyplot as plt
import seaborn as sns

print("="*100)
print("ğŸ” Feature Importance ë¶„ì„")
print("="*100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì´ì „ê³¼ ë™ì¼)
# ============================================================================

print("\nğŸ“‚ ë°ì´í„° ì¤€ë¹„...")
df = pd.read_csv('datasets/exoplanets.csv')
df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()

# ê¸°ë³¸ íŠ¹ì§•
base_features = [
    'koi_prad', 'koi_teq', 'koi_insol', 'koi_period', 'koi_sma', 'koi_impact',
    'koi_eccen', 'koi_depth', 'koi_duration', 'koi_srad', 'koi_smass',
    'koi_steff', 'koi_slogg', 'koi_smet', 'ra', 'dec'
]

# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
df_fe = df_binary[base_features + ['koi_disposition']].copy()

df_fe['planet_star_ratio'] = df_fe['koi_prad'] / (df_fe['koi_srad'] * 109)
df_fe['orbital_energy'] = df_fe['koi_smass'] / df_fe['koi_sma']
df_fe['transit_signal'] = df_fe['koi_depth'] * df_fe['koi_duration']
df_fe['habitable_flux'] = np.abs(df_fe['koi_insol'] - 1.0)
df_fe['stellar_density'] = df_fe['koi_smass'] / (df_fe['koi_srad']**3)
df_fe['planet_density_proxy'] = df_fe['koi_prad'] / np.sqrt(df_fe['koi_teq'] + 1)
df_fe['log_period'] = np.log10(df_fe['koi_period'] + 1)
df_fe['log_depth'] = np.log10(df_fe['koi_depth'] + 1)
df_fe['log_insol'] = np.log10(df_fe['koi_insol'] + 1)
df_fe['orbit_stability'] = df_fe['koi_impact'] * (1 - df_fe['koi_eccen'])

df_clean = df_fe.dropna()

feature_cols = [col for col in df_clean.columns if col != 'koi_disposition']
X = df_clean[feature_cols]
y = df_clean['koi_disposition']

le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"íŠ¹ì§• ìˆ˜: {len(feature_cols)}ê°œ")
print(f"í•™ìŠµ ìƒ˜í”Œ: {X_train.shape[0]}ê°œ")

# ============================================================================
# 2. ëª¨ë¸ë³„ Feature Importance ì¶”ì¶œ
# ============================================================================

print("\n" + "="*100)
print("ğŸ¤– ëª¨ë¸ í•™ìŠµ ë° Feature Importance ì¶”ì¶œ")
print("="*100)

# ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
feature_importance_dict = {}

# ============================================================================
# 2.1 CatBoost (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
# ============================================================================

print("\nğŸ”¹ CatBoost...")
catboost_model = CatBoostClassifier(
    iterations=500,
    depth=10,
    learning_rate=0.03,
    l2_leaf_reg=5.0,
    random_state=42,
    verbose=0
)

catboost_model.fit(X_train_scaled, y_train)
catboost_acc = accuracy_score(y_test, catboost_model.predict(X_test_scaled))

# Feature importance ì¶”ì¶œ
catboost_importance = catboost_model.feature_importances_
feature_importance_dict['CatBoost'] = dict(zip(feature_cols, catboost_importance))

print(f"ì •í™•ë„: {catboost_acc*100:.2f}%")

# ============================================================================
# 2.2 XGBoost
# ============================================================================

print("\nğŸ”¹ XGBoost...")
xgb_model = XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)

xgb_model.fit(X_train_scaled, y_train)
xgb_acc = accuracy_score(y_test, xgb_model.predict(X_test_scaled))

# Feature importance ì¶”ì¶œ
xgb_importance = xgb_model.feature_importances_
feature_importance_dict['XGBoost'] = dict(zip(feature_cols, xgb_importance))

print(f"ì •í™•ë„: {xgb_acc*100:.2f}%")

# ============================================================================
# 2.3 LightGBM
# ============================================================================

print("\nğŸ”¹ LightGBM...")
lgbm_model = LGBMClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgbm_model.fit(X_train_scaled, y_train)
lgbm_acc = accuracy_score(y_test, lgbm_model.predict(X_test_scaled))

# Feature importance ì¶”ì¶œ
lgbm_importance = lgbm_model.feature_importances_
feature_importance_dict['LightGBM'] = dict(zip(feature_cols, lgbm_importance))

print(f"ì •í™•ë„: {lgbm_acc*100:.2f}%")

# ============================================================================
# 2.4 Random Forest
# ============================================================================

print("\nğŸ”¹ Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=500,
    max_depth=20,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train_scaled, y_train)
rf_acc = accuracy_score(y_test, rf_model.predict(X_test_scaled))

# Feature importance ì¶”ì¶œ
rf_importance = rf_model.feature_importances_
feature_importance_dict['Random Forest'] = dict(zip(feature_cols, rf_importance))

print(f"ì •í™•ë„: {rf_acc*100:.2f}%")

# ============================================================================
# 3. Feature Importance ë¶„ì„
# ============================================================================

print("\n" + "="*100)
print("ğŸ“Š Feature Importance ìƒì„¸ ë¶„ì„")
print("="*100)

# ê° ëª¨ë¸ë³„ Top 10 íŠ¹ì§•
print("\nğŸ† ê° ëª¨ë¸ë³„ Top 10 ì¤‘ìš” íŠ¹ì§•:")
print("="*100)

for model_name, importance_dict in feature_importance_dict.items():
    print(f"\nğŸ”¹ {model_name}")
    print("-"*100)
    
    # ì •ê·œí™” (í•©ì´ 100%ê°€ ë˜ë„ë¡)
    total_importance = sum(importance_dict.values())
    normalized_importance = {k: (v/total_importance)*100 for k, v in importance_dict.items()}
    
    # ì •ë ¬
    sorted_features = sorted(normalized_importance.items(), key=lambda x: x[1], reverse=True)
    
    # Top 10 ì¶œë ¥
    for i, (feature, importance) in enumerate(sorted_features[:10], 1):
        print(f"  {i:2d}. {feature:<30} {importance:>6.2f}%")
    
    # ë‚˜ë¨¸ì§€ íŠ¹ì§•ë“¤ì˜ í•©
    remaining_importance = sum([imp for feat, imp in sorted_features[10:]])
    print(f"  {'ê¸°íƒ€ (16ê°œ)':>33} {remaining_importance:>6.2f}%")
    
    # í¸í–¥ ê²€ì‚¬
    top1_importance = sorted_features[0][1]
    top3_importance = sum([imp for feat, imp in sorted_features[:3]])
    top5_importance = sum([imp for feat, imp in sorted_features[:5]])
    
    print(f"\n  ğŸ“Š ì§‘ì¤‘ë„ ë¶„ì„:")
    print(f"     - Top 1 íŠ¹ì§• ë¹„ìœ¨: {top1_importance:.2f}%", end="")
    if top1_importance > 30:
        print(" âš ï¸ ê³¼ë„í•œ ì§‘ì¤‘!")
    elif top1_importance > 20:
        print(" âš¡ ë†’ì€ í¸")
    else:
        print(" âœ… ì–‘í˜¸")
    
    print(f"     - Top 3 íŠ¹ì§• ë¹„ìœ¨: {top3_importance:.2f}%", end="")
    if top3_importance > 60:
        print(" âš ï¸ ê³¼ë„í•œ ì§‘ì¤‘!")
    elif top3_importance > 50:
        print(" âš¡ ë†’ì€ í¸")
    else:
        print(" âœ… ì–‘í˜¸")
    
    print(f"     - Top 5 íŠ¹ì§• ë¹„ìœ¨: {top5_importance:.2f}%", end="")
    if top5_importance > 70:
        print(" âš ï¸ ê³¼ë„í•œ ì§‘ì¤‘!")
    elif top5_importance > 60:
        print(" âš¡ ë†’ì€ í¸")
    else:
        print(" âœ… ì–‘í˜¸")

# ============================================================================
# 4. íŠ¹ì§• ì¤‘ìš”ë„ ë¹„êµ (ëª¨ë¸ ê°„)
# ============================================================================

print("\n" + "="*100)
print("ğŸ”„ ëª¨ë¸ ê°„ Feature Importance ë¹„êµ")
print("="*100)

# ëª¨ë“  ëª¨ë¸ì˜ í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
avg_importance = {}
for feature in feature_cols:
    importances = []
    for model_name, importance_dict in feature_importance_dict.items():
        # ì •ê·œí™”
        total = sum(importance_dict.values())
        normalized = (importance_dict[feature] / total) * 100
        importances.append(normalized)
    
    avg_importance[feature] = {
        'mean': np.mean(importances),
        'std': np.std(importances),
        'values': importances
    }

# í‰ê·  ì¤‘ìš”ë„ ì •ë ¬
sorted_avg_importance = sorted(avg_importance.items(), key=lambda x: x[1]['mean'], reverse=True)

print("\nğŸ“Š í‰ê·  Feature Importance (4ê°œ ëª¨ë¸ í‰ê· ):")
print("-"*100)
print(f"{'ìˆœìœ„':<5} {'íŠ¹ì§•':<30} {'í‰ê·  ì¤‘ìš”ë„':<15} {'í‘œì¤€í¸ì°¨':<15} {'ì¼ê´€ì„±':<10}")
print("-"*100)

for i, (feature, stats) in enumerate(sorted_avg_importance[:15], 1):
    consistency = "ë†’ìŒ" if stats['std'] < 2 else ("ì¤‘ê°„" if stats['std'] < 4 else "ë‚®ìŒ")
    consistency_emoji = "âœ…" if stats['std'] < 2 else ("âš¡" if stats['std'] < 4 else "âš ï¸")
    
    print(f"{i:<5} {feature:<30} {stats['mean']:>6.2f}%        {stats['std']:>6.2f}%        "
          f"{consistency_emoji} {consistency}")

# ============================================================================
# 5. íŠ¹ì§• ì¤‘ìš”ë„ ê· í˜• í‰ê°€
# ============================================================================

print("\n" + "="*100)
print("âš–ï¸ Feature Importance ê· í˜• í‰ê°€")
print("="*100)

# Gini ê³„ìˆ˜ ê³„ì‚° (ë¶ˆí‰ë“± ì§€ìˆ˜)
def calculate_gini_coefficient(importances):
    """Gini ê³„ìˆ˜: 0 (ì™„ì „ ê· ë“±) ~ 1 (ì™„ì „ ë¶ˆê· ë“±)"""
    importances = np.sort(np.array(importances))
    n = len(importances)
    index = np.arange(1, n + 1)
    return (2 * np.sum(index * importances)) / (n * np.sum(importances)) - (n + 1) / n

print("\nğŸ“Š ê° ëª¨ë¸ì˜ íŠ¹ì§• ì¤‘ìš”ë„ ë¶ˆê· í˜• ì§€ìˆ˜ (Gini Coefficient):")
print("-"*100)
print(f"{'ëª¨ë¸':<25} {'Gini ê³„ìˆ˜':<15} {'ê· í˜• í‰ê°€':<20} {'ì •í™•ë„':<10}")
print("-"*100)

gini_results = []
accuracies = {
    'CatBoost': catboost_acc,
    'XGBoost': xgb_acc,
    'LightGBM': lgbm_acc,
    'Random Forest': rf_acc
}

for model_name, importance_dict in feature_importance_dict.items():
    importances = list(importance_dict.values())
    gini = calculate_gini_coefficient(importances)
    
    # ê· í˜• í‰ê°€
    if gini < 0.3:
        balance = "âœ… ë§¤ìš° ê· í˜•ì "
    elif gini < 0.5:
        balance = "âš¡ ë‹¤ì†Œ ë¶ˆê· í˜•"
    else:
        balance = "âš ï¸ ì‹¬ê°í•œ ë¶ˆê· í˜•"
    
    accuracy = accuracies[model_name]
    gini_results.append((model_name, gini, balance, accuracy))
    
    print(f"{model_name:<25} {gini:>6.4f}         {balance:<20} {accuracy*100:>6.2f}%")

print("\nğŸ’¡ í•´ì„:")
print("  â€¢ Gini ê³„ìˆ˜ < 0.3: íŠ¹ì§•ë“¤ì´ ê· í˜•ìˆê²Œ ì‚¬ìš©ë¨ (ì¢‹ìŒ)")
print("  â€¢ Gini ê³„ìˆ˜ 0.3-0.5: ì¼ë¶€ íŠ¹ì§•ì— ì¹˜ìš°ì¹¨ (ë³´í†µ)")
print("  â€¢ Gini ê³„ìˆ˜ > 0.5: ì†Œìˆ˜ íŠ¹ì§•ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´ (ë‚˜ì¨)")

# ============================================================================
# 6. ì‹œê°í™” (Feature Importance)
# ============================================================================

print("\n" + "="*100)
print("ğŸ“ˆ Feature Importance ì‹œê°í™”")
print("="*100)

# Top 15 íŠ¹ì§•ë§Œ ì‹œê°í™”
top_n = 15
top_features = [feat for feat, stats in sorted_avg_importance[:top_n]]

# ê° ëª¨ë¸ì˜ Top 15 íŠ¹ì§• ì¤‘ìš”ë„ ì¶”ì¶œ
importance_df = pd.DataFrame()
for model_name, importance_dict in feature_importance_dict.items():
    total = sum(importance_dict.values())
    normalized = {k: (v/total)*100 for k, v in importance_dict.items()}
    
    model_importances = [normalized.get(feat, 0) for feat in top_features]
    importance_df[model_name] = model_importances

importance_df.index = top_features

# í”Œë¡¯ ìƒì„±
fig, axes = plt.subplots(2, 2, figsize=(20, 16))
fig.suptitle('Feature Importance ë¹„êµ (ìƒìœ„ 15ê°œ íŠ¹ì§•)', fontsize=20, fontweight='bold')

models = list(feature_importance_dict.keys())
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']

for idx, (ax, model_name) in enumerate(zip(axes.flat, models)):
    importance_values = importance_df[model_name].sort_values(ascending=True)
    
    bars = ax.barh(range(len(importance_values)), importance_values, color=colors[idx], alpha=0.7)
    ax.set_yticks(range(len(importance_values)))
    ax.set_yticklabels(importance_values.index, fontsize=10)
    ax.set_xlabel('ì¤‘ìš”ë„ (%)', fontsize=12)
    ax.set_title(f'{model_name} (ì •í™•ë„: {accuracies[model_name]*100:.2f}%)', 
                 fontsize=14, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for i, (bar, value) in enumerate(zip(bars, importance_values)):
        ax.text(value + 0.3, i, f'{value:.1f}%', va='center', fontsize=9)

plt.tight_layout()
plt.savefig('feature_importance_comparison.png', dpi=150, bbox_inches='tight')
print("\nâœ… ì‹œê°í™” ì €ì¥: feature_importance_comparison.png")

# ============================================================================
# 7. í‰ê·  Feature Importance ì‹œê°í™”
# ============================================================================

fig, ax = plt.subplots(figsize=(12, 10))

avg_importances = [stats['mean'] for feat, stats in sorted_avg_importance[:top_n]]
std_importances = [stats['std'] for feat, stats in sorted_avg_importance[:top_n]]
feature_names = [feat for feat, stats in sorted_avg_importance[:top_n]]

# ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
indices = np.argsort(avg_importances)
avg_importances_sorted = [avg_importances[i] for i in indices]
std_importances_sorted = [std_importances[i] for i in indices]
feature_names_sorted = [feature_names[i] for i in indices]

bars = ax.barh(range(len(avg_importances_sorted)), avg_importances_sorted, 
               xerr=std_importances_sorted, color='#3498db', alpha=0.7, 
               error_kw={'linewidth': 2, 'ecolor': '#e74c3c'})

ax.set_yticks(range(len(feature_names_sorted)))
ax.set_yticklabels(feature_names_sorted, fontsize=11)
ax.set_xlabel('í‰ê·  ì¤‘ìš”ë„ (%) Â± í‘œì¤€í¸ì°¨', fontsize=13, fontweight='bold')
ax.set_title('í‰ê·  Feature Importance (4ê°œ ëª¨ë¸)', fontsize=16, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

# ê°’ í‘œì‹œ
for i, (bar, value, std) in enumerate(zip(bars, avg_importances_sorted, std_importances_sorted)):
    ax.text(value + std + 0.5, i, f'{value:.1f}%', va='center', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.savefig('feature_importance_average.png', dpi=150, bbox_inches='tight')
print("âœ… ì‹œê°í™” ì €ì¥: feature_importance_average.png")

# ============================================================================
# 8. ìµœì¢… ê¶Œì¥ì‚¬í•­
# ============================================================================

print("\n" + "="*100)
print("ğŸ’¡ ìµœì¢… ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­")
print("="*100)

# í‰ê·  Gini ê³„ìˆ˜
avg_gini = np.mean([result[1] for result in gini_results])
avg_accuracy = np.mean([result[3] for result in gini_results])

print(f"\nğŸ“Š ì „ì²´ ìš”ì•½:")
print(f"  â€¢ í‰ê·  Gini ê³„ìˆ˜: {avg_gini:.4f}")
print(f"  â€¢ í‰ê·  ì •í™•ë„: {avg_accuracy*100:.2f}%")

# ìƒìœ„ 3ê°œ íŠ¹ì§•ì˜ í‰ê·  ë¹„ìœ¨
top3_avg_importance = sum([stats['mean'] for feat, stats in sorted_avg_importance[:3]])
print(f"  â€¢ Top 3 íŠ¹ì§• í‰ê·  ë¹„ìœ¨: {top3_avg_importance:.2f}%")

print(f"\nğŸ” ëª¨ë¸ í¸í–¥ ì§„ë‹¨:")

if avg_gini < 0.3 and top3_avg_importance < 40:
    print("  âœ… ë§¤ìš° ìš°ìˆ˜: íŠ¹ì§•ë“¤ì´ ê· í˜•ìˆê²Œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤!")
    print("     - ë‹¤ì–‘í•œ íŠ¹ì§•ì„ í™œìš©í•˜ì—¬ robustnessê°€ ë†’ìŠµë‹ˆë‹¤.")
    print("     - ê³¼ì í•© ìœ„í—˜ì´ ë‚®ìŠµë‹ˆë‹¤.")
    
elif avg_gini < 0.5 and top3_avg_importance < 50:
    print("  âš¡ ì–‘í˜¸: ì¼ë¶€ íŠ¹ì§•ì— ì¹˜ìš°ì³ ìˆì§€ë§Œ í—ˆìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
    print("     - ì£¼ìš” íŠ¹ì§•ë“¤ì´ ì‹¤ì œë¡œ ì¤‘ìš”í•œ ë¬¼ë¦¬ëŸ‰ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("     - êµì°¨ ê²€ì¦ìœ¼ë¡œ ê³¼ì í•©ì„ ê³„ì† ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.")
    
else:
    print("  âš ï¸ ì£¼ì˜ í•„ìš”: ì†Œìˆ˜ íŠ¹ì§•ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
    print("     - íŠ¹ì • íŠ¹ì§•ì— ëŒ€í•œ ê³¼ì í•© ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
    print("     - ì •ê·œí™” ê°•í™” ë˜ëŠ” íŠ¹ì§• ì„ íƒ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

print(f"\nğŸ¯ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§• Top 5:")
for i, (feature, stats) in enumerate(sorted_avg_importance[:5], 1):
    print(f"  {i}. {feature:<30} {stats['mean']:>6.2f}% Â± {stats['std']:>4.2f}%")

print(f"\nğŸ“š ë¬¼ë¦¬ì  í•´ì„:")

# Top íŠ¹ì§•ë“¤ì˜ ë¬¼ë¦¬ì  ì˜ë¯¸
feature_meanings = {
    'koi_depth': 'í†µê³¼ ê¹Šì´ - í–‰ì„± í¬ê¸° ì§ì ‘ ì¸¡ì •',
    'koi_duration': 'í†µê³¼ ì§€ì†ì‹œê°„ - ê¶¤ë„ ê¸°í•˜í•™',
    'koi_prad': 'í–‰ì„± ë°˜ì§€ë¦„ - í–‰ì„± í¬ê¸°',
    'koi_period': 'ê¶¤ë„ ì£¼ê¸° - ê¶¤ë„ íŠ¹ì„±',
    'transit_signal': 'í†µê³¼ ì‹ í˜¸ ì ë¶„ - ì‹ í˜¸ ê°•ë„',
    'koi_impact': 'ì¶©ê²© ë§¤ê°œë³€ìˆ˜ - ê¶¤ë„ ì •ë ¬',
    'koi_sma': 'ë°˜ì¥ì¶• - ê¶¤ë„ ê±°ë¦¬',
    'koi_teq': 'í‰í˜• ì˜¨ë„ - í–‰ì„± ì˜¨ë„',
    'planet_star_ratio': 'í–‰ì„±/í•­ì„± ë¹„ìœ¨ - ìƒëŒ€ í¬ê¸°',
    'koi_insol': 'ì…ì‚¬ í”ŒëŸ­ìŠ¤ - ì—ë„ˆì§€ ìˆ˜ì¤€'
}

for i, (feature, stats) in enumerate(sorted_avg_importance[:5], 1):
    meaning = feature_meanings.get(feature, 'íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ë³€ìˆ˜')
    print(f"  {i}. {feature}: {meaning}")

print("\n" + "="*100)
print("âœ… Feature Importance ë¶„ì„ ì™„ë£Œ!")
print("="*100)
