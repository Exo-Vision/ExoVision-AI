"""
ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- ë°ì´í„° ë¶„í¬ ë¶„ì„
- í•™ìŠµ ê³¡ì„  ë¶„ì„
- í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸
- íŠ¹ì§• ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
- ëª¨ë¸ ë³µì¡ë„ vs ì„±ëŠ¥ ë¶„ì„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score
from catboost import CatBoostClassifier
import warnings
warnings.filterwarnings('ignore')

print("="*100)
print("ğŸ” ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„")
print("="*100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„ì„
# ============================================================================

print("\n" + "="*100)
print("ğŸ“‚ 1. ë°ì´í„° ë¶„ì„")
print("="*100)

df = pd.read_csv('datasets/exoplanets.csv')
df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()

print(f"\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
print(df_binary['koi_disposition'].value_counts())
print(f"\në¹„ìœ¨:")
for label, count in df_binary['koi_disposition'].value_counts().items():
    print(f"  â€¢ {label}: {count/len(df_binary)*100:.2f}%")

# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨
class_ratio = df_binary['koi_disposition'].value_counts().max() / df_binary['koi_disposition'].value_counts().min()
print(f"\nâš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {class_ratio:.2f}:1")

if class_ratio > 1.5:
    print("  âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì¡´ì¬í•©ë‹ˆë‹¤ (>1.5:1)")
else:
    print("  âœ… í´ë˜ìŠ¤ê°€ ê· í˜•ì ì…ë‹ˆë‹¤ (â‰¤1.5:1)")

# ============================================================================
# 2. íŠ¹ì§• ìƒê´€ê´€ê³„ ë¶„ì„
# ============================================================================

print("\n" + "="*100)
print("ğŸ”— 2. íŠ¹ì§• ìƒê´€ê´€ê³„ ë¶„ì„")
print("="*100)

base_features = [
    'koi_prad', 'koi_teq', 'koi_insol', 'koi_period', 'koi_sma', 'koi_impact',
    'koi_eccen', 'koi_depth', 'koi_duration', 'koi_srad', 'koi_smass',
    'koi_steff', 'koi_slogg', 'koi_smet', 'ra', 'dec'
]

# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ë™ì¼í•˜ê²Œ ì ìš©)
df_fe = df_binary[base_features + ['koi_disposition']].copy()
df_fe['planet_star_ratio'] = df_fe['koi_prad'] / (df_fe['koi_srad'] * 109)
df_fe['orbital_energy'] = df_fe['koi_smass'] / df_fe['koi_sma']
df_fe['transit_signal'] = df_fe['koi_depth'] * df_fe['koi_duration']
df_fe['habitable_flux'] = np.abs(df_fe['koi_insol'] - 1.0)
df_fe['stellar_density'] = df_fe['koi_smass'] / (df_fe['koi_srad']**3)
df_fe['planet_density_proxy'] = df_fe['koi_prad'] / np.sqrt(df_fe['koi_teq'] + 1)
df_fe['log_period'] = np.log10(df_fe['koi_period'] + 1)
df_fe['log_depth'] = np.log10(df_fe['koi_depth'] + 1)
df_fe['log_insol'] = np.log10(df_fe['koi_insol'] + 1)
df_fe['orbit_stability'] = df_fe['koi_impact'] * (1 - df_fe['koi_eccen'])

df_fe = df_fe.dropna()

# ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
X = df_fe.drop('koi_disposition', axis=1)
corr_matrix = X.corr().abs()

# ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸° (>0.8)
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        if corr_matrix.iloc[i, j] > 0.8:
            high_corr_pairs.append((
                corr_matrix.columns[i],
                corr_matrix.columns[j],
                corr_matrix.iloc[i, j]
            ))

print(f"\nğŸ”´ ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì§• ìŒ (>0.8):")
if high_corr_pairs:
    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: -x[2]):
        print(f"  â€¢ {feat1:30s} â†” {feat2:30s}: {corr:.3f}")
    print(f"\n  âš ï¸ {len(high_corr_pairs)}ê°œì˜ ê³ ìƒê´€ íŠ¹ì§• ìŒ ë°œê²¬!")
    print(f"  ğŸ’¡ ë‹¤ì¤‘ê³µì„ ì„±ì´ ëª¨ë¸ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    print("  âœ… ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì§• ì—†ìŒ (ë‹¤ì¤‘ê³µì„ ì„± ì—†ìŒ)")

# ì¤‘ê°„ ìƒê´€ê´€ê³„ (0.6~0.8)
medium_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        if 0.6 < corr_matrix.iloc[i, j] <= 0.8:
            medium_corr_pairs.append((
                corr_matrix.columns[i],
                corr_matrix.columns[j],
                corr_matrix.iloc[i, j]
            ))

print(f"\nğŸŸ¡ ì¤‘ê°„ ìƒê´€ê´€ê³„ íŠ¹ì§• ìŒ (0.6~0.8):")
if medium_corr_pairs:
    print(f"  â€¢ {len(medium_corr_pairs)}ê°œì˜ ì¤‘ìƒê´€ íŠ¹ì§• ìŒ ë°œê²¬")
    print(f"  âš¡ ì¼ë¶€ ì¤‘ë³µ ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    print("  âœ… ì¤‘ê°„ ìƒê´€ê´€ê³„ íŠ¹ì§• ì—†ìŒ")

# ============================================================================
# 3. í•™ìŠµ ê³¡ì„  ë¶„ì„ (ë°ì´í„° í¬ê¸° vs ì„±ëŠ¥)
# ============================================================================

print("\n" + "="*100)
print("ğŸ“ˆ 3. í•™ìŠµ ê³¡ì„  ë¶„ì„")
print("="*100)

y = df_fe['koi_disposition']
le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# CatBoostë¡œ í•™ìŠµ ê³¡ì„  ìƒì„±
print("\nâ³ í•™ìŠµ ê³¡ì„  ìƒì„± ì¤‘... (ì•½ 1ë¶„ ì†Œìš”)")
model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.03,
    depth=6,
    l2_leaf_reg=2.0,
    random_state=42,
    verbose=False
)

train_sizes, train_scores, val_scores = learning_curve(
    model, X_train_scaled, y_train,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

print("\nğŸ“Š í•™ìŠµ ê³¡ì„  ê²°ê³¼:")
print("\nìƒ˜í”Œ ìˆ˜       í•™ìŠµ ì •í™•ë„      ê²€ì¦ ì •í™•ë„      ê²©ì°¨")
print("-" * 70)
for i, size in enumerate(train_sizes):
    gap = (train_mean[i] - val_mean[i]) * 100
    print(f"{int(size):6d}ê°œ     {train_mean[i]*100:6.2f}% Â± {train_std[i]*100:4.2f}%   "
          f"{val_mean[i]*100:6.2f}% Â± {val_std[i]*100:4.2f}%   {gap:5.2f}%p")

# ê²©ì°¨ ë¶„ì„
final_gap = (train_mean[-1] - val_mean[-1]) * 100
print(f"\nìµœì¢… ê²©ì°¨ (ì „ì²´ ë°ì´í„°): {final_gap:.2f}%p")

if final_gap > 5:
    print("  âš ï¸ ë†’ì€ ê²©ì°¨: ê³¼ì í•© ì˜ì‹¬")
elif final_gap > 2:
    print("  âš¡ ì¤‘ê°„ ê²©ì°¨: ì•½ê°„ì˜ ê³¼ì í•©")
else:
    print("  âœ… ë‚®ì€ ê²©ì°¨: ì–‘í˜¸í•œ ì¼ë°˜í™”")

# ìˆ˜ë ´ ì—¬ë¶€ í™•ì¸
last_3_val = val_mean[-3:]
val_improvement = (last_3_val[-1] - last_3_val[0]) * 100
print(f"\në§ˆì§€ë§‰ 3ê°œ êµ¬ê°„ ê²€ì¦ ì„±ëŠ¥ í–¥ìƒ: {val_improvement:.2f}%p")

if val_improvement > 1:
    print("  ğŸ“ˆ ì—¬ì „íˆ ì„±ëŠ¥ í–¥ìƒ ì¤‘ - ë” ë§ì€ ë°ì´í„°ê°€ ë„ì›€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
elif val_improvement > 0:
    print("  ğŸ“Š ì•½ê°„ í–¥ìƒ ì¤‘ - ë°ì´í„°ê°€ ì¶©ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
else:
    print("  ğŸ“‰ ì„±ëŠ¥ ì •ì²´ - ë°ì´í„°ë³´ë‹¤ ëª¨ë¸/íŠ¹ì§• ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")

# ============================================================================
# 4. ëª¨ë¸ ë³µì¡ë„ vs ì„±ëŠ¥ ë¶„ì„
# ============================================================================

print("\n" + "="*100)
print("ğŸ¯ 4. ëª¨ë¸ ë³µì¡ë„ vs ì„±ëŠ¥ ë¶„ì„")
print("="*100)

print("\nâ³ ë‹¤ì–‘í•œ ë³µì¡ë„ë¡œ ëª¨ë¸ í•™ìŠµ ì¤‘...")

complexities = [
    {'iterations': 100, 'depth': 4, 'name': 'ë§¤ìš° ë‹¨ìˆœ'},
    {'iterations': 200, 'depth': 5, 'name': 'ë‹¨ìˆœ'},
    {'iterations': 500, 'depth': 6, 'name': 'ì¤‘ê°„'},
    {'iterations': 1000, 'depth': 7, 'name': 'ë³µì¡'},
    {'iterations': 1500, 'depth': 8, 'name': 'ë§¤ìš° ë³µì¡'},
]

print("\në³µì¡ë„       í•™ìŠµ ì •í™•ë„    í…ŒìŠ¤íŠ¸ ì •í™•ë„    ê²©ì°¨      ê³¼ì í•©")
print("-" * 75)

best_test_acc = 0
best_complexity = None

for config in complexities:
    model = CatBoostClassifier(
        iterations=config['iterations'],
        learning_rate=0.03,
        depth=config['depth'],
        l2_leaf_reg=2.0,
        random_state=42,
        verbose=False
    )
    
    model.fit(X_train_scaled, y_train)
    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))
    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))
    gap = (train_acc - test_acc) * 100
    
    overfitting_status = "âš ï¸ ë†’ìŒ" if gap > 5 else "âš¡ ì¤‘ê°„" if gap > 2 else "âœ… ë‚®ìŒ"
    
    print(f"{config['name']:12s} {train_acc*100:6.2f}%       {test_acc*100:6.2f}%       "
          f"{gap:5.2f}%p   {overfitting_status}")
    
    if test_acc > best_test_acc:
        best_test_acc = test_acc
        best_complexity = config['name']

print(f"\nğŸ† ìµœê³  í…ŒìŠ¤íŠ¸ ì„±ëŠ¥: {best_test_acc*100:.2f}% ({best_complexity} ë³µì¡ë„)")

# ============================================================================
# 5. ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­
# ============================================================================

print("\n" + "="*100)
print("ğŸ’¡ 5. ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ ê¶Œì¥ì‚¬í•­")
print("="*100)

recommendations = []

# 1. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ
if class_ratio > 1.5:
    recommendations.append({
        'priority': 'ğŸ”´ ë†’ìŒ',
        'issue': 'í´ë˜ìŠ¤ ë¶ˆê· í˜•',
        'detail': f'ë¹„ìœ¨ {class_ratio:.2f}:1',
        'solution': [
            'â€¢ SMOTE/ADASYN ë“± ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²• ì ìš©',
            'â€¢ class_weight="balanced" íŒŒë¼ë¯¸í„° ì‚¬ìš©',
            'â€¢ focal loss ê°™ì€ ë¶ˆê· í˜• ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš©'
        ]
    })

# 2. ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ
if len(high_corr_pairs) > 0:
    recommendations.append({
        'priority': 'ğŸ”´ ë†’ìŒ',
        'issue': 'ë‹¤ì¤‘ê³µì„ ì„± (ë†’ì€ íŠ¹ì§• ìƒê´€ê´€ê³„)',
        'detail': f'{len(high_corr_pairs)}ê°œì˜ ê³ ìƒê´€ íŠ¹ì§• ìŒ',
        'solution': [
            'â€¢ PCAë¡œ ì°¨ì› ì¶•ì†Œí•˜ì—¬ ë…ë¦½ì ì¸ ì£¼ì„±ë¶„ ì‚¬ìš©',
            'â€¢ VIF(Variance Inflation Factor) ê¸°ë°˜ íŠ¹ì§• ì„ íƒ',
            'â€¢ ìƒê´€ë„ ë†’ì€ íŠ¹ì§• ìŒ ì¤‘ í•˜ë‚˜ ì œê±°',
            'â€¢ Ridge/Lasso ì •ê·œí™”ë¡œ ë‹¤ì¤‘ê³µì„ ì„± ì™„í™”'
        ]
    })

# 3. ê³¼ì í•© ë¬¸ì œ
if final_gap > 3:
    recommendations.append({
        'priority': 'ğŸŸ¡ ì¤‘ê°„',
        'issue': 'ê³¼ì í•© (Train-Validation ê²©ì°¨)',
        'detail': f'ê²©ì°¨ {final_gap:.2f}%p',
        'solution': [
            'â€¢ ë” ê°•í•œ ì •ê·œí™” (L2 ì¦ê°€, Dropout ì¶”ê°€)',
            'â€¢ ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ê°•í™”',
            'â€¢ ë°ì´í„° ì¦ê°•(Data Augmentation)',
            'â€¢ K-Fold êµì°¨ê²€ì¦ fold ìˆ˜ ì¦ê°€ (5â†’10)'
        ]
    })

# 4. ë°ì´í„° ë¶€ì¡± ë¬¸ì œ
if val_improvement > 0.5:
    recommendations.append({
        'priority': 'ğŸŸ¢ ë‚®ìŒ',
        'issue': 'ë°ì´í„° ê·œëª¨',
        'detail': 'í•™ìŠµ ê³¡ì„ ì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ',
        'solution': [
            'â€¢ CANDIDATE í´ë˜ìŠ¤ í¬í•¨í•˜ì—¬ í•™ìŠµ ë°ì´í„° ì¦ê°€',
            'â€¢ ë‹¤ë¥¸ ì™¸ê³„í–‰ì„± ë°ì´í„°ì…‹ ë³‘í•© (K2, TESS ë“±)',
            'â€¢ ì¤€ì§€ë„ í•™ìŠµ(Semi-supervised learning) ê³ ë ¤'
        ]
    })

# 5. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ê°œì„ 
if len(medium_corr_pairs) > 10:
    recommendations.append({
        'priority': 'ğŸŸ¡ ì¤‘ê°„',
        'issue': 'íŠ¹ì§• ì¤‘ë³µì„±',
        'detail': f'{len(medium_corr_pairs)}ê°œì˜ ì¤‘ìƒê´€ íŠ¹ì§• ìŒ',
        'solution': [
            'â€¢ íŠ¹ì§• ì„ íƒ ì•Œê³ ë¦¬ì¦˜ ì ìš© (RFE, SelectKBest)',
            'â€¢ AutoMLë¡œ ìµœì  íŠ¹ì§• ì¡°í•© íƒìƒ‰',
            'â€¢ ë¬¼ë¦¬ì ìœ¼ë¡œ ë…ë¦½ì ì¸ íŠ¹ì§• ìš°ì„  ì„ íƒ'
        ]
    })

# 6. ì•™ìƒë¸” ë‹¤ì–‘ì„± í–¥ìƒ
recommendations.append({
    'priority': 'ğŸŸ¢ ë‚®ìŒ',
    'issue': 'ì•™ìƒë¸” ë‹¤ì–‘ì„± ë¶€ì¡±',
    'detail': 'íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë§Œ ì‚¬ìš© ì¤‘',
    'solution': [
        'â€¢ SVM, Neural Network ë“± ë‹¤ë¥¸ ëª¨ë¸êµ° ì¶”ê°€',
        'â€¢ ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë‹¤ì–‘í™”',
        'â€¢ Baggingì—ì„œ íŠ¹ì§• ì„œë¸Œìƒ˜í”Œë§ ê°•í™”'
    ]
})

# ì¶œë ¥
print("\n" + "="*100)
for i, rec in enumerate(recommendations, 1):
    print(f"\n{rec['priority']} {i}. {rec['issue']}")
    print(f"{'':8s}ğŸ“Š ìƒíƒœ: {rec['detail']}")
    print(f"{'':8s}ğŸ’¡ í•´ê²°ì±…:")
    for sol in rec['solution']:
        print(f"{'':12s}{sol}")

print("\n" + "="*100)
print("âœ… ë¶„ì„ ì™„ë£Œ!")
print("="*100)
