"""
ê°œì„  ë²„ì „ 2: PCA ì°¨ì› ì¶•ì†Œ
- 26ê°œ íŠ¹ì§• â†’ ìµœì  ì£¼ì„±ë¶„ìœ¼ë¡œ ì¶•ì†Œ
- ë‹¤ì¤‘ê³µì„ ì„± ì™„ì „ ì œê±°
- ë…¸ì´ì¦ˆ ê°ì†Œë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score

from sklearn.ensemble import StackingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression

import matplotlib.pyplot as plt
import time

print("="*100)
print("ğŸš€ ê°œì„  ë²„ì „ 2: PCA ì°¨ì› ì¶•ì†Œ")
print("="*100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì›ë³¸ 26ê°œ íŠ¹ì§• ì‚¬ìš©)
# ============================================================================

print("\n" + "="*100)
print("ğŸ“‚ 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
print("="*100)

df = pd.read_csv('datasets/exoplanets.csv')
df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()
print(f"ì´ì§„ ë¶„ë¥˜ ë°ì´í„°: {df_binary.shape[0]} ìƒ˜í”Œ")

base_features = [
    'koi_prad', 'koi_teq', 'koi_insol', 'koi_period', 'koi_sma', 'koi_impact',
    'koi_eccen', 'koi_depth', 'koi_duration', 'koi_srad', 'koi_smass',
    'koi_steff', 'koi_slogg', 'koi_smet', 'ra', 'dec'
]

# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ì „ì²´ 26ê°œ)
df_fe = df_binary[base_features + ['koi_disposition']].copy()

df_fe['planet_star_ratio'] = df_fe['koi_prad'] / (df_fe['koi_srad'] * 109)
df_fe['orbital_energy'] = df_fe['koi_smass'] / df_fe['koi_sma']
df_fe['transit_signal'] = df_fe['koi_depth'] * df_fe['koi_duration']
df_fe['habitable_flux'] = np.abs(df_fe['koi_insol'] - 1.0)
df_fe['stellar_density'] = df_fe['koi_smass'] / (df_fe['koi_srad']**3)
df_fe['planet_density_proxy'] = df_fe['koi_prad'] / np.sqrt(df_fe['koi_teq'] + 1)
df_fe['log_period'] = np.log10(df_fe['koi_period'] + 1)
df_fe['log_depth'] = np.log10(df_fe['koi_depth'] + 1)
df_fe['log_insol'] = np.log10(df_fe['koi_insol'] + 1)
df_fe['orbit_stability'] = df_fe['koi_impact'] * (1 - df_fe['koi_eccen'])

df_fe = df_fe.dropna()

X = df_fe.drop('koi_disposition', axis=1)
y = df_fe['koi_disposition']

le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nì›ë³¸ ë°ì´í„°:")
print(f"  â€¢ íŠ¹ì§•: {X.shape[1]}ê°œ")
print(f"  â€¢ ìƒ˜í”Œ: {X.shape[0]}ê°œ")

# Train/Test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, 
    test_size=0.1,
    random_state=42,
    stratify=y_encoded
)

# ìŠ¤ì¼€ì¼ë§ (PCA ì „ì— í•„ìˆ˜!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")

# ============================================================================
# 2. PCA ë¶„ì„ - ìµœì  ì£¼ì„±ë¶„ ê°œìˆ˜ ì°¾ê¸°
# ============================================================================

print("\n" + "="*100)
print("ğŸ” 2. PCA ë¶„ì„ - ìµœì  ì£¼ì„±ë¶„ ê°œìˆ˜ íƒìƒ‰")
print("="*100)

# ì „ì²´ ì£¼ì„±ë¶„ ë¶„ì„
pca_full = PCA()
pca_full.fit(X_train_scaled)

explained_variance_ratio = pca_full.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

print("\nğŸ“Š ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨:")
print("ì£¼ì„±ë¶„    ê°œë³„ ë¶„ì‚°    ëˆ„ì  ë¶„ì‚°")
print("-" * 50)
for i in range(min(20, len(explained_variance_ratio))):
    print(f"PC{i+1:2d}      {explained_variance_ratio[i]*100:6.2f}%      {cumulative_variance_ratio[i]*100:6.2f}%")

# 95% ë¶„ì‚°ì„ ì„¤ëª…í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜
n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1
n_components_90 = np.argmax(cumulative_variance_ratio >= 0.90) + 1
n_components_85 = np.argmax(cumulative_variance_ratio >= 0.85) + 1

print(f"\nğŸ’¡ ë¶„ì‚° ë¹„ìœ¨ë³„ í•„ìš” ì£¼ì„±ë¶„:")
print(f"  â€¢ 85% ë¶„ì‚°: {n_components_85}ê°œ ì£¼ì„±ë¶„")
print(f"  â€¢ 90% ë¶„ì‚°: {n_components_90}ê°œ ì£¼ì„±ë¶„")
print(f"  â€¢ 95% ë¶„ì‚°: {n_components_95}ê°œ ì£¼ì„±ë¶„")

# ============================================================================
# 3. ë‹¤ì–‘í•œ ì£¼ì„±ë¶„ ê°œìˆ˜ë¡œ ëª¨ë¸ í•™ìŠµ
# ============================================================================

print("\n" + "="*100)
print("ğŸ¯ 3. ë‹¤ì–‘í•œ ì£¼ì„±ë¶„ ê°œìˆ˜ë¡œ ì„±ëŠ¥ ë¹„êµ")
print("="*100)

pca_results = []

for n_comp in [n_components_85, n_components_90, n_components_95, 15, 18, 20]:
    if n_comp > X_train_scaled.shape[1]:
        continue
    if n_comp in [r['n_components'] for r in pca_results]:
        continue
    
    print(f"\n{'='*100}")
    print(f"ğŸ”¹ PCA with {n_comp} components")
    print(f"{'='*100}")
    
    # PCA ë³€í™˜
    pca = PCA(n_components=n_comp)
    X_train_pca = pca.fit_transform(X_train_scaled)
    X_test_pca = pca.transform(X_test_scaled)
    
    variance_explained = pca.explained_variance_ratio_.sum()
    print(f"ğŸ“Š ì„¤ëª… ë¶„ì‚°: {variance_explained*100:.2f}%")
    
    # CatBoostë¡œ í‰ê°€
    model = CatBoostClassifier(
        iterations=500,
        learning_rate=0.03,
        depth=6,
        l2_leaf_reg=2.0,
        random_state=42,
        verbose=False
    )
    
    start_time = time.time()
    model.fit(X_train_pca, y_train)
    train_time = time.time() - start_time
    
    train_acc = accuracy_score(y_train, model.predict(X_train_pca))
    test_acc = accuracy_score(y_test, model.predict(X_test_pca))
    overfitting = train_acc - test_acc
    
    cv_scores = cross_val_score(model, X_train_pca, y_train, cv=5, scoring='accuracy')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
    print(f"ğŸ“Š í•™ìŠµ ì •í™•ë„: {train_acc*100:.2f}%")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc*100:.2f}%")
    print(f"âš ï¸ ê³¼ì í•©: {overfitting*100:.2f}%p")
    print(f"ğŸ”„ 5-Fold CV: {cv_mean*100:.2f}% Â± {cv_std*100:.2f}%")
    
    pca_results.append({
        'n_components': n_comp,
        'variance_explained': variance_explained,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'overfitting': overfitting,
        'pca': pca
    })

# ìµœì  ì£¼ì„±ë¶„ ê°œìˆ˜ ì„ íƒ
best_result = max(pca_results, key=lambda x: x['test_acc'])
print(f"\nğŸ† ìµœì  ì£¼ì„±ë¶„ ê°œìˆ˜: {best_result['n_components']}ê°œ")
print(f"   â€¢ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_result['test_acc']*100:.2f}%")
print(f"   â€¢ ì„¤ëª… ë¶„ì‚°: {best_result['variance_explained']*100:.2f}%")

# ============================================================================
# 4. ìµœì  PCAë¡œ ì•™ìƒë¸” í•™ìŠµ
# ============================================================================

print("\n" + "="*100)
print(f"ğŸ¯ 4. ìµœì  PCA ({best_result['n_components']}ê°œ ì£¼ì„±ë¶„)ë¡œ ì•™ìƒë¸” í•™ìŠµ")
print("="*100)

# ìµœì  PCA ë³€í™˜
best_pca = best_result['pca']
X_train_best = best_pca.transform(X_train_scaled)
X_test_best = best_pca.transform(X_test_scaled)

print(f"\nğŸ“Š ì°¨ì› ì¶•ì†Œ: {X_train_scaled.shape[1]}ê°œ â†’ {X_train_best.shape[1]}ê°œ")

# ê°œë³„ ëª¨ë¸ í•™ìŠµ
models_dict = {}

# CatBoost
print("\nâ³ CatBoost í•™ìŠµ ì¤‘...")
catboost_model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.03,
    depth=6,
    l2_leaf_reg=2.0,
    random_state=42,
    verbose=False
)
catboost_model.fit(X_train_best, y_train)
cat_test_acc = accuracy_score(y_test, catboost_model.predict(X_test_best))
print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {cat_test_acc*100:.2f}%")
models_dict['catboost'] = catboost_model

# XGBoost
print("\nâ³ XGBoost í•™ìŠµ ì¤‘...")
xgboost_model = XGBClassifier(
    n_estimators=500,
    learning_rate=0.03,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.5,
    reg_lambda=2.0,
    random_state=42,
    eval_metric='logloss'
)
xgboost_model.fit(X_train_best, y_train)
xgb_test_acc = accuracy_score(y_test, xgboost_model.predict(X_test_best))
print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {xgb_test_acc*100:.2f}%")
models_dict['xgboost'] = xgboost_model

# LightGBM
print("\nâ³ LightGBM í•™ìŠµ ì¤‘...")
lightgbm_model = LGBMClassifier(
    n_estimators=500,
    learning_rate=0.03,
    max_depth=6,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.5,
    reg_lambda=2.0,
    random_state=42,
    verbose=-1
)
lightgbm_model.fit(X_train_best, y_train)
lgb_test_acc = accuracy_score(y_test, lightgbm_model.predict(X_test_best))
print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {lgb_test_acc*100:.2f}%")
models_dict['lightgbm'] = lightgbm_model

# Stacking Ensemble
print("\nâ³ Stacking Ensemble í•™ìŠµ ì¤‘...")
base_learners = [
    ('catboost', catboost_model),
    ('xgboost', xgboost_model),
    ('lightgbm', lightgbm_model)
]

meta_learner = LogisticRegression(max_iter=1000, random_state=42)

stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5,
    n_jobs=-1
)

stacking_clf.fit(X_train_best, y_train)

stacking_train_acc = accuracy_score(y_train, stacking_clf.predict(X_train_best))
stacking_test_acc = accuracy_score(y_test, stacking_clf.predict(X_test_best))
stacking_overfitting = stacking_train_acc - stacking_test_acc

stacking_cv = cross_val_score(stacking_clf, X_train_best, y_train, cv=5, scoring='accuracy')

print(f"   í•™ìŠµ ì •í™•ë„: {stacking_train_acc*100:.2f}%")
print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {stacking_test_acc*100:.2f}%")
print(f"   ê³¼ì í•©: {stacking_overfitting*100:.2f}%p")
print(f"   5-Fold CV: {stacking_cv.mean()*100:.2f}% Â± {stacking_cv.std()*100:.2f}%")

# ============================================================================
# 5. ê²°ê³¼ ë¹„êµ
# ============================================================================

print("\n" + "="*100)
print("ğŸ“Š 5. ì„±ëŠ¥ ë¹„êµ (ì›ë³¸ vs PCA)")
print("="*100)

print("\n" + "="*100)
print("ëª¨ë¸                     í…ŒìŠ¤íŠ¸ ì •í™•ë„    ì°¨ì›      ì„¤ëª… ë¶„ì‚°")
print("="*100)

print(f"{'ì›ë³¸ (26ê°œ íŠ¹ì§•)':20s}   92.24%        26ê°œ      100.00%")
print(f"{'PCA CatBoost':20s}   {cat_test_acc*100:6.2f}%      {best_result['n_components']:2d}ê°œ      {best_result['variance_explained']*100:6.2f}%")
print(f"{'PCA XGBoost':20s}   {xgb_test_acc*100:6.2f}%      {best_result['n_components']:2d}ê°œ      {best_result['variance_explained']*100:6.2f}%")
print(f"{'PCA LightGBM':20s}   {lgb_test_acc*100:6.2f}%      {best_result['n_components']:2d}ê°œ      {best_result['variance_explained']*100:6.2f}%")
print(f"{'PCA Stacking':20s}   {stacking_test_acc*100:6.2f}%      {best_result['n_components']:2d}ê°œ      {best_result['variance_explained']*100:6.2f}%")

print("\n" + "="*100)
print("ğŸ’¡ PCA ê°œì„  íš¨ê³¼ ë¶„ì„")
print("="*100)

improvement = stacking_test_acc - 0.9224
print(f"\nğŸ“ˆ ì„±ëŠ¥ ë³€í™”: {improvement*100:+.2f}%p")
print(f"ğŸ“‰ ì°¨ì› ì¶•ì†Œ: 26ê°œ â†’ {best_result['n_components']}ê°œ ({(1-best_result['n_components']/26)*100:.1f}% ê°ì†Œ)")
print(f"ğŸ“Š ì •ë³´ ë³´ì¡´: {best_result['variance_explained']*100:.2f}%")

if improvement > 0.5:
    print("\nâœ… PCAë¡œ ì¼ë°˜í™” ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!")
elif improvement > 0:
    print("\nâš¡ PCAë¡œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì•½ê°„ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!")
elif improvement > -0.5:
    print("\nğŸ“Š ì„±ëŠ¥ì€ ë¹„ìŠ·í•˜ì§€ë§Œ ì°¨ì›ì´ í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤ (ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ)")
else:
    print("\nâš ï¸ PCAë¡œ ì„±ëŠ¥ì´ í•˜ë½í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ íŠ¹ì§•ì´ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

print("\nê³¼ì í•© ë¹„êµ:")
print(f"  â€¢ ì›ë³¸ (26ê°œ íŠ¹ì§•): 5.09%p")
print(f"  â€¢ PCA ({best_result['n_components']}ê°œ ì£¼ì„±ë¶„): {stacking_overfitting*100:.2f}%p")

if stacking_overfitting < 0.0509:
    print("  âœ… PCAë¡œ ê³¼ì í•©ì´ ê°ì†Œí–ˆìŠµë‹ˆë‹¤!")
else:
    print("  âš ï¸ ê³¼ì í•©ì´ ì—¬ì „íˆ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")

print("\n" + "="*100)
print("âœ… PCA ì°¨ì› ì¶•ì†Œ ì™„ë£Œ!")
print("="*100)
