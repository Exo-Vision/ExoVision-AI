"""
ìµœì¢… ëª¨ë¸: 95% ëª©í‘œ ë‹¬ì„±
- ê°•í•œ ì •ê·œí™” ì ìš© (ê³¼ì í•© ìµœì†Œí™”)
- ì•™ìƒë¸” ë‹¤ì–‘ì„± ì¦ê°€: SVM, Neural Network, Naive Bayes ì¶”ê°€
- íŠ¸ë¦¬ ê¸°ë°˜ + ë¹„íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì¡°í•©
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

from sklearn.ensemble import StackingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression

import time

print("="*100)
print("ğŸš€ ìµœì¢… ëª¨ë¸: 95% ëª©í‘œ ë‹¬ì„±")
print("="*100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================================================

print("\n" + "="*100)
print("ğŸ“‚ 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
print("="*100)

df = pd.read_csv('datasets/exoplanets.csv')
df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()
print(f"ì´ì§„ ë¶„ë¥˜ ë°ì´í„°: {df_binary.shape[0]} ìƒ˜í”Œ")

base_features = [
    'koi_prad', 'koi_teq', 'koi_insol', 'koi_period', 'koi_sma', 'koi_impact',
    'koi_eccen', 'koi_depth', 'koi_duration', 'koi_srad', 'koi_smass',
    'koi_steff', 'koi_slogg', 'koi_smet', 'ra', 'dec'
]

# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ì „ì²´ 26ê°œ)
df_fe = df_binary[base_features + ['koi_disposition']].copy()

print("\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§:")
df_fe['planet_star_ratio'] = df_fe['koi_prad'] / (df_fe['koi_srad'] * 109)
print("  âœ… planet_star_ratio")

df_fe['orbital_energy'] = df_fe['koi_smass'] / df_fe['koi_sma']
print("  âœ… orbital_energy")

df_fe['transit_signal'] = df_fe['koi_depth'] * df_fe['koi_duration']
print("  âœ… transit_signal")

df_fe['habitable_flux'] = np.abs(df_fe['koi_insol'] - 1.0)
print("  âœ… habitable_flux")

df_fe['stellar_density'] = df_fe['koi_smass'] / (df_fe['koi_srad']**3)
print("  âœ… stellar_density")

df_fe['planet_density_proxy'] = df_fe['koi_prad'] / np.sqrt(df_fe['koi_teq'] + 1)
print("  âœ… planet_density_proxy")

df_fe['log_period'] = np.log10(df_fe['koi_period'] + 1)
df_fe['log_depth'] = np.log10(df_fe['koi_depth'] + 1)
df_fe['log_insol'] = np.log10(df_fe['koi_insol'] + 1)
print("  âœ… log ë³€í™˜: period, depth, insol")

df_fe['orbit_stability'] = df_fe['koi_impact'] * (1 - df_fe['koi_eccen'])
print("  âœ… orbit_stability")

df_fe = df_fe.dropna()

X = df_fe.drop('koi_disposition', axis=1)
y = df_fe['koi_disposition']

le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nìµœì¢… ë°ì´í„°:")
print(f"  â€¢ íŠ¹ì§•: {X.shape[1]}ê°œ")
print(f"  â€¢ ìƒ˜í”Œ: {X.shape[0]}ê°œ")
print(f"  â€¢ ë ˆì´ë¸”: CONFIRMED={np.sum(y_encoded==1)}, FALSE POSITIVE={np.sum(y_encoded==0)}")

# Train/Test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, 
    test_size=0.1,
    random_state=42,
    stratify=y_encoded
)

print(f"\në°ì´í„° ë¶„í• :")
print(f"  â€¢ í•™ìŠµ: {X_train.shape[0]} ìƒ˜í”Œ")
print(f"  â€¢ í…ŒìŠ¤íŠ¸: {X_test.shape[0]} ìƒ˜í”Œ")

# ìŠ¤ì¼€ì¼ë§ (SVM, Neural Networkì— í•„ìˆ˜)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")

# ============================================================================
# 2. ë‹¤ì–‘í•œ ëª¨ë¸ í•™ìŠµ (ê°•í•œ ì •ê·œí™” ì ìš©)
# ============================================================================

print("\n" + "="*100)
print("ğŸ¯ 2. ë‹¤ì–‘í•œ ëª¨ë¸ í•™ìŠµ (íŠ¸ë¦¬ + ë¹„íŠ¸ë¦¬ ê¸°ë°˜)")
print("="*100)

model_results = []

def evaluate_model(name, model, X_train, y_train, X_test, y_test, use_cv=True):
    """ëª¨ë¸ í‰ê°€"""
    
    print(f"\n{'='*100}")
    print(f"ğŸ”¹ {name}")
    print(f"{'='*100}")
    
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    overfitting = train_acc - test_acc
    
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
    print(f"ğŸ“Š í•™ìŠµ ì •í™•ë„: {train_acc*100:.2f}%")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc*100:.2f}%")
    print(f"âš ï¸ ê³¼ì í•© ì •ë„: {overfitting*100:.2f}%p", end="")
    
    if overfitting > 0.05:
        print(" (âš ï¸ ê³¼ì í•© ê²½ê³ )")
    elif overfitting > 0.02:
        print(" (âš¡ ì•½ê°„ ê³¼ì í•©)")
    else:
        print(" (âœ… ì–‘í˜¸)")
    
    if use_cv:
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        print(f"ğŸ”„ 5-Fold CV: {cv_mean*100:.2f}% Â± {cv_std*100:.2f}%")
    else:
        cv_mean = test_acc
        cv_std = 0.0
    
    model_results.append({
        'name': name,
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'overfitting': overfitting
    })
    
    return model

# ============================================================================
# íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ (ê°•í•œ ì •ê·œí™”)
# ============================================================================

print("\n" + "-"*100)
print("ğŸŒ³ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ (ê°•í•œ ì •ê·œí™”)")
print("-"*100)

# CatBoost
print("\nâ³ CatBoost í•™ìŠµ ì¤‘...")
catboost_model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.03,
    depth=6,
    l2_leaf_reg=10.0,           # ê°•í•œ ì •ê·œí™”
    bagging_temperature=1.0,    # ë°°ê¹… ì¶”ê°€
    subsample=0.7,              # 70% ìƒ˜í”Œë§
    random_state=42,
    verbose=False
)
evaluate_model("CatBoost (ê°•í•œ ì •ê·œí™”)", catboost_model, X_train_scaled, y_train, X_test_scaled, y_test)

# XGBoost
print("\nâ³ XGBoost í•™ìŠµ ì¤‘...")
xgboost_model = XGBClassifier(
    n_estimators=500,
    learning_rate=0.03,
    max_depth=6,
    reg_lambda=10.0,            # ê°•í•œ L2
    reg_alpha=2.0,              # ê°•í•œ L1
    subsample=0.6,              # 60% ìƒ˜í”Œë§
    colsample_bytree=0.6,       # 60% íŠ¹ì§• ìƒ˜í”Œë§
    random_state=42,
    eval_metric='logloss'
)
evaluate_model("XGBoost (ê°•í•œ ì •ê·œí™”)", xgboost_model, X_train_scaled, y_train, X_test_scaled, y_test)

# LightGBM
print("\nâ³ LightGBM í•™ìŠµ ì¤‘...")
lightgbm_model = LGBMClassifier(
    n_estimators=500,
    learning_rate=0.03,
    max_depth=6,
    num_leaves=31,
    reg_lambda=10.0,            # ê°•í•œ L2
    reg_alpha=2.0,              # ê°•í•œ L1
    subsample=0.6,              # 60% ìƒ˜í”Œë§
    colsample_bytree=0.6,       # 60% íŠ¹ì§• ìƒ˜í”Œë§
    random_state=42,
    verbose=-1
)
evaluate_model("LightGBM (ê°•í•œ ì •ê·œí™”)", lightgbm_model, X_train_scaled, y_train, X_test_scaled, y_test)

# ============================================================================
# ë¹„íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ (ë‹¤ì–‘ì„± ì¦ê°€)
# ============================================================================

print("\n" + "-"*100)
print("ğŸ”¬ ë¹„íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ (ë‹¤ì–‘ì„±)")
print("-"*100)

# SVM (RBF Kernel)
print("\nâ³ SVM (RBF) í•™ìŠµ ì¤‘...")
svm_rbf_model = SVC(
    C=1.0,                      # ì •ê·œí™” ê°•ë„
    kernel='rbf',
    gamma='scale',
    probability=True,           # Stackingì— í•„ìš”
    random_state=42
)
evaluate_model("SVM (RBF Kernel)", svm_rbf_model, X_train_scaled, y_train, X_test_scaled, y_test, use_cv=False)

# SVM (Linear Kernel)
print("\nâ³ SVM (Linear) í•™ìŠµ ì¤‘...")
svm_linear_model = SVC(
    C=1.0,
    kernel='linear',
    probability=True,
    random_state=42
)
evaluate_model("SVM (Linear Kernel)", svm_linear_model, X_train_scaled, y_train, X_test_scaled, y_test, use_cv=False)

# Neural Network (MLP)
print("\nâ³ Neural Network í•™ìŠµ ì¤‘...")
mlp_model = MLPClassifier(
    hidden_layer_sizes=(128, 64, 32),  # 3ì¸µ ë„¤íŠ¸ì›Œí¬
    activation='relu',
    solver='adam',
    alpha=0.01,                 # L2 ì •ê·œí™”
    batch_size=128,
    learning_rate_init=0.001,
    max_iter=500,
    early_stopping=True,        # ì¡°ê¸° ì¢…ë£Œ
    validation_fraction=0.1,
    n_iter_no_change=20,
    random_state=42
)
evaluate_model("Neural Network (MLP)", mlp_model, X_train_scaled, y_train, X_test_scaled, y_test, use_cv=False)

# Naive Bayes
print("\nâ³ Naive Bayes í•™ìŠµ ì¤‘...")
nb_model = GaussianNB()
evaluate_model("Naive Bayes", nb_model, X_train_scaled, y_train, X_test_scaled, y_test)

# ============================================================================
# 3. ì•™ìƒë¸” ì „ëµ
# ============================================================================

print("\n" + "="*100)
print("ğŸ¯ 3. ì•™ìƒë¸” ì „ëµ")
print("="*100)

# ìƒìœ„ ëª¨ë¸ ì„ íƒ
sorted_results = sorted(model_results, key=lambda x: x['test_acc'], reverse=True)

print("\nğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„:")
print("-"*100)
print("ìˆœìœ„  ëª¨ë¸                            í…ŒìŠ¤íŠ¸ ì •í™•ë„    CV ì •í™•ë„       ê³¼ì í•©")
print("-"*100)
for i, result in enumerate(sorted_results, 1):
    print(f"{i:2d}.  {result['name']:30s}  {result['test_acc']*100:6.2f}%      "
          f"{result['cv_mean']*100:6.2f}%Â±{result['cv_std']*100:4.2f}%   {result['overfitting']*100:5.2f}%p")

# Top 5 ëª¨ë¸ ì„ íƒ
top_n = 5
top_models = sorted_results[:top_n]

print(f"\nğŸ† ìƒìœ„ {top_n}ê°œ ëª¨ë¸ ì„ íƒ:")
for i, result in enumerate(top_models, 1):
    print(f"  {i}. {result['name']:30s}  {result['test_acc']*100:.2f}%")

# ============================================================================
# 3-1. Soft Voting Ensemble
# ============================================================================

print("\n" + "-"*100)
print("ğŸ—³ï¸ Soft Voting Ensemble")
print("-"*100)

voting_estimators = [(result['name'], result['model']) for result in top_models]

voting_clf = VotingClassifier(
    estimators=voting_estimators,
    voting='soft',
    n_jobs=-1
)

print("\nâ³ Soft Voting í•™ìŠµ ì¤‘...")
start_time = time.time()
voting_clf.fit(X_train_scaled, y_train)
train_time = time.time() - start_time

voting_train_acc = accuracy_score(y_train, voting_clf.predict(X_train_scaled))
voting_test_acc = accuracy_score(y_test, voting_clf.predict(X_test_scaled))
voting_overfitting = voting_train_acc - voting_test_acc

print(f"\nâ±ï¸ í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
print(f"ğŸ“Š í•™ìŠµ ì •í™•ë„: {voting_train_acc*100:.2f}%")
print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {voting_test_acc*100:.2f}%")
print(f"âš ï¸ ê³¼ì í•© ì •ë„: {voting_overfitting*100:.2f}%p")

voting_cv = cross_val_score(voting_clf, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)
print(f"ğŸ”„ 5-Fold CV: {voting_cv.mean()*100:.2f}% Â± {voting_cv.std()*100:.2f}%")

# ============================================================================
# 3-2. Stacking Ensemble
# ============================================================================

print("\n" + "-"*100)
print("ğŸ—ï¸ Stacking Ensemble")
print("-"*100)

# Base learners: ìƒìœ„ 5ê°œ ëª¨ë¸
base_learners = [(result['name'], result['model']) for result in top_models]

# Meta learner: ì •ê·œí™”ëœ Logistic Regression
meta_learner = LogisticRegression(
    C=0.5,                  # ì •ê·œí™” ê°•í™”
    max_iter=1000,
    random_state=42
)

stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5,
    n_jobs=-1
)

print("\nâ³ Stacking í•™ìŠµ ì¤‘...")
start_time = time.time()
stacking_clf.fit(X_train_scaled, y_train)
train_time = time.time() - start_time

stacking_train_acc = accuracy_score(y_train, stacking_clf.predict(X_train_scaled))
stacking_test_acc = accuracy_score(y_test, stacking_clf.predict(X_test_scaled))
stacking_overfitting = stacking_train_acc - stacking_test_acc

print(f"\nâ±ï¸ í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
print(f"ğŸ“Š í•™ìŠµ ì •í™•ë„: {stacking_train_acc*100:.2f}%")
print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {stacking_test_acc*100:.2f}%")
print(f"âš ï¸ ê³¼ì í•© ì •ë„: {stacking_overfitting*100:.2f}%p")

stacking_cv = cross_val_score(stacking_clf, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)
print(f"ğŸ”„ 5-Fold CV: {stacking_cv.mean()*100:.2f}% Â± {stacking_cv.std()*100:.2f}%")

# ============================================================================
# 4. ìµœì¢… ê²°ê³¼
# ============================================================================

print("\n" + "="*100)
print("ğŸ“Š 4. ìµœì¢… ê²°ê³¼ ë¹„êµ")
print("="*100)

print("\n" + "="*100)
print("ëª¨ë¸                            í•™ìŠµ ì •í™•ë„    í…ŒìŠ¤íŠ¸ ì •í™•ë„    ê³¼ì í•©      CV ì •í™•ë„")
print("="*100)

# ê°œë³„ ëª¨ë¸ (ìƒìœ„ 5ê°œë§Œ)
for result in top_models:
    print(f"{result['name']:30s}  {result['train_acc']*100:6.2f}%      {result['test_acc']*100:6.2f}%      "
          f"{result['overfitting']*100:5.2f}%p    {result['cv_mean']*100:6.2f}%Â±{result['cv_std']*100:4.2f}%")

print("-"*100)

# ì•™ìƒë¸”
print(f"{'Soft Voting Ensemble':30s}  {voting_train_acc*100:6.2f}%      {voting_test_acc*100:6.2f}%      "
      f"{voting_overfitting*100:5.2f}%p    {voting_cv.mean()*100:6.2f}%Â±{voting_cv.std()*100:4.2f}%")

print(f"{'Stacking Ensemble':30s}  {stacking_train_acc*100:6.2f}%      {stacking_test_acc*100:6.2f}%      "
      f"{stacking_overfitting*100:5.2f}%p    {stacking_cv.mean()*100:6.2f}%Â±{stacking_cv.std()*100:4.2f}%")

print("="*100)

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸
all_final_results = [
    ('Soft Voting', voting_test_acc, voting_overfitting),
    ('Stacking', stacking_test_acc, stacking_overfitting)
] + [(r['name'], r['test_acc'], r['overfitting']) for r in top_models]

best_model = max(all_final_results, key=lambda x: x[1])

print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0]}")
print(f"   â€¢ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_model[1]*100:.2f}%")
print(f"   â€¢ ê³¼ì í•©: {best_model[2]*100:.2f}%p")

# ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
print("\n" + "="*100)
print("ğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")
print("="*100)

target_acc = 0.95
print(f"\nëª©í‘œ: {target_acc*100:.2f}%")
print(f"ë‹¬ì„±: {best_model[1]*100:.2f}%")
print(f"ì°¨ì´: {(best_model[1] - target_acc)*100:+.2f}%p")

if best_model[1] >= target_acc:
    print("\nğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! 95% ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
elif best_model[1] >= 0.93:
    print("\nâš¡ ëª©í‘œì— ê·¼ì ‘í–ˆìŠµë‹ˆë‹¤! (93%+)")
    print("\nğŸ’¡ ì¶”ê°€ ê°œì„  ë°©ì•ˆ:")
    print("   â€¢ CANDIDATE í´ë˜ìŠ¤ í¬í•¨ (10,065ê°œ ì¶”ê°€ ìƒ˜í”Œ)")
    print("   â€¢ Bayesian Optimization (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”)")
    print("   â€¢ Deep Learning (LSTM, Transformer)")
else:
    print("\nğŸ“Š ëª©í‘œ ë‹¬ì„± ì‹¤íŒ¨")
    print("\nğŸ’¡ ì¶”ê°€ ê°œì„  ë°©ì•ˆ:")
    print("   â€¢ ë” ë§ì€ ë°ì´í„° í•„ìš”")
    print("   â€¢ íŠ¹ì§• ì„ íƒ ì•Œê³ ë¦¬ì¦˜ (RFE)")
    print("   â€¢ AutoML í”„ë ˆì„ì›Œí¬ ì‹œë„")

# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ (ìµœê³  ëª¨ë¸)
print("\n" + "="*100)
print(f"ğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ({best_model[0]})")
print("="*100)

if best_model[0] == 'Soft Voting':
    y_pred = voting_clf.predict(X_test_scaled)
elif best_model[0] == 'Stacking':
    y_pred = stacking_clf.predict(X_test_scaled)
else:
    best_individual = [r for r in model_results if r['name'] == best_model[0]][0]
    y_pred = best_individual['model'].predict(X_test_scaled)

print("\n" + classification_report(y_test, y_pred, target_names=['FALSE POSITIVE', 'CONFIRMED']))

print("\n" + "="*100)
print("âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("="*100)
