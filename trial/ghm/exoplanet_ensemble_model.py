"""
ì™¸ê³„í–‰ì„± íŒë³„ ê³ ì„±ëŠ¥ ì•™ìƒë¸” ëª¨ë¸
- ëª©í‘œ: 90% ì¤‘ë°˜ ì •í™•ë„ ë‹¬ì„±
- ê³¼ì í•© ë°©ì§€: êµì°¨ ê²€ì¦, í•™ìŠµ ê³¡ì„  ë¶„ì„
- ì•™ìƒë¸”: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ ì¡°í•©
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ì•™ìƒë¸” ëª¨ë¸ë“¤
from sklearn.ensemble import (
    RandomForestClassifier,
    ExtraTreesClassifier,
    GradientBoostingClassifier,
    VotingClassifier,
    StackingClassifier
)

# ë¶€ìŠ¤íŒ… ëª¨ë¸ë“¤
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ê¸°íƒ€
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import time

print("="*100)
print("ğŸš€ ì™¸ê³„í–‰ì„± íŒë³„ ê³ ì„±ëŠ¥ ì•™ìƒë¸” ëª¨ë¸")
print("="*100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================================================

print("\n" + "="*100)
print("ğŸ“‚ 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
print("="*100)

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('datasets/exoplanets.csv')
print(f"ì›ë³¸ ë°ì´í„°: {df.shape}")

# ë ˆì´ë¸” ë¶„í¬ í™•ì¸
print(f"\në ˆì´ë¸” ë¶„í¬:")
print(df['koi_disposition'].value_counts())

# ì´ì§„ ë¶„ë¥˜: CONFIRMED vs FALSE POSITIVE (ê°€ì¥ ëª…í™•í•œ ë¶„ë¥˜)
print("\nğŸ¯ ë¶„ë¥˜ ë°©ì‹: ì´ì§„ ë¶„ë¥˜ (CONFIRMED vs FALSE POSITIVE)")
df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()
print(f"ì´ì§„ ë¶„ë¥˜ ë°ì´í„°: {df_binary.shape[0]} ìƒ˜í”Œ")

# ì™¸ê³„í–‰ì„± íŒë³„ì— ì¤‘ìš”í•œ íŠ¹ì§• ì„ íƒ
print("\nğŸ” ì™¸ê³„í–‰ì„± íŒë³„ ì¤‘ìš” íŠ¹ì§• ì„ íƒ:")

# í•µì‹¬ íŠ¹ì§• (ì™„ì„±ë„ ë†’ê³  ë¬¼ë¦¬ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì»¬ëŸ¼)
key_features = [
    # í–‰ì„± íŠ¹ì„±
    'koi_prad',       # í–‰ì„± ë°˜ì§€ë¦„ - í¬ê¸° êµ¬ë³„ (94.5%)
    'koi_teq',        # í‰í˜• ì˜¨ë„ - Habitable Zone (90.2%)
    'koi_insol',      # ì…ì‚¬ í”ŒëŸ­ìŠ¤ - ì—ë„ˆì§€ ìˆ˜ì¤€ (90.5%)
    
    # ê¶¤ë„ íŠ¹ì„±
    'koi_period',     # ê¶¤ë„ ì£¼ê¸° - ê¶¤ë„ íŠ¹ì„± (99.3%)
    'koi_sma',        # ë°˜ì¥ì¶• - ê¶¤ë„ ê±°ë¦¬ (86.7%)
    'koi_impact',     # ì¶©ê²© ë§¤ê°œë³€ìˆ˜ - ê¶¤ë„ ê¸°í•˜í•™ (88.0%)
    'koi_eccen',      # ê¶¤ë„ ì´ì‹¬ë¥  - ê¶¤ë„ ëª¨ì–‘ (99.8%)
    
    # í†µê³¼ ì‹ í˜¸ íŠ¹ì„±
    'koi_depth',      # í†µê³¼ ê¹Šì´ - ì‹ í˜¸ ê°•ë„ (96.9%)
    'koi_duration',   # í†µê³¼ ì§€ì†ì‹œê°„ - ì‹ í˜¸ íŠ¹ì„± (99.3%)
    
    # í•­ì„± íŠ¹ì„±
    'koi_srad',       # í•­ì„± ë°˜ì§€ë¦„ - í•­ì„± í¬ê¸° (95.3%)
    'koi_smass',      # í•­ì„± ì§ˆëŸ‰ - í•­ì„± ì§ˆëŸ‰ (87.2%)
    'koi_steff',      # ìœ íš¨ ì˜¨ë„ - í•­ì„± ì˜¨ë„ (92.5%)
    'koi_slogg',      # í‘œë©´ ì¤‘ë ¥ - í•­ì„± ë°€ë„ (87.5%)
    'koi_smet',       # ê¸ˆì†ì„± - í–‰ì„± í˜•ì„± (100%)
    
    # ìœ„ì¹˜ ì •ë³´
    'ra',             # ì ê²½ (100%)
    'dec',            # ì ìœ„ (100%)
]

print(f"ì„ íƒëœ íŠ¹ì§•: {len(key_features)}ê°œ")
for i, feat in enumerate(key_features, 1):
    completeness = df_binary[feat].notna().sum() / len(df_binary) * 100
    print(f"  {i:2d}. {feat:<20} - ì™„ì„±ë„ {completeness:5.1f}%")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬: ì™„ì „í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
df_clean = df_binary[key_features + ['koi_disposition']].dropna()
print(f"\nê²°ì¸¡ì¹˜ ì œê±° í›„: {df_clean.shape[0]} ìƒ˜í”Œ ({df_clean.shape[0]/df_binary.shape[0]*100:.1f}%)")

# íŠ¹ì§•ê³¼ ë ˆì´ë¸” ë¶„ë¦¬
X = df_clean[key_features]
y = df_clean['koi_disposition']

# ë ˆì´ë¸” ì¸ì½”ë”© (CONFIRMED=1, FALSE POSITIVE=0)
le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nìµœì¢… ë°ì´í„°:")
print(f"  â€¢ íŠ¹ì§•: {X.shape}")
print(f"  â€¢ ë ˆì´ë¸”: CONFIRMED={np.sum(y_encoded==1)}, FALSE POSITIVE={np.sum(y_encoded==0)}")

# Train/Test ë¶„í•  (90/10)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, 
    test_size=0.1,  # 10% í…ŒìŠ¤íŠ¸
    random_state=42,
    stratify=y_encoded  # ë ˆì´ë¸” ë¹„ìœ¨ ìœ ì§€
)

print(f"\në°ì´í„° ë¶„í•  (90/10):")
print(f"  â€¢ í•™ìŠµ: {X_train.shape[0]} ìƒ˜í”Œ")
print(f"  â€¢ í…ŒìŠ¤íŠ¸: {X_test.shape[0]} ìƒ˜í”Œ")

# íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì—ëŠ” í•„ìˆ˜ëŠ” ì•„ë‹ˆì§€ë§Œ ì¼ë¶€ ëª¨ë¸ì— ë„ì›€)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")

# ============================================================================
# 2. ê°œë³„ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# ============================================================================

print("\n" + "="*100)
print("ğŸ¤– 2. ê°œë³„ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ê³¼ì í•© ê²€ì‚¬ í¬í•¨)")
print("="*100)

# ëª¨ë¸ ê²°ê³¼ ì €ì¥
model_results = []

def evaluate_model_with_overfitting_check(name, model, X_train, y_train, X_test, y_test, use_scaled=False):
    """ëª¨ë¸ í‰ê°€ ë° ê³¼ì í•© ê²€ì‚¬"""
    
    print(f"\n{'='*100}")
    print(f"ğŸ”¹ {name}")
    print(f"{'='*100}")
    
    start_time = time.time()
    
    # í•™ìŠµ
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # ì •í™•ë„
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    # ê³¼ì í•© ê²€ì‚¬
    overfitting = train_acc - test_acc
    
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
    print(f"ğŸ“Š í•™ìŠµ ì •í™•ë„: {train_acc*100:.2f}%")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc*100:.2f}%")
    print(f"âš ï¸ ê³¼ì í•© ì •ë„: {overfitting*100:.2f}%p", end="")
    
    if overfitting > 0.05:  # 5% ì´ìƒ ì°¨ì´
        print(" (âš ï¸ ê³¼ì í•© ê²½ê³ !)")
    elif overfitting > 0.02:  # 2-5% ì°¨ì´
        print(" (âš¡ ì•½ê°„ ê³¼ì í•©)")
    else:
        print(" (âœ… ì–‘í˜¸)")
    
    # êµì°¨ ê²€ì¦ (5-fold)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    print(f"ğŸ”„ êµì°¨ ê²€ì¦ (5-fold): {cv_mean*100:.2f}% Â± {cv_std*100:.2f}%")
    
    # ìƒì„¸ ë¦¬í¬íŠ¸
    print(f"\nìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_test_pred, 
                                target_names=['FALSE POSITIVE', 'CONFIRMED'],
                                digits=4))
    
    # ê²°ê³¼ ì €ì¥
    model_results.append({
        'name': name,
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'overfitting': overfitting,
        'train_time': train_time
    })
    
    return model, test_acc

# ============================================================================
# 2.1 Random Forest (ê³¼ì í•© ë°©ì§€ íŒŒë¼ë¯¸í„°)
# ============================================================================

rf_model = RandomForestClassifier(
    n_estimators=300,        # íŠ¸ë¦¬ ê°œìˆ˜ ì¦ê°€
    max_depth=15,            # ê¹Šì´ ì œí•œ (ê³¼ì í•© ë°©ì§€)
    min_samples_split=10,    # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    min_samples_leaf=5,      # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    max_features='sqrt',     # íŠ¹ì§• ìƒ˜í”Œë§
    class_weight='balanced', # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
    random_state=42,
    n_jobs=-1
)

rf_model, rf_acc = evaluate_model_with_overfitting_check(
    "Random Forest", rf_model, X_train_scaled, y_train, X_test_scaled, y_test, use_scaled=True
)

# ============================================================================
# 2.2 Extra Trees (Random Forestë³´ë‹¤ ë” ë¬´ì‘ìœ„ì„±)
# ============================================================================

et_model = ExtraTreesClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

et_model, et_acc = evaluate_model_with_overfitting_check(
    "Extra Trees", et_model, X_train_scaled, y_train, X_test_scaled, y_test, use_scaled=True
)

# ============================================================================
# 2.3 XGBoost (ê°•ë ¥í•œ ë¶€ìŠ¤íŒ…)
# ============================================================================

xgb_model = XGBClassifier(
    n_estimators=300,
    max_depth=8,             # ê¹Šì´ ì œí•œ
    learning_rate=0.05,      # ë‚®ì€ í•™ìŠµë¥ 
    subsample=0.8,           # ìƒ˜í”Œ ì„œë¸Œìƒ˜í”Œë§
    colsample_bytree=0.8,    # íŠ¹ì§• ì„œë¸Œìƒ˜í”Œë§
    min_child_weight=5,      # ê³¼ì í•© ë°©ì§€
    gamma=0.1,               # ë¶„í•  ì •ê·œí™”
    reg_alpha=0.1,           # L1 ì •ê·œí™”
    reg_lambda=1.0,          # L2 ì •ê·œí™”
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)

xgb_model, xgb_acc = evaluate_model_with_overfitting_check(
    "XGBoost", xgb_model, X_train_scaled, y_train, X_test_scaled, y_test, use_scaled=True
)

# ============================================================================
# 2.4 LightGBM (ë¹ ë¥´ê³  íš¨ìœ¨ì )
# ============================================================================

lgbm_model = LGBMClassifier(
    n_estimators=300,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_samples=10,    # ê³¼ì í•© ë°©ì§€
    reg_alpha=0.1,
    reg_lambda=1.0,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgbm_model, lgbm_acc = evaluate_model_with_overfitting_check(
    "LightGBM", lgbm_model, X_train_scaled, y_train, X_test_scaled, y_test, use_scaled=True
)

# ============================================================================
# 2.5 CatBoost (ë²”ì£¼í˜• íŠ¹ì§• ì²˜ë¦¬ ìš°ìˆ˜)
# ============================================================================

catboost_model = CatBoostClassifier(
    iterations=300,
    depth=8,
    learning_rate=0.05,
    l2_leaf_reg=3.0,         # L2 ì •ê·œí™”
    random_strength=1.0,     # ë¬´ì‘ìœ„ì„± ì¶”ê°€
    bagging_temperature=1.0,
    class_weights=[1, 1],
    random_state=42,
    verbose=0
)

catboost_model, catboost_acc = evaluate_model_with_overfitting_check(
    "CatBoost", catboost_model, X_train_scaled, y_train, X_test_scaled, y_test, use_scaled=True
)

# ============================================================================
# 2.6 Gradient Boosting (ì•ˆì •ì )
# ============================================================================

gb_model = GradientBoostingClassifier(
    n_estimators=300,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    min_samples_split=10,
    min_samples_leaf=5,
    max_features='sqrt',
    random_state=42
)

gb_model, gb_acc = evaluate_model_with_overfitting_check(
    "Gradient Boosting", gb_model, X_train_scaled, y_train, X_test_scaled, y_test, use_scaled=True
)

# ============================================================================
# 3. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±
# ============================================================================

print("\n" + "="*100)
print("ğŸ¯ 3. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±")
print("="*100)

# ëª¨ë¸ ì„±ëŠ¥ ì •ë ¬
sorted_results = sorted(model_results, key=lambda x: x['test_acc'], reverse=True)

print("\nğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„:")
print("-"*100)
print(f"{'ìˆœìœ„':<5} {'ëª¨ë¸':<25} {'í…ŒìŠ¤íŠ¸ ì •í™•ë„':<15} {'CV ì •í™•ë„':<20} {'ê³¼ì í•©':<15} {'í•™ìŠµì‹œê°„':<10}")
print("-"*100)

for i, result in enumerate(sorted_results, 1):
    print(f"{i:<5} {result['name']:<25} {result['test_acc']*100:>6.2f}%      "
          f"{result['cv_mean']*100:>6.2f}% Â± {result['cv_std']*100:>4.2f}%   "
          f"{result['overfitting']*100:>6.2f}%p     {result['train_time']:>6.2f}ì´ˆ")

# ìƒìœ„ 5ê°œ ëª¨ë¸ë¡œ Voting Classifier êµ¬ì„±
top_5_models = sorted_results[:5]

print(f"\nğŸ† ìƒìœ„ 5ê°œ ëª¨ë¸ë¡œ Soft Voting Classifier êµ¬ì„±:")
for i, result in enumerate(top_5_models, 1):
    print(f"  {i}. {result['name']} (í…ŒìŠ¤íŠ¸ ì •í™•ë„: {result['test_acc']*100:.2f}%)")

voting_clf = VotingClassifier(
    estimators=[(result['name'], result['model']) for result in top_5_models],
    voting='soft',  # í™•ë¥  ê¸°ë°˜ ì†Œí”„íŠ¸ ë³´íŒ…
    n_jobs=-1
)

print("\nğŸ”„ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì¤‘...")
start_time = time.time()
voting_clf.fit(X_train_scaled, y_train)
ensemble_train_time = time.time() - start_time

# ì•™ìƒë¸” ì˜ˆì¸¡
y_train_pred_ensemble = voting_clf.predict(X_train_scaled)
y_test_pred_ensemble = voting_clf.predict(X_test_scaled)

# ì•™ìƒë¸” ì •í™•ë„
ensemble_train_acc = accuracy_score(y_train, y_train_pred_ensemble)
ensemble_test_acc = accuracy_score(y_test, y_test_pred_ensemble)
ensemble_overfitting = ensemble_train_acc - ensemble_test_acc

print(f"â±ï¸ ì•™ìƒë¸” í•™ìŠµ ì‹œê°„: {ensemble_train_time:.2f}ì´ˆ")
print(f"ğŸ“Š ì•™ìƒë¸” í•™ìŠµ ì •í™•ë„: {ensemble_train_acc*100:.2f}%")
print(f"ğŸ“Š ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ ì •í™•ë„: {ensemble_test_acc*100:.2f}%")
print(f"âš ï¸ ì•™ìƒë¸” ê³¼ì í•© ì •ë„: {ensemble_overfitting*100:.2f}%p", end="")

if ensemble_overfitting > 0.05:
    print(" (âš ï¸ ê³¼ì í•© ê²½ê³ !)")
elif ensemble_overfitting > 0.02:
    print(" (âš¡ ì•½ê°„ ê³¼ì í•©)")
else:
    print(" (âœ… ì–‘í˜¸)")

# ì•™ìƒë¸” êµì°¨ ê²€ì¦
print("\nğŸ”„ ì•™ìƒë¸” êµì°¨ ê²€ì¦ (5-fold)...")
ensemble_cv_scores = cross_val_score(voting_clf, X_train_scaled, y_train, cv=5, scoring='accuracy')
ensemble_cv_mean = ensemble_cv_scores.mean()
ensemble_cv_std = ensemble_cv_scores.std()

print(f"ğŸ”„ ì•™ìƒë¸” êµì°¨ ê²€ì¦: {ensemble_cv_mean*100:.2f}% Â± {ensemble_cv_std*100:.2f}%")

# ìƒì„¸ ë¦¬í¬íŠ¸
print(f"\nğŸ“Š ì•™ìƒë¸” ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_test_pred_ensemble, 
                            target_names=['FALSE POSITIVE', 'CONFIRMED'],
                            digits=4))

# ============================================================================
# 4. ìµœì¢… ê²°ê³¼ ìš”ì•½
# ============================================================================

print("\n" + "="*100)
print("ğŸ“ˆ 4. ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("="*100)

print("\nğŸ† ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„:")
print("-"*100)
for i, result in enumerate(sorted_results, 1):
    print(f"  {i}. {result['name']:<25} : {result['test_acc']*100:>6.2f}%")

print(f"\nğŸ¯ ì•™ìƒë¸” ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„:")
print("-"*100)
print(f"  Soft Voting Ensemble (Top 5) : {ensemble_test_acc*100:>6.2f}%")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸
best_single = sorted_results[0]
print(f"\nğŸ¥‡ ìµœê³  ë‹¨ì¼ ëª¨ë¸:")
print(f"  â€¢ ëª¨ë¸: {best_single['name']}")
print(f"  â€¢ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_single['test_acc']*100:.2f}%")
print(f"  â€¢ CV ì •í™•ë„: {best_single['cv_mean']*100:.2f}% Â± {best_single['cv_std']*100:.2f}%")
print(f"  â€¢ ê³¼ì í•© ì •ë„: {best_single['overfitting']*100:.2f}%p")

# ê°œì„  ì •ë„
improvement = (ensemble_test_acc - best_single['test_acc']) * 100
print(f"\nğŸ“Š ì•™ìƒë¸” ê°œì„ :")
if improvement > 0:
    print(f"  â€¢ ì•™ìƒë¸”ì´ ìµœê³  ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ {improvement:.2f}%p ë†’ìŒ â¬†ï¸")
elif improvement < 0:
    print(f"  â€¢ ì•™ìƒë¸”ì´ ìµœê³  ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ {abs(improvement):.2f}%p ë‚®ìŒ â¬‡ï¸")
else:
    print(f"  â€¢ ì•™ìƒë¸”ê³¼ ìµœê³  ë‹¨ì¼ ëª¨ë¸ì´ ë™ì¼ â¡ï¸")

# ëª©í‘œ ë‹¬ì„± í™•ì¸
target_acc = 0.95  # 90% ì¤‘ë°˜ ëª©í‘œ
print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
print(f"  â€¢ ëª©í‘œ ì •í™•ë„: {target_acc*100:.0f}%")
print(f"  â€¢ ë‹¬ì„± ì •í™•ë„: {ensemble_test_acc*100:.2f}%")

if ensemble_test_acc >= target_acc:
    print(f"  â€¢ ê²°ê³¼: âœ… ëª©í‘œ ë‹¬ì„±!")
elif ensemble_test_acc >= target_acc - 0.02:
    print(f"  â€¢ ê²°ê³¼: ğŸ”¥ ê±°ì˜ ë‹¬ì„± (ëª©í‘œê¹Œì§€ {(target_acc - ensemble_test_acc)*100:.2f}%p)")
else:
    print(f"  â€¢ ê²°ê³¼: âš¡ ì¶”ê°€ ìµœì í™” í•„ìš” (ëª©í‘œê¹Œì§€ {(target_acc - ensemble_test_acc)*100:.2f}%p)")

print("\n" + "="*100)
print("âœ… ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
print("="*100)

# í˜¼ë™ í–‰ë ¬ ì¶œë ¥
print("\nğŸ“Š ì•™ìƒë¸” í˜¼ë™ í–‰ë ¬:")
cm = confusion_matrix(y_test, y_test_pred_ensemble)
print("\nì‹¤ì œ\\ì˜ˆì¸¡   FALSE POSITIVE   CONFIRMED")
print("-"*45)
print(f"FALSE POS   {cm[0,0]:>8}          {cm[0,1]:>8}")
print(f"CONFIRMED   {cm[1,0]:>8}          {cm[1,1]:>8}")

# ì •ë°€ë„, ì¬í˜„ìœ¨ ê³„ì‚°
precision = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
recall = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f"\nğŸ“Š ì£¼ìš” ë©”íŠ¸ë¦­:")
print(f"  â€¢ Precision (ì •ë°€ë„): {precision*100:.2f}%")
print(f"  â€¢ Recall (ì¬í˜„ìœ¨): {recall*100:.2f}%")
print(f"  â€¢ F1-Score: {f1*100:.2f}%")

print("\n" + "="*100)
print("ğŸ‰ ì™¸ê³„í–‰ì„± íŒë³„ ëª¨ë¸ ì™„ì„±!")
print("="*100)
